{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f442428-5793-4bbd-aa6c-8730ce584a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466b85b-74dd-440c-a64a-c4463e29ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bdd42e-ce68-4353-a837-4d20f9c8bbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ed180d-d1e4-4773-8686-93421073c265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "http://d-dedyt64u6kib.studio.us-east-1.sagemaker.aws/jupyter/default/proxy/8501/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker as sm\n",
    "\n",
    "sagemaker_session  = sm.session.Session()\n",
    "region             = sagemaker_session._region_name\n",
    "\n",
    "# These are needed to show where the streamlit app is hosted\n",
    "sagemaker_metadata = json.load(open('/opt/ml/metadata/resource-metadata.json', 'r'))\n",
    "domain_id          = sagemaker_metadata['DomainId']\n",
    "resource_name      = sagemaker_metadata['ResourceName']\n",
    "\n",
    "print(f'http://{domain_id}.studio.{region}.sagemaker.aws/jupyter/default/proxy/8501/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1d90c-08b2-4678-8cbd-68640203229e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a726b9da-b824-4fe9-9b7c-2ef52fd2bfac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "\n",
    "config = {\n",
    "    \"region\": region,\n",
    "    \"datastores\":{\n",
    "        \"local\": \"./\"\n",
    "    },\n",
    "    \"models_text\":[\n",
    "        {'name':'Claude 3 Sonnet', 'endpoint':'anthropic.claude-3-sonnet-20240229-v1:0', 'source':'bedrock', 'style':'message', 'version':'bedrock-2023-05-31'},\n",
    "        {'name':'Claude 3 Haiku', 'endpoint':'anthropic.claude-3-haiku-20240307-v1:0', 'source':'bedrock', 'style':'message', 'version':'bedrock-2023-05-31'},\n",
    "        {'name':'LLAMA 3 70B', 'endpoint':'meta.llama3-70b-instruct-v1:0', 'source':'bedrock', 'style':'completion', 'version':''},\n",
    "        {'name':'LLAMA 3 8B', 'endpoint':'meta.llama3-8b-instruct-v1:0', 'source':'bedrock', 'style':'completion', 'version':''},\n",
    "        {'name':'Mistral Large', 'endpoint':'mistral.mistral-large-2402-v1:0', 'source':'bedrock', 'style':'completion', 'version':''},\n",
    "    ],\n",
    "    \"categories\": ['Finance','Healthcare','Media','Manufacturing'],\n",
    "    \"modality\": ['Text','Image','Embedding','Code']\n",
    "}\n",
    "json.dump(config, open('config_prompt_library.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92ce50-643d-40f6-81a4-f6de2d3d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Streamlit UI\n",
    "This defines the streamlit code. See the main function for the code loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1deea9a0-54d8-4dc0-8643-644756adcfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_prompt_library_ui.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_prompt_library.py\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "from botocore.config import Config\n",
    "st.set_page_config(layout=\"wide\", page_title=\"Prompt Playground\")\n",
    "\n",
    "\n",
    "def initialize_session(filepath):\n",
    "    \"\"\"Initializes session state variables for chat history, context, and cost.\"\"\"\n",
    "    st.session_state['config'] = get_config(filepath)\n",
    "    st.session_state[\"prompt\"] = \"\"\n",
    "    st.session_state['prompt_id'] = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(6))\n",
    "    st.session_state['name'] = \"\"\n",
    "    st.session_state['description'] = \"\"\n",
    "    st.session_state['modality'] = []\n",
    "    st.session_state['category'] = []\n",
    "    st.session_state[\"variables\"] = []\n",
    "    st.session_state['test_payloads'] = [None, None, None]\n",
    "    st.session_state['test_responses'] = [{}, {}, {}]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_config(filepath):\n",
    "    \"\"\"Loads configuration data from a JSON file including model endpoints, vector database indexes, AWS region, etc.\"\"\"\n",
    "    config = json.load(open(filepath, 'r'))\n",
    "    region = config['region']\n",
    "    diagram = config['arch_diagram']\n",
    "    \n",
    "    models_text = config['models_text']\n",
    "    datastore = config['datastores']['local']\n",
    "    categories = config['categories']\n",
    "    modality = config['modality']\n",
    "    \n",
    "    bedrock = boto3.client(service_name='bedrock-runtime', region_name=region, config=Config(read_timeout=240))\n",
    "    config = {\n",
    "        'models_text': models_text,\n",
    "        'region': region,\n",
    "        'datastore': datastore,\n",
    "        'bedrock': bedrock,\n",
    "        'diagram': diagram,\n",
    "        'modality': modality,\n",
    "        'categories': categories\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    client = payload['client']\n",
    "    model = payload['model']\n",
    "    history = payload['history']\n",
    "    system = payload['system']\n",
    "    \n",
    "    if 'claude-3' in model['endpoint']:\n",
    "        if 'message' in model['style']:\n",
    "            messages = []\n",
    "            for msg in history:\n",
    "                messages += [{'role': 'user', 'content': [{'type': 'text', 'text': msg['prompt']}]}]\n",
    "                messages += [{'role': 'assistant', 'content': [{'type': 'text', 'text': msg['response']}]}]\n",
    "            for i, msg in enumerate(payload['prompt']):\n",
    "                if i%2 == 0:\n",
    "                    messages += [{'role': 'user', 'content': [{'type': 'text', 'text': msg}]}]\n",
    "                else:\n",
    "                    messages += [{'role': 'assistant', 'content': [{'type': 'text', 'text': msg}]}]\n",
    "            body = json.dumps({\n",
    "                \"messages\": messages,\n",
    "                'system': system,\n",
    "                \"anthropic_version\": model['version'],\n",
    "                \"max_tokens\": payload['max_len'],\n",
    "                \"temperature\": payload['temp'],\n",
    "                \"top_p\": payload['top_p']\n",
    "                })\n",
    "            print(messages)\n",
    "            response, attempts, time_total = call_bedrock(client, body, model['endpoint'])\n",
    "            try:\n",
    "                response_body = json.loads(response.get(\"body\").read()).get(\"content\")[0]['text']\n",
    "            except:\n",
    "                response_body = '**Failed to generate!**'\n",
    "    elif 'llama3' in model['endpoint']:\n",
    "        if len(system) > 0:\n",
    "            system = f'<|start_header_id|>system<|end_header_id|>\\n{system}<|eot_id|>\\n'\n",
    "        prompt = f'<|begin_of_text|>{system}'\n",
    "        for msg in history:\n",
    "            prompt += f'<|start_header_id|>user<|end_header_id|>\\n{msg[\"prompt\"]}<|eot_id|>\\n'\n",
    "            prompt += f'<|start_header_id|>assistant<|end_header_id|>\\n{msg[\"response\"]}<|eot_id|>\\n'\n",
    "        for i, msg in enumerate(payload['prompt']):\n",
    "            if i%2 == 0:\n",
    "                prompt += f'<|start_header_id|>user<|end_header_id|>\\n{msg}<|eot_id|>\\n'\n",
    "            elif (i%2 == 1) and (i < len(payload['prompt'])-1):\n",
    "                prompt += f'<|start_header_id|>assistant<|end_header_id|>\\n{msg}<|eot_id|>\\n'\n",
    "            else:\n",
    "                prompt += f'<|start_header_id|>assistant<|end_header_id|>\\n{msg}'\n",
    "        if len(payload['prompt'])%2 == 1:\n",
    "            prompt += f'<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_gen_len\": int(min([payload['max_len'], 2048])),\n",
    "            \"temperature\": payload['temp'],\n",
    "            'top_p':payload['top_p']\n",
    "        })\n",
    "        print(prompt)\n",
    "        response, attempts, time_total = call_bedrock(client, body, model['endpoint'])\n",
    "        try:\n",
    "            response_body = json.loads(response.get('body').read().decode('utf-8'))['generation']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            response_body = '**Failed to generate!**'\n",
    "    elif 'mistral' in model['endpoint']:\n",
    "        if len(system) > 0:\n",
    "            system = f'<<SYS>>{system}<</SYS>>'\n",
    "        prompt = f'<s>[INST]{system}'\n",
    "        for msg in history:\n",
    "            prompt += f'{msg[\"prompt\"]}[/INST]'\n",
    "            prompt += f'{msg[\"response\"]}</s><s>[INST]'\n",
    "        for i, msg in enumerate(payload['prompt']):\n",
    "            if i%2 == 0:\n",
    "                prompt += f'{msg}[/INST]'\n",
    "            elif (i%2 == 1) and (i < len(payload['prompt'])-1):\n",
    "                prompt += f'{msg}</s><s>[INST]'\n",
    "            else:\n",
    "                prompt += f'{msg}'\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": payload['max_len'],\n",
    "            \"temperature\": payload['temp'],\n",
    "            'top_p': payload['top_p']\n",
    "        })\n",
    "        print(prompt)\n",
    "        response, attempts, time_total = call_bedrock(client, body, model['endpoint'])\n",
    "        try:\n",
    "            response_body = json.loads(response.get('body').read().decode('utf-8'))['outputs'][0]['text']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            response_body = '**Failed to generate!**'\n",
    "    tokens = {'tokens_in': response.get('ResponseMetadata').get('HTTPHeaders').get('x-amzn-bedrock-input-token-count'), 'tokens_out':response.get('ResponseMetadata').get('HTTPHeaders').get('x-amzn-bedrock-output-token-count')}\n",
    "    latency = response.get('ResponseMetadata').get('HTTPHeaders').get('x-amzn-bedrock-invocation-latency')\n",
    "    return (response_body, attempts, time_total, tokens, latency)\n",
    "\n",
    "\n",
    "def call_bedrock(client, body, endpoint, attempts=5, accept='application/json', contentType='application/json'):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            tic = time.time()\n",
    "            response = client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=endpoint,\n",
    "                accept=accept,\n",
    "                contentType=contentType\n",
    "            )\n",
    "            toc = time.time()\n",
    "            return response, i+1, toc-tic\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(2 + np.random.rand()/2.)\n",
    "            continue\n",
    "    return None, i+1, 0.\n",
    "\n",
    "\n",
    "def test_prompt(text):\n",
    "    def format_json(data):\n",
    "        prompt = []\n",
    "        max_length = max(len(data[\"user\"]), len(data[\"assistant\"]))\n",
    "        for i in range(max_length):\n",
    "            if i < len(data[\"user\"]):\n",
    "                prompt.append(data[\"user\"][i])\n",
    "            if i < len(data[\"assistant\"]):\n",
    "                prompt.append(data[\"assistant\"][i])\n",
    "        output_json = {\n",
    "            \"system\": data[\"system\"],\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        return output_json\n",
    "    \n",
    "    for idx, payload in enumerate(st.session_state['test_payloads']):\n",
    "        text = fill_variables(text)\n",
    "        json_prompt = prompt_to_json(text)\n",
    "        json_prompt = format_json(json_prompt)\n",
    "        payload['prompt'] = json_prompt['prompt']\n",
    "        payload['system'] = json_prompt['system']\n",
    "        (response_body, attempts, time_total, tokens, latency) = query_endpoint(payload)\n",
    "        st.session_state['test_responses'][idx] = {'response':response_body, 'latency':latency, 'tokens':tokens}\n",
    "\n",
    "\n",
    "def fill_variables(text):\n",
    "    for var in st.session_state['variables']:\n",
    "        text = text.replace('{{'+var['var']+'}}', var['var_ex'])\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_prompt():\n",
    "    prompt_data = {\n",
    "        \"prompt_id\": st.session_state['prompt_id'],\n",
    "        \"prompt_name\": st.session_state['name'],\n",
    "        \"prompt_text\": st.session_state[\"prompt\"],\n",
    "        \"prompt_formatted\": prompt_to_json(st.session_state[\"prompt\"]),\n",
    "        \"description\": st.session_state['description'],\n",
    "        \"modality\": st.session_state['modality'],\n",
    "        \"category\": st.session_state['category'],\n",
    "        \"variables\": st.session_state[\"variables\"],\n",
    "        \"test_results\": st.session_state['test_responses']\n",
    "    }\n",
    "    json_data = json.dumps(prompt_data, indent=4)\n",
    "    return json_data\n",
    "\n",
    "@st.cache_data\n",
    "def set_metadata(name, description, modality, category):\n",
    "    st.session_state['name'] = name \n",
    "    st.session_state['description'] = description\n",
    "    st.session_state['modality'] = modality\n",
    "    st.session_state['category'] = category\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def set_payload(_client, model, max_len, temp, loc):\n",
    "    payload = {\n",
    "        'prompt':'',\n",
    "        'history':[],\n",
    "        'system': '',\n",
    "        'max_len':max_len,\n",
    "        'temp':temp,\n",
    "        'top_p':.9,\n",
    "        'model':model,\n",
    "        'client':_client\n",
    "    }\n",
    "    st.session_state['test_payloads'][loc-1] = payload\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def prompt_to_json(prompt):\n",
    "    system_text = re.search(r'<<system>>(.*?)(<<|$)', prompt, re.DOTALL)\n",
    "    system_text = system_text.group(1).strip() if system_text else \"\"\n",
    "\n",
    "    user_texts = re.findall(r'<<user>>(.*?)(<<|$)', prompt, re.DOTALL)\n",
    "    user_texts = [t.strip() for t, _ in user_texts]\n",
    "\n",
    "    assistant_texts = re.findall(r'<<assistant>>(.*?)(<<|$)', prompt, re.DOTALL)\n",
    "    assistant_texts = [t.strip() for t, _ in assistant_texts]\n",
    "\n",
    "    if not user_texts and not system_text and not assistant_texts:\n",
    "        user_texts = [prompt.strip()]\n",
    "\n",
    "    json_data = {\n",
    "        \"system\": system_text,\n",
    "        \"user\": user_texts,\n",
    "        \"assistant\": assistant_texts\n",
    "    }\n",
    "    return json_data\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def set_prompt(prompt):\n",
    "    st.session_state[\"prompt\"] = prompt\n",
    "\n",
    "\n",
    "def set_variables(variables):\n",
    "    st.session_state[\"variables\"] = variables\n",
    "\n",
    "\n",
    "def main(config=None):\n",
    "    st.title(\"Prompt Developer\")\n",
    "    st.subheader(\"Build your prompt library for production\")\n",
    "    \n",
    "    with st.container(border=True):\n",
    "        col1,col2 = st.columns(2)\n",
    "        with col1:\n",
    "            prompt = st.text_area(\"Prompt Draft\", value=\"\", height=400)\n",
    "            set_prompt(prompt)\n",
    "            btn1,btn2,btn3 = st.columns(3)\n",
    "            with btn1:\n",
    "                if st.button(\"Test Prompt\"):\n",
    "                    test_prompt(st.session_state[\"prompt\"])\n",
    "            with btn2:\n",
    "                if st.download_button(\"Save Prompt\", save_prompt(), file_name='prompt_'+st.session_state['prompt_id']+'.json'):\n",
    "                    pass\n",
    "            with btn3:\n",
    "                if st.button(\"New Prompt\"):\n",
    "                    st.session_state['prompt_id'] = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(6))\n",
    "        \n",
    "        with col2:\n",
    "            st.write(f\"**Prompt ID:** {st.session_state['prompt_id']}\")\n",
    "            name = st.text_input(\"Prompt Name\", \"\")\n",
    "            description = st.text_area(\"Description\", value=\"\", height=25)\n",
    "            modality = st.multiselect(\"Modality\", config['modality'])\n",
    "            category = st.multiselect(\"Category\", config['categories'])\n",
    "            set_metadata(name, description, modality, category)\n",
    "    \n",
    "    with st.container(border=True):\n",
    "        variables = list(set(list(re.findall('{{\\s*([^}]+)\\s*}}', st.session_state[\"prompt\"]))))\n",
    "        var_store = []\n",
    "        \n",
    "        if len(variables) > 0:\n",
    "            num_cols = min(3, len(variables))\n",
    "            cols = st.columns(num_cols)\n",
    "            col_idx = 0\n",
    "            for i, var in enumerate(variables):\n",
    "                with cols[col_idx]:\n",
    "                    st.write(f\"**{var}**\")\n",
    "                    var_desc = st.text_input(\"Variable Description\", \"\", key=(var + \"1\"))\n",
    "                    var_ex = st.text_input(\"Test Example\", \"\", key=(var + \"2\"))\n",
    "                    st.write(\"---\")\n",
    "                    var_store.append({\"var\": var, \"var_desc\": var_desc, \"var_ex\": var_ex})\n",
    "                col_idx = (col_idx + 1) % num_cols\n",
    "            set_variables(var_store)\n",
    "\n",
    "            if len(variables) > 9:\n",
    "                st.warning(\"Only the first 9 variables are displayed due to space constraints.\")\n",
    "    \n",
    "    with st.container(border=True):\n",
    "        col1,col2,col3 = st.columns(3)\n",
    "        with col1:\n",
    "            model_text = st.selectbox('Model', [m['name'] for m in config['models_text']], key='col11')\n",
    "            max_len_text = st.number_input('Max Generation Length', 100, 10000, 300, 100, key='col12') \n",
    "            temp_text = st.slider('Temperature', 0.01, 1., 0.01, .01, key='col13')\n",
    "            set_payload(config['bedrock'], [m for m in config['models_text'] if m['name'] == model_text][0], max_len_text, temp_text, 1)\n",
    "            msg = st.session_state['test_responses'][0]\n",
    "            if len(msg) > 0:\n",
    "                st.write('**Latency:** ' + str(int(msg['latency'])/1000) + 's')\n",
    "                st.write('**Tokens in:** ' + str(msg['tokens']['tokens_in']))\n",
    "                st.write('**Tokens out:** ' + str(msg['tokens']['tokens_out']))\n",
    "                st.write(msg['response'])\n",
    "        with col2:\n",
    "            model_text = st.selectbox('Model', [m['name'] for m in config['models_text']], key='col21')\n",
    "            max_len_text = st.number_input('Max Generation Length', 100, 10000, 300, 100, key='col22') \n",
    "            temp_text = st.slider('Temperature', 0.01, 1., 0.01, .01, key='col23')\n",
    "            set_payload(config['bedrock'], [m for m in config['models_text'] if m['name'] == model_text][0], max_len_text, temp_text, 2)\n",
    "            msg = st.session_state['test_responses'][1]\n",
    "            if len(msg) > 0:\n",
    "                st.write('**Latency:** ' + str(int(msg['latency'])/1000) + 's')\n",
    "                st.write('**Tokens in:** ' + str(msg['tokens']['tokens_in']))\n",
    "                st.write('**Tokens out:** ' + str(msg['tokens']['tokens_out']))\n",
    "                st.write(msg['response'])\n",
    "        with col3:\n",
    "            model_text = st.selectbox('Model', [m['name'] for m in config['models_text']], key='col31')\n",
    "            max_len_text = st.number_input('Max Generation Length', 100, 10000, 300, 100, key='col32') \n",
    "            temp_text = st.slider('Temperature', 0.01, 1., 0.01, .01, key='col33')\n",
    "            set_payload(config['bedrock'], [m for m in config['models_text'] if m['name'] == model_text][0], max_len_text, temp_text, 3)\n",
    "            msg = st.session_state['test_responses'][2]\n",
    "            if len(msg) > 0:\n",
    "                st.write('**Latency:** ' + str(int(msg['latency'])/1000) + 's')\n",
    "                st.write('**Tokens in:** ' + str(msg['tokens']['tokens_in']))\n",
    "                st.write('**Tokens out:** ' + str(msg['tokens']['tokens_out']))\n",
    "                st.write(msg['response'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FILEPATH = './config_prompt_library.json'\n",
    "    if 'config' not in st.session_state:\n",
    "        initialize_session(FILEPATH)\n",
    "    #config = sidebar(st.session_state['config'])\n",
    "    config = st.session_state['config']\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadfb43-f899-407f-ad31-1595ca84246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6772f-9a6e-4b42-ac9e-b64ef21df3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
