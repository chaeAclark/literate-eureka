{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f43b4802-c249-45b3-8f01-927b7626094d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67891b32-fc41-4d10-869a-6cc9642a7923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_tags(endpoint):\n",
    "    if 'claude' in endpoint:\n",
    "        human_tag = '\\n\\nHuman:'\n",
    "        robot_tag = '\\n\\nAssistant:'\n",
    "    elif 'ai21.j2' in endpoint:\n",
    "        human_tag = '\\n##\\n'\n",
    "        robot_tag = ''\n",
    "    elif 'titan' in endpoint:\n",
    "        human_tag = '\\n\\nUser:'\n",
    "        robot_tag = '\\n\\nBot:'\n",
    "    else:\n",
    "        human_tag = '\\n\\n'\n",
    "        robot_tag = ''\n",
    "    return human_tag, robot_tag\n",
    "\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    client = payload['client']\n",
    "    endpoint = payload['endpoint']\n",
    "    if 'titan' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'inputText':payload['prompt'],\n",
    "            'textGenerationConfig':{\n",
    "                'maxTokenCount':payload['max_len'],\n",
    "                'temperature':payload['temp'],\n",
    "                'topP':payload['top_p'],\n",
    "        }})\n",
    "        response, attempts, gen_time = call_bedrock(client, body, endpoint)\n",
    "        try:\n",
    "            response_body = json.loads(response.get('body').read()).get('results')[0].get('outputText')\n",
    "        except:\n",
    "            response_body = '**Failed to generate!**'\n",
    "    elif 'claude' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'prompt':payload['prompt'],\n",
    "            'max_tokens_to_sample':payload['max_len'],\n",
    "            'temperature':payload['temp'],\n",
    "            'top_p':payload['top_p'],\n",
    "        })\n",
    "        response, attempts, gen_time = call_bedrock(client, body, endpoint)\n",
    "        try:\n",
    "            response_body = json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "        except:\n",
    "            response_body = '**Failed to generate!**'\n",
    "    elif 'ai21.j2' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'prompt':payload['prompt'],\n",
    "            'maxTokens':payload['max_len'],\n",
    "            'temperature':payload['temp'],\n",
    "            'topP':payload['top_p'],\n",
    "            'stopSequences':['##']\n",
    "        })\n",
    "        response, attempts, gen_time = call_bedrock(client, body, endpoint)\n",
    "        try:\n",
    "            response_body = json.loads(response.get(\"body\").read()).get(\"completions\")[0].get(\"data\").get(\"text\")\n",
    "        except:\n",
    "            response_body = '**Failed to generate!**'\n",
    "    return (response_body, attempts, gen_time)\n",
    "\n",
    "\n",
    "def call_bedrock(client, body, endpoint, attempts=45, accept='application/json', contentType='application/json'):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            tic = time.time()\n",
    "            response = client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=endpoint,\n",
    "                accept=accept,\n",
    "                contentType=contentType\n",
    "            )\n",
    "            toc = time.time()\n",
    "            return response, i+1, toc-tic\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(2 + np.random.rand()/2.)\n",
    "            continue\n",
    "    return None, i+1, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90a6c44-9738-42fc-920a-4338409a58e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=240)\n",
    "client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1', config=config)\n",
    "endpoint = 'anthropic.claude-v2:1'\n",
    "payload = {\n",
    "    'prompt': '',\n",
    "    'max_len': 9000,\n",
    "    'temp': 0.75,\n",
    "    'top_p': .99,\n",
    "    'endpoint': endpoint,\n",
    "    'client': client,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106788a1-ef85-4f47-a27c-2886fb3965c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_dir = 'papers'\n",
    "filepaths = [in_dir+'/'+f for f in os.listdir(\"papers\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c47ed3-aea9-4f11-afd1-24a02d5f14ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers/2312.00752.txt\n",
      "2697\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"You are an AI Researcher whose goal is to analyze articles and produce robust and understandable Python code examples.\\n\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and implement a well formatted and well documented code example,\n",
    "that impliments the models and/or algorithms precented in the paper. This can use sample data, but the explanation should be clear in the code:\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, implement a well formatted and well documented code example in <code></code> tags,\n",
    "that impliments the models and/or algorithms precented in the paper. This can use sample data, but the explanation should be clear in the code.\\n\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing code:\n",
    "1. Read the paper thoroughly and make sure you understand the key concepts. Don't just skim it and try to code. Take notes, draw diagrams, explain ideas out loud to yourself or others. Comprehension is key before attempting implementation.\n",
    "2. Identify the key components that need to be coded - the models, algorithms, data preprocessing steps, etc. Break the implementation down into smaller modular pieces.\n",
    "3. Find or create appropriate data to test the models on. Many papers include links to data or code repositories used. If not, try to create or find representative sample data.\n",
    "4. Start by hard-coding and verifying the simplest pieces first. For example, if implementing a complex transformer model, first recreate the multi-headed self-attention mechanism and test it independently before adding other components.\n",
    "5. Adhere to sound coding principles - modularize code into functions and classes, add comments explaining the purpose of areas of code, use descriptive variable names matching paper terminology. These practices will help ensure clear understanding.\n",
    "6. Visualize and print intermediate outputs to check that data transforms, model internal states, etc are as expected per paper descriptions to methodically validate implementation components.  \n",
    "7. Once pieces are individually working, incrementally connect and test them together until the full model or algorithm is constructed. Confirm outputs match expected behavior from paper either mathematically or qualitatively at each step.\n",
    "8. Write tests and experiment with model variations. Try ablations by removing components and quantify differences observed. Push and analyze limits of model capacity using larger data. These tests further validate understanding and highlight subtleties.\n",
    "\n",
    "In summary, read critically, decompose implementations into verifiable steps, validate incrementally against paper, and test rigorously. This systematic process can reliably translate research ideas into reproducible code.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"<code>\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee0514b-f6bb-4740-9147-8da8f76a59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15b986a-dbd2-44fd-a80d-5621c777d380",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import math\n",
      "\n",
      "class SelectiveSSMLayer(nn.Module):\n",
      "    \"\"\"\n",
      "    Selective State Space Model (S6) Layer\n",
      "    Implements the core algorithm from Section 3.3\n",
      "    \n",
      "    Key Components:\n",
      "    - Input-dependent SSM parameters A, B, C \n",
      "    - Efficient parallel scan implementation with kernel fusion and recomputation\n",
      "    \n",
      "    Args:\n",
      "        input_size: Input dimension\n",
      "        state_size: Latent SSM state dimension\n",
      "        discretize_fn: Discretization function \n",
      "        activation_fn: Activation function for A parameter\n",
      "        \n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, input_size, state_size, \n",
      "                 discretize_fn, activation_fn):\n",
      "        super().__init__()\n",
      "        \n",
      "        self.input_proj = nn.Linear(input_size, state_size) \n",
      "        self.state_proj = nn.Linear(input_size, state_size)\n",
      "        self.output_proj = nn.Linear(state_size, input_size)\n",
      "        \n",
      "        self.discretize_fn = discretize_fn\n",
      "        self.activation_fn = activation_fn\n",
      "        \n",
      "        self.register_buffer('A', torch.randn(state_size)) \n",
      "        \n",
      "    def forward(self, x):\n",
      "        \n",
      "        B = self.input_proj(x) \n",
      "        C = self.output_proj(x)\n",
      "        A = self.activation_fn(self.state_proj(x)) + self.A\n",
      "        \n",
      "        A_d, B_d = self.discretize_fn(A, B) \n",
      "        \n",
      "        # Efficient scan implementation\n",
      "        y = scan(A_d, B_d, C, x.shape)  \n",
      "        \n",
      "        return y\n",
      "\n",
      "    \n",
      "def scan(A, B, C, shape):\n",
      "\n",
      "    \"\"\"\n",
      "    Hardware-aware parallel scan implementation\n",
      "    Fuses discretization, scan, output multiplication\n",
      "    \n",
      "    Args:\n",
      "        A, B, C: SSM parameters\n",
      "        shape: Input shape (batch_size, seq_len, input_size) \n",
      "        \n",
      "    Returns:\n",
      "        Output sequence\n",
      "    \"\"\"\n",
      "\n",
      "    # Algorithms from Section 3.3\n",
      "    # SRAM computations, recomputation, etc\n",
      "    \n",
      "    return output\n",
      "    \n",
      "    \n",
      "class MambaBlock(nn.Module):\n",
      "    \"\"\"\n",
      "    Mamba Block\n",
      "    Simplified SSM architecture block, Figure 3\n",
      "    \n",
      "    Args:\n",
      "        input_size: Input dimension\n",
      "        expand_factor: Inner dimension expansion factor \n",
      "        ssm_size: SSM state size\n",
      "        discretize_fn: Discretization function\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, input_size, expand_factor, ssm_size, \n",
      "                 discretize_fn):\n",
      "        \n",
      "        super().__init__()\n",
      "        \n",
      "        self.proj_in = nn.Linear(input_size, input_size*expand_factor)\n",
      "        self.act = nn.SiLU() \n",
      "        self.ssm = SelectiveSSMLayer(input_size*expand_factor, \n",
      "                                     ssm_size, discretize_fn)\n",
      "        self.proj_out = nn.Linear(input_size*expand_factor, \n",
      "                                  input_size)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        \n",
      "        x = self.proj_in(x)\n",
      "        x = self.act(x)\n",
      "        x = self.ssm(x) \n",
      "        x = self.proj_out(x)\n",
      "        \n",
      "        return x\n",
      "\n",
      "</code>\n",
      "\n",
      "This implements the core Selective SSM Layer from Section 3 of the paper, including the efficient parallel scan algorithm, as well as the simplified Mamba Block architecture.\n",
      "\n",
      "Key points:\n",
      "\n",
      "- The SelectiveSSMLayer has input-dependent projections to compute the SSM parameters A, B, C\n",
      "- The forward pass applies the discretization function and then performs the efficient scan implementation\n",
      "- The scan function fuses computations like discretization and output multiplication into the parallel scan kernel to optimize speed and memory \n",
      "\n",
      "- The MambaBlock wraps the SelectiveSSMLayer with linear projections and activations similar to the Transformer MLP block\n",
      "- Expanding the dimension before the SSM layer increases the state size for better modeling capacity\n",
      "\n",
      "To build the full Mamba model, MambaBlocks would be stacked, interleaved with residuals, normalization etc. as described in Section 3.4.\n",
      "\n",
      "The code demonstrates core ideas like selection mechanisms and state expansion for sequence modeling, using SSM theory and hardware-aware optimizations. Modular functions help understand and validate pieces incrementally.\n"
     ]
    }
   ],
   "source": [
    "print(responses[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe5ac48-0ded-435d-bdd3-a44360d1f5f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Import necessary libraries\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from cvxpy import *\n",
      "\n",
      "# Set random seed for reproducibility\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate a random frame in R^2 with M vectors\n",
      "N = 2\n",
      "M = 7  \n",
      "Phi = np.random.randn(N, M) \n",
      "Phi /= np.linalg.norm(Phi, axis=0) # Normalize columns to unit norm\n",
      "\n",
      "# Define frame analysis operator\n",
      "def analysis_op(Phi):\n",
      "    return Phi.T @ Phi\n",
      "\n",
      "# Objective function to minimize \n",
      "u = Variable(M)\n",
      "objective = Minimize(sum_entries(u))\n",
      "\n",
      "# Constraints \n",
      "constraints = [analysis_op(diag(u) * Phi) == Identity(N), \n",
      "               norm(u,1) == 1,\n",
      "               u >= 0]\n",
      "\n",
      "# Solve optimization problem\n",
      "prob = Problem(objective, constraints)\n",
      "result = prob.solve()\n",
      "\n",
      "# Extract scaling values  \n",
      "scaling_vals = np.sqrt(u.value)  \n",
      "\n",
      "# Plot original and scaled frames\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(Phi[0,:], Phi[1,:], color='blue')  \n",
      "ax.scatter(diag(scaling_vals) @ Phi[0,:], \n",
      "           diag(scaling_vals) @ Phi[1,:], color='red')\n",
      "ax.set_aspect('equal')\n",
      "ax.set_title('Original Frame Vectors (blue) and Scaled Frame Vectors (red)')\n",
      "plt.show()\n",
      "\n",
      "</code>\n",
      "\n",
      "In this example, I:\n",
      "\n",
      "1. Generated a random frame Phi in R^2 with M=7 vectors \n",
      "2. Defined the frame analysis operator to enforce tight frame constraint\n",
      "3. Created optimization variables and objective function to minimize l1 norm \n",
      "4. Added constraints for tight frame and vector norm\n",
      "5. Solved the optimization problem to find scaling values\n",
      "6. Extracted the scaling values and plotted original and scaled frames\n",
      "\n",
      "This shows a basic implementation of the scalable frames optimization problem formulation. Some ways to extend this:\n",
      "\n",
      "- Vary frame size M and dimension N \n",
      "- Experiment with different objective functions \n",
      "- Implement optimization with augmented Lagrangian method\n",
      "- Add testing and validation checks at each step\n",
      "- Quantify differences between original and scaled frames\n",
      "\n",
      "The key is to start simple and incrementally build up the implementation while verifying correctness. Modularization and validation at each step is critical.\n"
     ]
    }
   ],
   "source": [
    "print(responses[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "234d8152-5ed1-488a-bfea-628cb66f7f28",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import numpy as np\n",
      "import random\n",
      "\n",
      "# Game of 24 Task\n",
      "# Input is 4 numbers, output is an equation that uses all numbers to equal 24 \n",
      "def game_of_24(numbers):\n",
      "\n",
      "    numbers = sorted(numbers)\n",
      "    \n",
      "    # Thought generator - generate possible intermediate equation steps \n",
      "    def generate_thought(state):\n",
      "        left_numbers = state[0]\n",
      "        thoughts = []\n",
      "        \n",
      "        for i in range(len(left_numbers)-1):\n",
      "            n1 = left_numbers[i]\n",
      "            for j in range(i+1, len(left_numbers)):\n",
      "                n2 = left_numbers[j]\n",
      "                \n",
      "                # Try all pairs of numbers\n",
      "                thoughts.append(f\"{n1}+{n2}=\") \n",
      "                thoughts.append(f\"{n1}-{n2}=\")\n",
      "                thoughts.append(f\"{n1}*{n2}=\")\n",
      "                if n2 != 0:\n",
      "                    thoughts.append(f\"{n1}/{n2}=\")\n",
      "                    \n",
      "        random.shuffle(thoughts)\n",
      "        \n",
      "        return thoughts[:5] # Top 5 options\n",
      "    \n",
      "    # State evaluator \n",
      "    def evaluate_state(state):\n",
      "        left_numbers = state[0]\n",
      "        eqn = state[1]\n",
      "        \n",
      "        if eval(eqn) == 24:\n",
      "            return \"sure\"\n",
      "        elif sum(left_numbers) > 100: \n",
      "            return \"impossible\"\n",
      "        else:\n",
      "            return \"maybe\"\n",
      "        \n",
      "    # Breadth-first search  \n",
      "    queue = [(numbers,\"\")] \n",
      "    for _ in range(3): \n",
      "        next_queue = []\n",
      "        for state in queue:\n",
      "            for thought in generate_thought(state):\n",
      "                new_state = do_arith(state, thought) # Apply equation \n",
      "                if new_state is not None:\n",
      "                    val = evaluate_state(new_state)\n",
      "                    if val == \"sure\":\n",
      "                        return new_state[1]\n",
      "                    elif val == \"maybe\":\n",
      "                        next_queue.append(new_state)\n",
      "        queue = next_queue[:5] # Top 5\n",
      "        \n",
      "    return None\n",
      "    \n",
      "def do_arith(state, thought):\n",
      "    left_numbers, eqn = state\n",
      "    nums = [int(x) for x in left_numbers]\n",
      "    thought = thought.strip()\n",
      "    \n",
      "    # Apply arithmetic \n",
      "    if thought.endswith(\"+\"):\n",
      "        n1,n2 = nums[:2] \n",
      "        out = n1+n2\n",
      "    elif thought.endswith(\"-\"):\n",
      "        n1,n2 = nums[:2]\n",
      "        out = n1-n2\n",
      "    elif thought.endswith(\"*\"):\n",
      "        n1,n2 = nums[:2]\n",
      "        out = n1*n2\n",
      "    elif thought.endswith(\"/\"):\n",
      "        n1,n2 = nums[:2]\n",
      "        if n2 == 0: \n",
      "            return None\n",
      "        out = n1/n2\n",
      "    else:\n",
      "        return None\n",
      "    \n",
      "    new_numbers = [out] + nums[2:] if len(nums) > 2 else []\n",
      "    new_eqn = eqn + thought\n",
      "    \n",
      "    return (new_numbers, new_eqn)\n",
      "    \n",
      "if __name__ == \"__main__\":  \n",
      "    numbers = [2,3,10,9]\n",
      "    print(game_of_24(numbers))\n",
      "</code>\n",
      "\n",
      "This code implements the Tree of Thoughts algorithm for solving the Game of 24 problem from the paper. Here are some key points:\n",
      "\n",
      "1. The `generate_thought` function generates possible intermediate equation steps to try by considering pairwise combinations of the remaining numbers. It implements the thought generator component.\n",
      "\n",
      "2. The `evaluate_state` function checks if the current equation totals 24 (success), if the remaining numbers are too large to reachable 24 (impossible), or neither (maybe). This provides the state evaluation.\n",
      "\n",
      "3. We search with a simple breadth-first search, maintaining a queue of candidate states. At each step, we generate new states for each state, evaluate them, and keep the top 5 most promising to explore next.\n",
      "\n",
      "4. When applying a thought equation, `do_arith` appropriately updates the remaining numbers and full equation. It returns None if invalid (e.g. divide by 0). \n",
      "\n",
      "5. The overall pipeline integrates thought generation, state evaluation, and search to deliberately solve the Game of 24 by constructing an equation step-by-step.\n",
      "\n",
      "The code is modularized into reusable functions and uses a priority queue for search, achieving 74% success rate on novel games as shown empirically in the paper. It can be extended by trying more advanced search algorithms like Monte Carlo Tree Search.\n"
     ]
    }
   ],
   "source": [
    "print(responses[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d5aa8e-8d3a-4c4c-981d-41200760f64c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.nn import functional as F\n",
      "\n",
      "# Implementation of the LoRA (Low-Rank Adaptation) approach\n",
      "# for efficiently adapting large pre-trained language models \n",
      "# to downstream tasks\n",
      "\n",
      "class LoRA(nn.Module):\n",
      "    def __init__(self, pretrained_model, rank):\n",
      "        super().__init__()\n",
      "        self.pretrained_model = pretrained_model\n",
      "        self.rank = rank\n",
      "        \n",
      "        # Freeze parameters in pretrained model\n",
      "        for param in self.pretrained_model.parameters():\n",
      "            param.requires_grad = False\n",
      "            \n",
      "        # Add LoRA layers        \n",
      "        self.lora_layers = nn.ModuleList()\n",
      "        for layer in self.pretrained_model.transformer.h:\n",
      "            self.lora_layers.append(\n",
      "                LoRALayer(layer.attn.in_proj_weight, self.rank))\n",
      "            \n",
      "    def forward(self, x):\n",
      "        # Forward pass through pretrained model\n",
      "        x = self.pretrained_model(x)\n",
      "        \n",
      "        # Forward pass through each LoRA layer\n",
      "        for lora_layer in self.lora_layers:\n",
      "            x = lora_layer(x)\n",
      "        \n",
      "        return x\n",
      "\n",
      "class LoRALayer(nn.Module):\n",
      "    def __init__(self, weight_matrix, rank):\n",
      "        super().__init__()\n",
      "        d_model = weight_matrix.shape[0]\n",
      "        \n",
      "        # Learnable low-rank matrices\n",
      "        self.A = nn.Parameter(torch.randn(d_model, rank)) \n",
      "        self.B = nn.Parameter(torch.randn(d_model, rank))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        # Projection using fixed pretrained weights\n",
      "        h = x @ self.weight_matrix\n",
      "        \n",
      "        # Low-rank update\n",
      "        delta = self.A @ self.B\n",
      "        h = h + x @ delta\n",
      "        \n",
      "        return h\n",
      "        \n",
      "if __name__ == \"__main__\":\n",
      "    \n",
      "    # Sample usage\n",
      "    \n",
      "    import transformers\n",
      "    \n",
      "    # Load pretrained model\n",
      "    model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
      "    \n",
      "    # Create LoRA model\n",
      "    lora_model = LoRA(model, rank=4) \n",
      "    \n",
      "    # Freeze pretrained weights \n",
      "    for param in lora_model.parameters():\n",
      "        param.requires_grad = False\n",
      "        \n",
      "    # Train only LoRA parameters A and B\n",
      "    for lora_layer in lora_model.lora_layers:\n",
      "        for param in lora_layer.parameters():\n",
      "            param.requires_grad = True\n",
      "            \n",
      "    # Forward pass\n",
      "    input_ids = torch.tensor([[1,2,3]])\n",
      "    outputs = lora_model(input_ids)\n",
      "    \n",
      "</code>\n",
      "\n",
      "This code implements the core components of the LoRA approach from the paper for efficiently adapting a pretrained language model (here GPT-2 from HuggingFace Transformers).\n",
      "\n",
      "The main ideas:\n",
      "\n",
      "- Initialize a pretrained model and freeze parameters \n",
      "- Add a LoRALayer module after each self-attention layer. Each LoRALayer contains low-rank matrices A and B.\n",
      "- Forward pass applies the frozen pretrained weights first, then the low-rank update delta = A@B\n",
      "- Only A and B parameters are trainable, greatly reducing training costs.\n",
      "\n",
      "The code allows flexibility in terms of which specific layers get the LoRA treatment and the rank can be set as desired.\n",
      "\n",
      "Usage demo initializes GPT-2, wraps with LoRA module, then shows how to set requires_grad to only train the LoRA parameters. \n",
      "\n",
      "Additional code could implement the training/evaluation loops, hyperparameter tuning, etc. following details in paper.\n"
     ]
    }
   ],
   "source": [
    "print(responses[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786c3835-4470-44b7-91b9-ffd2e5a1086f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "\"\"\"\n",
      "Implementation of ReLoRA training algorithm from paper: \n",
      "\"ReLoRA: High-Rank Training Through Low-Rank Updates\"\n",
      "\n",
      "Key concepts:\n",
      "- Replace linear layers in model with Low Rank Adaptation (LoRA) layers \n",
      "- LoRA layers decompose weight matrix into low rank A and B matrices\n",
      "- Periodically merge A and B into original weight matrix W \n",
      "- Reset optimizer momentum and LR schedule at merge points\n",
      "- Over multiple merge cycles, aggregate low rank updates into high rank\n",
      "\n",
      "Advantages:\n",
      "- Memory efficient - fewer trainable parameters\n",
      "- Faster training - improved hardware efficiency\n",
      "\n",
      "Results:\n",
      "- Matches performance of full rank training\n",
      "- Saves GPU memory and improves training speed\n",
      "\"\"\"\n",
      "\n",
      "class LoRA(nn.Module):\n",
      "    \"\"\"\n",
      "    Low Rank Adaptation linear layer.\n",
      "    \n",
      "    Decomposes weight matrix into low rank A and B \n",
      "    with only B being trainable.\n",
      "    \"\"\"\n",
      "    def __init__(self, in_features, out_features, rank):\n",
      "        super().__init__()\n",
      "        self.rank = rank\n",
      "        \n",
      "        # Original frozen weight matrix\n",
      "        self.W = nn.Parameter(torch.randn(out_features, in_features))\n",
      "        \n",
      "        # Low rank matrices\n",
      "        self.A = nn.Parameter(torch.randn(out_features, rank)) \n",
      "        self.B = nn.Parameter(torch.zeros(rank, in_features))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        W = self.A @ self.B + self.W\n",
      "        return x @ W\n",
      "    \n",
      "class ReLoRAModel(nn.Module):\n",
      "    \"\"\"Sample transformer model with ReLoRA training\"\"\" \n",
      "    def __init__(self, n_layers, n_heads, hidden_size):\n",
      "        super().__init__()\n",
      "        self.layers = nn.ModuleList()\n",
      "        for _ in range(n_layers):\n",
      "            self.layers.append(nn.ModuleList([\n",
      "                LoRA(hidden_size, hidden_size, rank=128), # Self-attention\n",
      "                LoRA(hidden_size, hidden_size, rank=128), # MLP\n",
      "                nn.LayerNorm(hidden_size) \n",
      "            ]))\n",
      "    \n",
      "    def forward(self, x):\n",
      "        for self_attn, mlp, norm in self.layers:\n",
      "            x = self_attn(norm(x)) + x \n",
      "            x = mlp(norm(x)) + x\n",
      "        return x\n",
      "\n",
      "def train_epoch(model, data, optimizer):\n",
      "    \"\"\"Single training epoch\"\"\"\n",
      "    for x in data:\n",
      "        pred = model(x)\n",
      "        loss = pred.mean() # Dummy loss\n",
      "        loss.backward()\n",
      "        \n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "        \n",
      "def relora_training(model, data, lr):\n",
      "    \"\"\"ReLoRA training loop\"\"\"\n",
      "    opt = optim.Adam(model.parameters(), lr=lr)\n",
      "    \n",
      "    # Warm start\n",
      "    for _ in range(1000):\n",
      "        train_epoch(model, data, opt) \n",
      "    \n",
      "    # Reset learning rate  \n",
      "    opt = optim.Adam(model.parameters(), lr=1e-4)\n",
      "    \n",
      "    merge_cycles = 3\n",
      "    for cycle in range(merge_cycles):\n",
      "        \n",
      "        # Train with low rank adapter matrices\n",
      "        for step in range(5000):\n",
      "            train_epoch(model, data, opt)\n",
      "            \n",
      "        # Merge A and B into W  \n",
      "        for layer in model.layers:\n",
      "            for module in layer:\n",
      "                if isinstance(module, LoRA):\n",
      "                    W = module.A @ module.B + module.W\n",
      "                    module.W.data.copy_(W.data)  \n",
      "                    \n",
      "        # Reset optimizer\n",
      "        opt = optim.Adam(model.parameters(), lr=1e-4) \n",
      "        \n",
      "# Usage:\n",
      "model = ReLoRAModel()\n",
      "data = torch.randn(64, 512) \n",
      "relora_training(model, data, 1e-3)\n",
      "</code>\n",
      "\n",
      "In this implementation, the key components of ReLoRA training are:\n",
      "\n",
      "1. LoRA layer replaces linear layers to enable low rank decomposition\n",
      "2. Warm start pre-trains the full model \n",
      "3. Training loop alternates between low rank update steps and merge+reset cycles\n",
      "4. At merge, low rank A and B are aggregated into the original weight W\n",
      "5. Optimizer is reset to clear momentum and enable fresh directions\n",
      "\n",
      "The code shows the modularization of ReLoRA via custom layers and training functions. It implements the core concepts from the paper like low rank updates and periodic merge+resets.\n",
      "\n",
      "Some ways the code could be extended:\n",
      "- Add transformer encoder/decoder classes with attention\n",
      "- Create datasets and dataloader for language modeling\n",
      "- Implement full transformer model training loop \n",
      "- Add learning rate scheduler and jagged warmup\n",
      "- Scale model size from 60M to 1B parameters\n",
      "- Benchmark speed and memory gains\n",
      "\n",
      "But overall this shows a basic pattern for applying ReLoRA training to an existing model in PyTorch.\n"
     ]
    }
   ],
   "source": [
    "print(responses[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b009e482-76c3-49b8-a71c-851119c4dc69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "import re\n",
      "import difflib\n",
      "import spacy\n",
      "from datasets import load_dataset\n",
      "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "\n",
      "# Load gendered word lookup tables from paper\n",
      "pronouns = {\n",
      "    \"he\":\"they\", \n",
      "    \"she\":\"they\",\n",
      "    \"his\":\"their\", \n",
      "    \"her\":\"their\",\n",
      "    ...\n",
      "}\n",
      "\n",
      "nouns = {\n",
      "    \"chairman\":\"chairperson\",\n",
      "    \"congressman\":\"member of congress\",\n",
      "    ... \n",
      "}\n",
      "\n",
      "# Load and preprocess parallel data\n",
      "dataset = load_dataset(\"oscar\", split=\"train\")  \n",
      "dataset = filter_and_deduplicate(dataset)\n",
      "\n",
      "def tokenize(example):\n",
      "    return tokenizer(example[\"text\"])\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")  \n",
      "dataset = dataset.map(tokenize)\n",
      "\n",
      "# Create training examples with rule-based augmentation\n",
      "def forward_augment(example):\n",
      "    text = back_translate(example[\"text\"]) \n",
      "    for k,v in pronouns.items():\n",
      "        text = text.replace(k,v) \n",
      "    for k,v in nouns.items():\n",
      "        text = text.replace(k,v)\n",
      "    example[\"text\"] = text\n",
      "    return example\n",
      "\n",
      "augmented_dataset = dataset.map(forward_augment)\n",
      "\n",
      "# Train model   \n",
      "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      "model.train(augmented_dataset)\n",
      "\n",
      "# Inference\n",
      "text = \"The spokesman addressed the congressmen.\" \n",
      "text = tokenizer(text, return_tensors=\"pt\").input_ids  \n",
      "\n",
      "outputs = model.generate(text)\n",
      "print(tokenizer.decode(outputs[0]))\n",
      "# \"The spokesperson addressed the members of congress.\"\n",
      "\n",
      "</code>\n",
      "\n",
      "This code example loads gender-fair word lookup tables from the paper, uses them to augment parallel text data in a rule-based manner, trains a seq2seq model on this augmented data, and shows an inference example to rewrite a biased text input in a more gender-neutral way.\n",
      "\n",
      "It focuses on the Forward Augmentation data creation approach, preprocessing and augmentation steps, and training loop - key components needing implementation based on the paper.\n",
      "\n",
      "The code is structured into logical steps: data loading and preprocessing, augmentation, model definition and training, and inference. It attempts to name variables and functions based on terminology from paper, and includes comments explaining purpose of areas of code.\n",
      "\n",
      "Additional enhancements could include:\n",
      "- Implementation of Backward Augmentation and Roundtrip Augmentation approaches\n",
      "- More robust preprocessing and augmentation\n",
      "- Evaluation on test sets and metric computation \n",
      "- Experiments with model variations and sizes\n",
      "- Visualizations of outputs\n",
      "\n",
      "But this covers core algorithm implementation, validation, and testing based on the information provided in the paper.\n"
     ]
    }
   ],
   "source": [
    "print(responses[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38cf7d42-78cf-447b-903a-1d05a8c6d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"You are an AI Researcher whose goal is to analyze articles and produce robust and understandable Python code examples.\\n\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and implement a well formatted and well documented code example,\n",
    "that impliments the models and/or algorithms precented in the paper. This can use sample data, but the explanation should be clear in the code:\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, implement a well formatted and well documented code example in <code></code> tags,\n",
    "that impliments the models and/or algorithms precented in the paper. This can use sample data, but the explanation should be clear in the code.\\n\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing code:\n",
    "1. Read the paper thoroughly and make sure you understand the key concepts. Don't just skim it and try to code. Take notes, draw diagrams, explain ideas out loud to yourself or others. Comprehension is key before attempting implementation.\n",
    "2. Identify the key components that need to be coded - the models, algorithms, data preprocessing steps, etc. Break the implementation down into smaller modular pieces.\n",
    "3. Find or create appropriate data to test the models on. Many papers include links to data or code repositories used. If not, try to create or find representative sample data.\n",
    "4. Start by hard-coding and verifying the simplest pieces first. For example, if implementing a complex transformer model, first recreate the multi-headed self-attention mechanism and test it independently before adding other components.\n",
    "5. Adhere to sound coding principles - modularize code into functions and classes, add comments explaining the purpose of areas of code, use descriptive variable names matching paper terminology. These practices will help ensure clear understanding.\n",
    "6. Visualize and print intermediate outputs to check that data transforms, model internal states, etc are as expected per paper descriptions to methodically validate implementation components.  \n",
    "7. Once pieces are individually working, incrementally connect and test them together until the full model or algorithm is constructed. Confirm outputs match expected behavior from paper either mathematically or qualitatively at each step.\n",
    "8. Write tests and experiment with model variations. Try ablations by removing components and quantify differences observed. Push and analyze limits of model capacity using larger data. These tests further validate understanding and highlight subtleties.\n",
    "\n",
    "In summary, read critically, decompose implementations into verifiable steps, validate incrementally against paper, and test rigorously. This systematic process can reliably translate research ideas into reproducible code.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de001598-928d-4280-9339-aa71597af7a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n",
      "1\n",
      "2\n",
      "An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses_free = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_free.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e0dd37-d4f9-4ee1-a1b6-6fa05938ccbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is an example implementation of the selective state space model (S6) architecture from the paper in PyTorch. It focuses on clearly documenting and explaining the components rather than being optimized for efficiency.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class SelectiveSSM(nn.Module):\n",
      "    \"\"\"\n",
      "    Selective State Space Model (S6)\n",
      "    Allows input-dependent interaction along sequence length dimension\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, input_size, state_size, discretize='zoh'):\n",
      "        super().__init__()\n",
      "        self.input_size = input_size\n",
      "        self.state_size = state_size\n",
      "        \n",
      "        # Continuous SSM parameters  \n",
      "        self.A_param = nn.Parameter(torch.randn(state_size)) \n",
      "        self.B = nn.Linear(input_size, state_size)\n",
      "        self.C = nn.Linear(state_size, input_size)\n",
      "        \n",
      "        # Discretization rule\n",
      "        if discretize.lower() == 'zoh':\n",
      "            self.discretize = self._zoh\n",
      "        else:\n",
      "            raise ValueError(f'Invalid discretization {discretize}')\n",
      "            \n",
      "        # Selection functions \n",
      "        self.sel_A = nn.Linear(input_size, 1) \n",
      "        self.sel_B = self.B\n",
      "        self.sel_C = self.C\n",
      "        \n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            x: (batch_size, sequence_len, input_size)\n",
      "        Returns: \n",
      "            y: (batch_size, sequence_len, input_size)\n",
      "        \"\"\"\n",
      "        batch_size, seq_len, input_size = x.shape\n",
      "        \n",
      "        sel_A = self._broadcast(self.sel_A(x)) \n",
      "        sel_B = self.sel_B(x)  \n",
      "        sel_C = self.sel_C(x)\n",
      "        \n",
      "        A, B = self.discretize(self.A_param, sel_A, self.B(x))\n",
      "       \n",
      "        # Efficient scan implementation\n",
      "        y = self._selective_scan(A, B, sel_C, x)  \n",
      "        \n",
      "        return y\n",
      "    \n",
      "    def _zoh(self, A_param, sel_A, B):\n",
      "        \"\"\"\n",
      "        Zero-order hold discretization\n",
      "        \"\"\"\n",
      "        A = torch.exp(sel_A * A_param)\n",
      "        A_inv = 1.0 / A\n",
      "        delta_A = 1.0 - A\n",
      "        B = delta_A * A_inv * B  \n",
      "        return A, B\n",
      "    \n",
      "    def _selective_scan(self, A, B, C, x):\n",
      "        \"\"\"\n",
      "        Efficient parallel scan implementation using GPU memory hierarchy\n",
      "        \"\"\"\n",
      "        # Fuse discretization, scan, output projection\n",
      "        return C * self._scan(A, B)  \n",
      "        \n",
      "    def _scan(self, A, B):\n",
      "        \"\"\"\n",
      "        Associative scan operation along sequence length\n",
      "        \"\"\"\n",
      "        # Pytorch has efficient parallel scan implementation\n",
      "        return torch.scan(B, torch.zeros_like(B), accumulate=A) \n",
      "    \n",
      "    def _broadcast(self, x):\n",
      "        return x.unsqueeze(-1).expand(-1, -1, self.state_size)\n",
      "        \n",
      "```\n",
      "\n",
      "The key components:\n",
      "\n",
      "- `A_param`, `B`, `C`: Continuous SSM parameters\n",
      "- `sel_A`, `sel_B`, `sel_C`: Selection functions \n",
      "- `discretize()`: Discretization rule e.g. ZOH\n",
      "- `selective_scan()`: Efficient scan op\n",
      "- `_broadcast()`: Expand for state dimension\n",
      "\n",
      "The forward pass:\n",
      "\n",
      "1. Compute selection parameters from input\n",
      "2. Discretize continuous parameters to discrete ones\n",
      "3. Perform selective scan operation \n",
      "4. Project output\n",
      "\n",
      "This demonstrates the core ideas like input-dependent selection and efficient recurrence. Additional engineering for speed and memory optimization can build on this foundation.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f5394da-f819-4df5-9c84-03f0ad150aaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is an example code implementation of the scalable frames and convex optimization concepts from the paper:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from cvxpy import Variable, Minimize, Problem\n",
      "\n",
      "# Frame generation\n",
      "def generate_frame(n, m):\n",
      "    \"\"\"Generate random frame matrix Phi in R^n with m frame vectors\"\"\"\n",
      "    Phi = np.random.randn(n, m) \n",
      "    Phi = Phi / np.linalg.norm(Phi, axis=0) # Normalize columns\n",
      "    return Phi\n",
      "\n",
      "# Objective functions\n",
      "def linear_obj(u):\n",
      "    return np.ones(len(u)) @ u\n",
      "\n",
      "def log_barrier(u):\n",
      "    t = 1e-3\n",
      "    return sum(np.log(u+t))\n",
      "\n",
      "# Scalability constraints\n",
      "def scalability_constraints(Phi, u):   \n",
      "    n, m = Phi.shape\n",
      "    \n",
      "    # Construct F(Phi) matrix\n",
      "    d = int((n-1)*(n+2)/2)  \n",
      "    F_phi = np.zeros((d, m))\n",
      "    \n",
      "    for j in range(m):\n",
      "        col = np.zeros(d)\n",
      "        phi_j = Phi[:, j]\n",
      "        \n",
      "        for k in range(n+1):\n",
      "            col[k] = (phi_j @ phi_j) - 1\n",
      "        \n",
      "        for k in range(1, n):\n",
      "            col[n+k] = phi_j[0] * phi_j[k]\n",
      "            for l in range(1,k):\n",
      "                col[n+k+(k-1)*k/2+l-1] = phi_j[l] * phi_j[k]\n",
      "        \n",
      "        F_phi[:, j] = col\n",
      "        \n",
      "    return [F_phi @ u == 0, np.ones(m) @ u == 1, u >= 0]\n",
      "\n",
      "# Optimization problem\n",
      "def solve_scalability(Phi, objective):\n",
      "    m = Phi.shape[1]\n",
      "    u = Variable(m)\n",
      "    \n",
      "    constraints = scalability_constraints(Phi, u)\n",
      "    prob = Problem(Minimize(objective(u)), constraints)\n",
      "    prob.solve()\n",
      "\n",
      "    return u.value\n",
      "\n",
      "# Example usage\n",
      "n = 5\n",
      "m = 10\n",
      "Phi = generate_frame(n, m)\n",
      "\n",
      "u_linear = solve_scalability(Phi, linear_obj) \n",
      "print(\"Sparsity with linear objective:\", np.count_nonzero(u_linear))\n",
      "\n",
      "u_barrier = solve_scalability(Phi, log_barrier)  \n",
      "print(\"Density with barrier objective:\", np.count_nonzero(u_barrier))\n",
      "```\n",
      "\n",
      "The key steps implemented:\n",
      "\n",
      "- Generate random frame matrix Phi \n",
      "- Construct scalability constraint matrix F(Phi)\n",
      "- Define objective functions for sparsity and density\n",
      "- Set up optimization problem over u\n",
      "- Solve problem to find scalings that make frame tight\n",
      "\n",
      "The code demonstrates solving for u with different objectives, illustrating the ability to produce sparse vs dense scalings. Additional examples and analyses could be added as noted in the last section of the instructions.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9fbd045-95ef-4f16-b76d-ce6d452230bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately I do not have the capability to fully implement and test the models from the paper in this context. However, I can provide a high-level overview of the key ideas and outline some of the major components that would need to be coded up to realize the Tree of Thoughts (ToT) framework:\n",
      "\n",
      "The main goal of ToT is to enable more systematic and deliberate exploration of reasoning paths for a language model (LM) to solve problems. This is done by representing the intermediate steps toward a solution as a tree of \"thoughts\", where each thought is a coherent piece of text. \n",
      "\n",
      "Key aspects that would need implementation:\n",
      "\n",
      "1. **Thought Generator**: Function that can sample possible next thoughts by prompting the LM, given the current state (input + thoughts so far). Two strategies are proposed: (a) Sample thoughts independently or (b) Propose them sequentially.\n",
      "\n",
      "```python\n",
      "# Example sketch\n",
      "def generate_thoughts(lm, state, num_samples):\n",
      "    # Strategy (a)\n",
      "    thoughts = [] \n",
      "    for i in range(num_samples):\n",
      "        thought = lm.generate(prompt=create_independent_prompt(state)) \n",
      "        thoughts.append(thought)\n",
      "\n",
      "    # Strategy (b) \n",
      "    thoughts = []\n",
      "    context = state  \n",
      "    for i in range(num_samples):\n",
      "        thought = lm.generate(prompt=create_proposal_prompt(context))\n",
      "        context += thought \n",
      "        thoughts.append(thought)\n",
      "\n",
      "    return thoughts\n",
      "```\n",
      "\n",
      "2. **State Evaluator**: Function to score or assess states (nodes in the tree) based on the thoughts generated so far. Two strategies are described: (a) Score each state independently (b) Compare and vote across states.\n",
      "\n",
      "```python \n",
      "# Example sketch\n",
      "def evaluate_states(lm, states):\n",
      "    values = []\n",
      "    for state in states:\n",
      "        # Strategy (a) \n",
      "        value = lm.generate(prompt=create_value_prompt(state))   \n",
      "        values.append(float(value))\n",
      "    \n",
      "    # Strategy (b)\n",
      "    best_state = lm.generate(prompt=create_vote_prompt(states)) \n",
      "    values = [1.0 if s == best_state else 0.0 for s in states] \n",
      "\n",
      "    return values\n",
      "```\n",
      "\n",
      "3. **Tree Search Algorithm**: Breadth-first or depth-first search to traverse the tree based on state evaluations. Generic tree search algorithms can be used.\n",
      "\n",
      "To actually implement the full framework, additional glue code and prompts would need to be written for a specific problem's thought decomposition, output rendering etc. The paper shows 3 examples - Game of 24, Creative Writing and Mini Crosswords.\n",
      "\n",
      "While not a full implementation, I tried to extract and explain the key ideas and components that could serve as a starting point. Please let me know if any part needs more clarification!\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4188d727-6299-4358-8a1c-2d903e08abc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is an example Python implementation of the LoRA (Low-Rank Adaptation) method from the paper:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class LoRA(nn.Module):\n",
      "    \"\"\"\n",
      "    Implements Low-Rank Adaptation (LoRA) for efficient tuning of large pre-trained models.\n",
      "    \n",
      "    Freezes base model weights and injects trainable low-rank matrices A and B \n",
      "    to decompose weight updates in each layer.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, base_model, rank):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            base_model: Pre-trained model whose weights will be frozen\n",
      "            rank: Rank of the LoRA decomposition matrices \n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.base_model = base_model\n",
      "        self.rank = rank\n",
      "        \n",
      "        # Freeze parameters of base model\n",
      "        for param in base_model.parameters():\n",
      "            param.requires_grad = False\n",
      "            \n",
      "        # Initialize LoRA A and B matrices for each layer\n",
      "        self.A_matrices = []\n",
      "        self.B_matrices = []\n",
      "        for layer in base_model.layers:\n",
      "            d_in, d_out = layer.weight.shape\n",
      "            \n",
      "            A = torch.randn(d_out, rank) \n",
      "            B = torch.zeros(rank, d_in)\n",
      "            \n",
      "            self.A_matrices.append(nn.Parameter(A)) \n",
      "            self.B_matrices.append(nn.Parameter(B))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            x: Input data\n",
      "        \n",
      "        Returns:\n",
      "            Output of the adapted base model with injected LoRA weights\n",
      "        \"\"\"\n",
      "        \n",
      "        # Forward pass through base model with frozen weights\n",
      "        with torch.no_grad():\n",
      "            x = self.base_model(x) \n",
      "        \n",
      "        # Incrementally add LoRA weights  \n",
      "        for A_matrix, B_matrix, layer in zip(self.A_matrices, self.B_matrices, self.base_model.layers):\n",
      "            weight = layer.weight\n",
      "            update = torch.matmul(A_matrix, B_matrix) # Low-rank update\n",
      "            x = layer(x, weight + update) \n",
      "            \n",
      "        return x\n",
      "```\n",
      "\n",
      "The key ideas:\n",
      "\n",
      "- Initialize a `LoRA` module wrapping the base pre-trained model\n",
      "- Freeze the base model weights \n",
      "- For each layer:\n",
      "    - Initialize low-rank `A` and `B` matrices \n",
      "    - Compute `update = torch.matmul(A, B)` \n",
      "    - Incrementally add `update` to layer weights\n",
      "- Forward pass uses frozen base weights + low-rank update\n",
      "\n",
      "This is a basic example implementing LoRA for standard dense feedforward networks. The paper also explores applying it to Transformer self-attention.\n",
      "\n",
      "Some ways to extend this:\n",
      "\n",
      "- Add option to only decompose select layers\n",
      "- Implement for Transformer layers\n",
      "- Add model saving logic and utilities for easy swapping of A,B matrices\n",
      "- Add training harness and experiment code\n",
      "\n",
      "Let me know if any part of the explanation or implementation is unclear!\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41fd8c60-6fc3-4972-adaf-c0cf0e6c043b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is an example Python implementation of the ReLoRA algorithm from the paper:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.optim import Adam\n",
      "\n",
      "class ReLoRA(nn.Module):\n",
      "    def __init__(self, model):\n",
      "        super().__init__()\n",
      "        self.model = model\n",
      "        self.a_matrices = []\n",
      "        self.b_matrices = []\n",
      "        \n",
      "        # Replace linear layers with LoRA decomposition\n",
      "        for name, module in self.model.named_modules():\n",
      "            if isinstance(module, nn.Linear):\n",
      "                out_features, in_features = module.weight.shape\n",
      "                \n",
      "                a = nn.Parameter(torch.randn(out_features, rank))\n",
      "                b = nn.Parameter(torch.randn(rank, in_features))\n",
      "                \n",
      "                self.a_matrices.append(a) \n",
      "                self.b_matrices.append(b)\n",
      "                \n",
      "                module.weight.requires_grad = False\n",
      "                module.weight = nn.Parameter(module.weight)\n",
      "                \n",
      "        self.optimizer = Adam(self.parameters())\n",
      "        self.reset_frequency = 5000\n",
      "        \n",
      "    def forward(self, x):\n",
      "        for a, b in zip(self.a_matrices, self.b_matrices):\n",
      "            x = x @ b.matmul(a) \n",
      "        return self.model(x)\n",
      "    \n",
      "    def train_step(self):\n",
      "        # Forward and backward pass\n",
      "        loss = self.loss_fn(self.forward(x), y) \n",
      "        loss.backward()\n",
      "        \n",
      "        # Optimize a, b parameters\n",
      "        self.optimizer.step()\n",
      "        self.optimizer.zero_grad()\n",
      "        \n",
      "        # Reset a, b parameters periodically\n",
      "        if self.global_step % self.reset_frequency == 0:\n",
      "            self.reset_parameters()\n",
      "            \n",
      "    def reset_parameters(self):\n",
      "        for a in self.a_matrices:\n",
      "            # Reinitialize a \n",
      "            nn.init.kaiming_normal_(a)\n",
      "            \n",
      "        for b in self.b_matrices:\n",
      "            # Set b to zeros\n",
      "            b.data.zero_()\n",
      "            \n",
      "        # Partial optimizer reset\n",
      "        prune_optimizer_state(self.optimizer, amount=0.9) \n",
      "        \n",
      "        # Learning rate warmup\n",
      "        n_warmup_steps = 100 \n",
      "        warmup_scheduler = LinearLR(self.optimizer, start_lr=0.0, end_lr=self.lr, steps=n_warmup_steps)\n",
      "```\n",
      "\n",
      "The key aspects implemented:\n",
      "\n",
      "- Replace model linear layers with LoRA decomposition \n",
      "- Keep original weights frozen, add a,b matrices as extra parameters\n",
      "- Forward pass applies a,b decomposition  \n",
      "- Train a,b parameters with Adam optimizer\n",
      "- Periodically reset a,b and reinitialize\n",
      "- Prune optimizer states on reset \n",
      "- Apply learning rate warmup schedule after reset\n",
      "\n",
      "Let me know if any part needs more explanation or if you would like to see additional implementations from the paper!\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab518d4e-3c6d-46f8-a973-aef4618f1937",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a Python implementation of the key models and algorithms from the paper \"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model\":\n",
      "\n",
      "```python\n",
      "import spacy\n",
      "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
      "\n",
      "# Load pre-trained translation models\n",
      "en_de_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/wmt19-en-de\") \n",
      "de_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/wmt19-de-en\")\n",
      "\n",
      "# Load tokenizers\n",
      "en_tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-en-de\")\n",
      "de_tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-de-en\")\n",
      "\n",
      "# Example gender-biased English input text\n",
      "text = \"The policemen searched for a capable leader.\"\n",
      "\n",
      "# Round-trip translation for pseudo German text\n",
      "en_text = en_tokenizer(text, return_tensors=\"pt\")\n",
      "de_text = de_model.generate(**en_text)  \n",
      "en_text2 = de_model.generate(**de_text)\n",
      "rt_text = de_tokenizer.decode(de_text[0], skip_special_tokens=True)\n",
      "\n",
      "# Merge to preserve gendered forms  \n",
      "doc = spacy.load(\"de_core_news_sm\")\n",
      "original = doc(text)\n",
      "roundtrip = doc(rt_text)\n",
      "merged = []\n",
      "for token in original:\n",
      "  if token.text+\"innen\" in roundtrip.text or token.text+\"in\" in roundtrip.text:\n",
      "    merged.append(roundtrip[roundtrip.text.index(token.text+\"innen\" if token.text+\"innen\" in roundtrip.text else token.text+\"in\")])\n",
      "  else:\n",
      "    merged.append(token)\n",
      "merged_text = \" \".join([token.text for token in merged])\n",
      "\n",
      "# Print final texts   \n",
      "print(\"Original:\", text)\n",
      "print(\"Round-trip:\", rt_text) \n",
      "print(\"Merged:\", merged_text)\n",
      "\n",
      "# Train gender-fair seq2seq model\n",
      "train_model(clean_gender_fair_data, merged_pseudo_data) \n",
      "\n",
      "def train_model(gender_fair_data, pseudo_parallel_data):\n",
      "\n",
      "  # Implement training loop...\n",
      "  \n",
      "  return model\n",
      "\n",
      "# Usage:\n",
      "rewriter_model = train_model(gender_fair_deu, pseudo_deu)\n",
      "rewritten = rewrite(rewriter_model, biased_input)\n",
      "```\n",
      "\n",
      "This shows the round-trip translation to create pseudo parallel data, merging algorithm to preserve gendered forms, and overall training process for the neural sequence-to-sequence gender rewriting model. The code is documented and modularized into functions for clarity. Additional data loading, model configuration, training loop implementation, and evaluation code would be needed for a full implementation.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a01d874-3713-427a-a24b-33b69b8a508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"You are an AI Researcher whose goal is to analyze articles and produce robust and understandable Python code examples.\\n\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and implement a well formatted and well documented code example,\n",
    "that impliments the models and/or algorithms precented in the paper. This should use sample data, but the explanation should be clear in the code:\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, implement well formatted and well documented Python code,\n",
    "that impliments the models and/or algorithms precented in the paper. This can use sample data, but the explanation should be clear in the code.\\n\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing code:\n",
    "1. Read the paper thoroughly and make sure you understand the key concepts. Don't just skim it and try to code. Take notes, draw diagrams, explain ideas out loud to yourself or others. Comprehension is key before attempting implementation.\n",
    "2. Identify the key components that need to be coded - the models, algorithms, data preprocessing steps, etc. Break the implementation down into smaller modular pieces.\n",
    "3. Find or create appropriate data to test the models on. Many papers include links to data or code repositories used. If not, try to create or find representative sample data.\n",
    "4. Start by hard-coding and verifying the simplest pieces first. For example, if implementing a complex transformer model, first recreate the multi-headed self-attention mechanism and test it independently before adding other components.\n",
    "5. Adhere to sound coding principles - modularize code into functions and classes, add comments explaining the purpose of areas of code, use descriptive variable names matching paper terminology. These practices will help ensure clear understanding.\n",
    "6. Visualize and print intermediate outputs to check that data transforms, model internal states, etc are as expected per paper descriptions to methodically validate implementation components.  \n",
    "7. Once pieces are individually working, incrementally connect and test them together until the full model or algorithm is constructed. Confirm outputs match expected behavior from paper either mathematically or qualitatively at each step.\n",
    "8. Write tests and experiment with model variations. Try ablations by removing components and quantify differences observed. Push and analyze limits of model capacity using larger data. These tests further validate understanding and highlight subtleties.\n",
    "\n",
    "In summary, read critically, decompose implementations into verifiable steps, validate incrementally against paper, and test rigorously. This systematic process can reliably translate research ideas into reproducible code.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"```python\\n# Detailed Description:\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "811d8558-3a06-42fa-b7a5-10eb1314e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses_free = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_free.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10747b44-52c7-42e8-a853-c5f6073f6e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# This code implements the core Selective State Space Model (S6) described in \n",
      "# the paper, including the efficient fused kernel for training.\n",
      "# It allows the SSM parameters A, B, C to be functions of the input x, \n",
      "# making it selective and able to perform content-dependent reasoning.\n",
      "\n",
      "# The S6 layer can flexibly be incorporated into neural network architectures.\n",
      "# Here we demonstrate usage by wrapping it in the simplified \"Mamba\" architecture\n",
      "# also described in the paper, which stacks the S6 layer with linear projections.\n",
      "\n",
      "# For demonstration, we train the Mamba model on a small synthetic \"Selective Copy\"\n",
      "# task that requires filtering irrelevant information. We show Mamba can solve this\n",
      "# while non-selective baselines cannot.\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# S6 Layer definition\n",
      "class S6Layer(nn.Module):\n",
      "    def __init__(self, input_size, state_size):\n",
      "        super().__init__()\n",
      "        \n",
      "        # Core SSM parameters  \n",
      "        self.A = nn.Parameter(torch.randn(input_size, state_size)) \n",
      "        self.B = nn.Linear(input_size, state_size)\n",
      "        self.C = nn.Linear(input_size, state_size)\n",
      "\n",
      "        # For discretization\n",
      "        self.A_bar = nn.Parameter(torch.randn(input_size)) \n",
      "        \n",
      "        self.state_size = state_size\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        # Input-dependent discretization\n",
      "        A_hat = F.softplus(self.A_bar) + self.B(x) \n",
      "        exp_A_delta = torch.exp(A_hat)\n",
      "        A = torch.eye(self.state_size).type_as(x) - exp_A_delta\n",
      "        B = (1 - A) / A_hat\n",
      "        \n",
      "        # Efficient fused kernel selective state expansion\n",
      "        return selective_state_expansion(\n",
      "            A, B, self.C(x), x, self.state_size)\n",
      "\n",
      "\n",
      "# Helper functions\n",
      "def selective_state_expansion(A, B, C, x, state_size):\n",
      "\n",
      "    # State initialization\n",
      "    state = torch.zeros(x.shape[0], x.shape[1], state_size, device=x.device)\n",
      "    \n",
      "    for i in range(x.shape[1]):\n",
      "        state[:, i] = A @ state[:, i-1] + B[:, i] @ x[:, i]  \n",
      "\n",
      "    return (C @ state).sum(dim=-1)\n",
      "\n",
      "\n",
      "# Mamba architecture \n",
      "class Mamba(nn.Module):\n",
      "    def __init__(self, input_size, state_size, model_dim):\n",
      "        super().__init__()\n",
      "        self.linear1 = nn.Linear(input_size, model_dim)\n",
      "        self.s6_layer = S6Layer(model_dim, state_size) \n",
      "        self.linear2 = nn.Linear(model_dim, model_dim)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        x = self.linear1(x)\n",
      "        x = torch.silu(x)\n",
      "        x = self.s6_layer(x)\n",
      "        x = self.linear2(x)\n",
      "        return x\n",
      "\n",
      "# Selective Copy Task \n",
      "MAX_LENGTH = 16  \n",
      "VOCAB_SIZE = 16\n",
      "\n",
      "model = Mamba(VOCAB_SIZE, state_size=32, model_dim=64)\n",
      "baseline = Mamba(VOCAB_SIZE, state_size=32, model_dim=64) \n",
      "\n",
      "# Make baseline non-selective by removing input-dependence\n",
      "for p in baseline.parameters():\n",
      "    p.requires_grad = False\n",
      "\n",
      "for length in [8, 16, 32]:\n",
      "    x = torch.randint(0, VOCAB_SIZE, (4, length))\n",
      "    mask = torch.bernoulli(0.3, (4, length)).to(torch.bool)  \n",
      "    y = x.masked_select(mask)\n",
      "    \n",
      "    print(f\"Length {length} Selective Copying:\")\n",
      "    print(f\"  Input: {x[0]}\")\n",
      "    print(f\"  Mask: {mask[0].to(torch.int)}\")\n",
      "    print(f\"  Output: {y[0]}\")  \n",
      "\n",
      "    print(f\"  Mamba Prediction: {model(x)[0][:len(y[0])]}\")\n",
      "\n",
      "    baseline_pred = baseline(x)[0][:len(y[0])]\n",
      "    print(f\"  Baseline Prediction: {baseline_pred}\")\n",
      "    print(f\"  Accuracy: {int(baseline_pred.eq(y[0]).all())}\")\n",
      "    print()\n",
      "```\n",
      "\n",
      "This code demonstrates the key ideas from the paper:\n",
      "\n",
      "- The S6 layer makes the SSM selective via input-dependent parameters A, B, C \n",
      "- It uses an efficient fused kernel to exploit hardware parallelism and memory locality\n",
      "- The Mamba architecture simply stacks S6 layers and projections  \n",
      "- On a synthetic selective copying task requiring filtering irrelevant information, Mamba succeeds while the non-selective baseline fails\n",
      "\n",
      "The modular implementation allows flexible usage of the S6 layer in other architectures, and demonstration on real tasks like language modeling. Additional engineering effort would be required to scale the implementation.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eed0bde7-d65f-4a14-99a9-1de9d879a399",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# This program implements the scalable frame algorithms described in the paper\n",
      "# \"On Optimal Frame Conditioners\" by Chae Clark and Kasso A. Okoudjou.\n",
      "# Key components implemented:\n",
      "# 1. Functions to generate random frames \n",
      "# 2. Scaling algorithms: Linear Programming, Barrier, Augmented Lagrangian\n",
      "# 3. Analysis of sparsity patterns and scalability testing\n",
      "\n",
      "# Imports and Dependencies\n",
      "import numpy as np\n",
      "from cvxpy import Problem, Minimize, Maximize, Variable, Parameter, norm1, abs\n",
      "\n",
      "# Frame Generation \n",
      "def generate_random_frame(N, M):\n",
      "    \"\"\"Generate an NxM frame matrix with gaussian random elements\"\"\"\n",
      "    F = np.random.normal(size=(N,M)) \n",
      "    return F\n",
      "\n",
      "# Objective Functions \n",
      "def linear_program_obj(F, u):\n",
      "    \"\"\"Linear program objective - l1 norm minimization\"\"\"\n",
      "    return norm1(u)\n",
      "\n",
      "def barrier_obj(F, u):\n",
      "    \"\"\"Barrier objective with log barrier epsilon term\"\"\" \n",
      "    eps = 1e-3\n",
      "    return sum(np.log(u + eps))\n",
      "\n",
      "def augmented_lagrangian_obj(F, u):\n",
      "    \"\"\"Augmented lagrangian objective - l2 norm minimization\"\"\"\n",
      "    return norm(u,2)**2\n",
      "\n",
      "# Scaling Algorithms\n",
      "def linear_program_scaling(F, objective=linear_program_obj):\n",
      "    \"\"\"Solve the scalability linear program to find sparse scaling\"\"\"\n",
      "    N, M = F.shape\n",
      "    \n",
      "    # Optimization variables\n",
      "    u = Variable(M)\n",
      "    \n",
      "    # Constraints \n",
      "    constraints = [F*u == 0, \n",
      "                  norm1(u) == 1,\n",
      "                  u >= 0]\n",
      "    \n",
      "    # Solve LP        \n",
      "    prob = Problem(Minimize(objective(F, u)), constraints)\n",
      "    prob.solve()\n",
      "    \n",
      "    return u.value\n",
      "\n",
      "def barrier_scaling(F, objective=barrier_obj):\n",
      "    \"\"\"Solve barrier optimization program to find dense scaling\"\"\"\n",
      "    N, M = F.shape\n",
      "    \n",
      "    # Optimization variables \n",
      "    u = Variable(M)\n",
      "    \n",
      "    # Constraints\n",
      "    constraints = [F*u == 0,  \n",
      "                   norm1(u) == 1,\n",
      "                   u > 0] # Barrier requires strict positivity\n",
      "    \n",
      "    # Solve program\n",
      "    prob = Problem(Maximize(objective(F,u)), constraints) \n",
      "    prob.solve()\n",
      "    \n",
      "    return u.value\n",
      "\n",
      "def augmented_lagrangian_scaling(F):\n",
      "    \"\"\"Custom augmented lagrangian method to solve l2 program\"\"\"\n",
      "    N, M = F.shape\n",
      "    MAX_ITERS = 20\n",
      "    u = np.ones(M)/M # Initialize uniform scaling\n",
      "    \n",
      "    for i in range(MAX_ITERS):\n",
      "        # Algorithm steps from paper\n",
      "        u = update_dual_variable(F, u) \n",
      "        u = project_nonnegative(u) \n",
      "    \n",
      "    return u\n",
      "\n",
      "# Helper Functions\n",
      "def update_dual_variable(F, u):\n",
      "    \"\"\"Perform augmented Lagrangian update step on dual variable u\"\"\"\n",
      "    v = 2*(F.T @ F @ u) - F.T @ (F @ u)  \n",
      "    return v  \n",
      "\n",
      "def project_nonnegative(v):\n",
      "    \"\"\"Project arbitrary vector v onto nonnegative orthant\"\"\"\n",
      "    return np.maximum(v, 0)\n",
      "\n",
      "# Analysis Functions\n",
      "def compute_sparsity(u):\n",
      "    \"\"\"Given a scaling vector u, compute fraction of zero elements\"\"\"\n",
      "    num_zero = (u == 0).sum()\n",
      "    return num_zero / len(u)\n",
      "\n",
      "def check_scalability(F, u):\n",
      "    \"\"\"Check if scaling vector u makes frame F tight (scalable)\"\"\"\n",
      "    return np.allclose(F @ u, np.ones(F.shape[0]))\n",
      "\n",
      "# Driver Code\n",
      "if __name__ == \"__main__\":\n",
      "    \n",
      "    # Generate random frame\n",
      "    N, M = 3, 10\n",
      "    F = generate_random_frame(N, M)\n",
      "    \n",
      "    # Solve for sparse scaling vector \n",
      "    u_sparse = linear_program_scaling(F)  \n",
      "    print(\"Sparsity = %.2f\" % compute_sparsity(u_sparse))\n",
      "    \n",
      "    # Solve for dense scaling vector\n",
      "    u_dense = barrier_scaling(F) \n",
      "    print(\"All scaling elements > 0: \", (u_dense > 0).all())\n",
      "    \n",
      "    # Check if scalings worked\n",
      "    print(\"Sparse Scalability:\", check_scalability(F, u_sparse)) \n",
      "    print(\"Dense Scalability:\", check_scalability(F, u_dense))\n",
      "    \n",
      "```\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The code implements key algorithms from the paper for generating random frames and finding scalings using various optimization objectives:\n",
      "\n",
      "1. `generate_random_frame` creates random gaussian frames\n",
      "\n",
      "2. `linear_program_scaling` solves a linear program to minimize the l1 norm and induce sparsity\n",
      "\n",
      "3. `barrier_scaling` maximizes a log barrier objective to produce a dense scaling  \n",
      "\n",
      "4. `augmented_lagrangian_scaling` is a custom method following the algorithm in the paper\n",
      "\n",
      "5. Analysis functions like `compute_sparsity`, `check_scalability` are used to test and verify the scalings\n",
      "\n",
      "The driver code shows example usage to generate a frame, solve for sparse and dense scalings, print sparsity stats, and test if the frame condition is satisfied.\n",
      "\n",
      "The code is modularized into reusable components, includes comments explaining logic, and makes an effort to match variable names and terminology from paper. It outputs key metrics to validate against paper expectations.\n",
      "\n",
      "Additional testing could include larger sample sizes, varying frame sizes N and M, parameter tweaks, timing and complexity analysis to further analyze performance. But this represents a basic implementation of the core ideas.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70eac513-0738-4e9f-a0eb-e1b88787910c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "This code implements the Tree of Thoughts (ToT) framework from the paper \n",
      "\"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\".\n",
      "\n",
      "ToT allows language models (LMs) to explore multiple reasoning paths over intermediate\n",
      "\"thought\" steps to solve problems that require planning or search. \n",
      "\n",
      "It involves maintaining a tree where each node is a \"state\" representing the \n",
      "sequence of thoughts so far. New thoughts extend the state to child nodes. \n",
      "A state evaluator heuristic guides tree search to promising states.\n",
      "\n",
      "This code shows an example implementation of ToT for the \"Game of 24\" mathematical\n",
      "reasoning challenge, where the goal is to reach 24 from 4 input numbers using basic \n",
      "arithmetic operations.\n",
      "\n",
      "The key components implemented:\n",
      "\n",
      "1. ThoughtGenerator: Generate next possible thought (intermediate equation)\n",
      "   given current state\n",
      "\n",
      "2. StateEvaluator: Evaluate if a state's thoughts can reach 24\n",
      "   Values each state as \"sure\", \"maybe\", \"impossible\" \n",
      "   \n",
      "3. SearchManager: Explores tree using Breadth First Search\n",
      "   At each level, keeps top B states based on StateEvaluator\n",
      "   Outputs solution equation from best final state\n",
      "\n",
      "The process flows as:\n",
      "1. Initialize tree with root state having input numbers\n",
      "2. Loop for max tree depth (intermediate equations needed):\n",
      "   - Use ThoughtGenerator to get possible next thoughts \n",
      "   - Use StateEvaluator to value each resulting state\n",
      "   - Keep top B states using values\n",
      "   - Add kept states as children extending the tree\n",
      "3. Output final equation from best state  \n",
      "\n",
      "Can configure components independently, e.g.:\n",
      "- Use different StateEvaluators \n",
      "- Explore tree with Depth First Search\n",
      "- Increase tree breadth B\n",
      "- Modify thought generation prompting\n",
      "\n",
      "Thus this provides a modular implementation showing how ToT works in code.\n",
      "\n",
      "```\n",
      "\n",
      "import random\n",
      "from collections import defaultdict\n",
      "\n",
      "class ThoughtGenerator:\n",
      "    \"\"\"\n",
      "    Generates possible next thoughts extending a state \n",
      "    towards solving the problem\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, lm): \n",
      "        self.lm = lm # Language model to use\n",
      "    \n",
      "    def generate_thoughts(self, state):\n",
      "        \"\"\"\n",
      "        Generate next thoughts for given state\n",
      "        State is tuple of (numbers left, thoughts so far) \n",
      "        e.g. ((10, 13, 13), [])\n",
      "        \"\"\"\n",
      "        \n",
      "        numbers_left, thoughts = state \n",
      "        prompt = f\"Numbers left: {numbers_left}. Thoughts so far: {thoughts}\"\n",
      "        # Use LM to generate next possible thoughts \n",
      "        # given prompt context\n",
      "        next_thoughts = self.lm.generate(\n",
      "            prompt = prompt, \n",
      "            max_tokens=64,\n",
      "            temperature=0.7\n",
      "        )  \n",
      "        return next_thoughts\n",
      "\n",
      "\n",
      "class StateEvaluator:\n",
      "    \"\"\"\n",
      "    Evaluates states to guide tree search\n",
      "    Heuristically values states as:\n",
      "      \"sure\" - sure to lead to solution \n",
      "      \"maybe\" - may lead to solution  \n",
      "      \"impossible\" - surely will NOT lead to solution\n",
      "      \n",
      "    This uses the LM to deliberate by considering possible\n",
      "    future states from current state\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, lm):\n",
      "        self.lm = lm\n",
      "        \n",
      "    def evaluate(self, state):\n",
      "        \"\"\"\n",
      "        Evaluate if a state can reach 24\n",
      "        \"\"\"        \n",
      "        numbers_left, thoughts = state\n",
      "        prompt = f\"Numbers left: {numbers_left}. Thoughts: {thoughts}\" \n",
      "        value = self.lm.generate(\n",
      "            prompt=prompt,  \n",
      "            max_tokens=32,\n",
      "            temperature=0\n",
      "        )\n",
      "        \n",
      "        # Heuristically convert LM response to value\n",
      "        if \"sure\" in value:\n",
      "            return \"sure\" \n",
      "        elif \"maybe\" in value:\n",
      "            return \"maybe\"\n",
      "        else:\n",
      "            return \"impossible\"\n",
      "\n",
      "        \n",
      "class SearchManager:\n",
      "    \"\"\"\n",
      "    Explores tree using Breadth First Search\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, thought_generator, state_evaluator, max_depth=3, max_breadth=5):\n",
      "        self.thought_generator = thought_generator \n",
      "        self.state_evaluator = state_evaluator\n",
      "        self.max_depth = max_depth # max iterations \n",
      "        self.max_breadth = max_breadth # beam size\n",
      "        \n",
      "    def search(self, initial_state):\n",
      "        \"\"\"\n",
      "        Perform BFS search from initial state\n",
      "        Returns best final state \n",
      "        \"\"\"\n",
      "        \n",
      "        queue = [initial_state] \n",
      "        for _ in range(self.max_depth): \n",
      "            next_queue = []\n",
      "            for state in queue:\n",
      "                if self._is_final(state):\n",
      "                    return state\n",
      "                \n",
      "                # Extend state with possible next thoughts\n",
      "                next_thoughts = self.thought_generator.generate_thoughts(state)  \n",
      "                next_states = [\n",
      "                    self._extend_state(state, thought)  \n",
      "                    for thought in next_thoughts\n",
      "                ]\n",
      "                \n",
      "                # Evaluate and filter states\n",
      "                valued_states = []\n",
      "                for next_state in next_states:\n",
      "                    value = self.state_evaluator.evaluate(next_state)  \n",
      "                    valued_states.append((value, next_state))\n",
      "                    \n",
      "                # Keep top B states\n",
      "                valued_states.sort(reverse=True) \n",
      "                valued_states = valued_states[:self.max_breadth]  \n",
      "                next_queue.extend([s for _, s in valued_states])\n",
      "                \n",
      "            queue = next_queue\n",
      "            \n",
      "        # Output final equation from best state \n",
      "        best_state = max(queue, \n",
      "                        key=lambda s: self.state_evaluator.evaluate(s)[0]) \n",
      "        return best_state\n",
      "        \n",
      "    def _extend_state(self, state, thought):\n",
      "        \"\"\"\n",
      "        Add thought to state, updating numbers left\n",
      "        \"\"\"\n",
      "        numbers_left, thoughts = state\n",
      "        new_thoughts = thoughts + [thought]\n",
      "        new_numbers = self._update_numbers(numbers_left, thought)\n",
      "        return (new_numbers, new_thoughts)\n",
      "    \n",
      "    def _update_numbers(self, numbers, thought):\n",
      "        \"\"\"\n",
      "        Update numbers left based on thought equation \n",
      "        \"\"\"\n",
      "        # Code to process equation and update numbers\n",
      "        return new_numbers \n",
      "    \n",
      "    def _is_final(self, state):\n",
      "        \"\"\"\n",
      "        Check if state has reached solution\n",
      "        \"\"\"\n",
      "        numbers_left, _ = state \n",
      "        return sum(numbers_left) == 24\n",
      "        \n",
      "# Example usage\n",
      "\n",
      "# Initial state\n",
      "initial_state = ([10, 13, 13], [])  \n",
      "\n",
      "# Create components\n",
      "lm = <your-lm>  \n",
      "thought_generator = ThoughtGenerator(lm)\n",
      "state_evaluator = StateEvaluator(lm)\n",
      "\n",
      "# Search manager explores state tree \n",
      "search_manager = SearchManager(\n",
      "    thought_generator,\n",
      "    state_evaluator\n",
      ")\n",
      "best_state = search_manager.search(initial_state)\n",
      "\n",
      "# Print out final solution equation\n",
      "print(best_state[1])\n",
      "```\n",
      "\n",
      "This code shows how the key components of ToT can be implemented in Python. The modular design allows flexibility to configure the search process. Hope this provides a clear walkthrough bringing the Tree of Thoughts ideas to life in code! Let me know if any part needs more explanation or if you have suggestions to improve the implementation.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "035432f7-3c5c-4abe-a5df-1050f4281325",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# This code implements the LoRA approach for adapting pre-trained language models \n",
      "# for downstream tasks, as presented in the paper \"LORA: Low-Rank Adaptation of Large \n",
      "# Language Models\".\n",
      "#\n",
      "# LoRA is a method for updating pre-trained model weights to tailor them for a particular task\n",
      "# using low-rank decompositions of the weight updates instead of directly modifying all the weights.\n",
      "# This significantly reduces the number of parameters needed to be tuned for adaptation while\n",
      "# maintaining the performance.\n",
      "#\n",
      "# The key components implemented:\n",
      "# - LowRankAdaptationModule: Custom module that replaces existing weight matrices\n",
      "#   with a low rank decomposition to constrain weight updates\n",
      "# - LoRAAdaptedModel: Builds a model (e.g. Transformer, BERT) with LowRankAdaptationModules \n",
      "#   inserted to replace and decompose selected weight matrices \n",
      "# - Model training loop with forward/backward passes through LoRAAdaptedModel\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.nn import functional as F\n",
      "\n",
      "# Low-rank decomposition module\n",
      "class LowRankAdaptationModule(nn.Module):\n",
      "    def __init__(self, weight_matrix, rank):\n",
      "        super().__init__()\n",
      "        d, k = weight_matrix.shape\n",
      "        \n",
      "        # Decompose into low rank matrices\n",
      "        self.A = nn.Parameter(torch.randn(d, rank)) \n",
      "        self.B = nn.Parameter(torch.zeros(rank, k))  \n",
      "        \n",
      "        # Register original weight matrix \n",
      "        self.weight = nn.Parameter(weight_matrix)\n",
      "        self.weight.requires_grad = False\n",
      "        \n",
      "    def forward(self, x):\n",
      "        Aw = self.A @ self.B # Low rank decomposition\n",
      "        return x @ (self.weight + Aw) # Original layer output + low rank correction\n",
      "    \n",
      "# Model definition\n",
      "class LoRAAdaptedModel(nn.Module):\n",
      "    def __init__(self, model):\n",
      "        super().__init__()\n",
      "        self.model = model\n",
      "        \n",
      "        # Identify key weight matrices to decompose  \n",
      "        for name, param in self.model.named_parameters():\n",
      "            if name in [\"attn.Wq\", \"attn.Wv\"]: \n",
      "                rank = 4\n",
      "                setattr(self, name, LowRankAdaptationModule(param, rank))\n",
      "                \n",
      "        # Freeze all other params\n",
      "        for name, param in self.model.named_parameters():\n",
      "            if name not in [\"attn.Wq\", \"attn.Wv\"]:\n",
      "                param.requires_grad = False\n",
      "                \n",
      "    def forward(self, x):\n",
      "        for name, module in self.named_children():\n",
      "            # Forward through low rank or original modules\n",
      "            if isinstance(module, LowRankAdaptationModule):\n",
      "                x = module(x)  \n",
      "            else:\n",
      "                x = self.model._modules[name](x) \n",
      "        return x\n",
      "                \n",
      "# Initialize model and load pre-trained weights\n",
      "model = TransformerModel() \n",
      "model.load_state_dict(torch.load(\"pretrained_weights.pt\"))\n",
      "\n",
      "# Build LoRA adapted version\n",
      "lora_model = LoRAAdaptedModel(model)\n",
      "\n",
      "# Sample training loop\n",
      "for input, target in train_data:\n",
      "  out = lora_model(input)\n",
      "  loss = loss_fn(out, target)\n",
      "  \n",
      "  optimizer.zero_grad()\n",
      "  loss.backward() \n",
      "  optimizer.step() # Only update low-rank decomposition params\n",
      "\n",
      "```\n",
      "\n",
      "This implements the core concept of replacing selected weight matrices in a model like a Transformer with LowRankAdaptationModules to constrain their updates to a low rank approximation. The forward and backward passes allow normally tuning the model while only updating the small decomposition matrices.\n",
      "\n",
      "Key points:\n",
      "- LowRankAdaptationModule class handles the low rank decomposition \n",
      "- LoRAAdaptedModel inserts those modules into chosen layers\n",
      "- Only low rank factors get gradient updates during training\n",
      "- Most model weights stay frozen for efficiency\n",
      "- Process allows drop-in replacement for fine-tuning with constrained updates\n",
      "\n",
      "The code is documented inline and demonstrates the main concepts from the paper in a modular, extensible way. Additional testing, data handling, training configs, etc can build on this.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f8dc1ef-a603-48b4-8f73-8c9eb1dc61d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# The paper proposed a method called ReLoRA, which stands for Re-starting Low-Rank Updates, \n",
      "# to train high-rank neural networks efficiently. The key ideas are:\n",
      "\n",
      "# 1. Use LoRA (Low-Rank Adaptation) to decompose a layer's weight matrix W into low rank matrices WA and WB.\n",
      "# Only WA and WB are trained while W is fixed. This reduces the number of trainable parameters. \n",
      "\n",
      "# 2. Periodically merge WA and WB back into W through a low-rank update to W.\n",
      "# Then reinitialize WA and WB. This allows accumulating higher rank updates to W over multiple rounds.\n",
      "\n",
      "# 3. Other enhancements like warm start with initial full rank training, \n",
      "# jagged learning rates around restarts, and partial optimizer resets enable stable training.\n",
      "\n",
      "# Below is sample code to demonstrate training a simple MLP model using ReLoRA.\n",
      "# Key components implemented:\n",
      "\n",
      "# - LoRA layer replacement of fully connected layers\n",
      "# - Scheduling restarts and optimizer resets \n",
      "# - Jagged learning rates aligned with restarts\n",
      "# - Tracking rank metrics to validate high rank updates\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.optim import Adam\n",
      "\n",
      "# MLP Model\n",
      "class MLP(nn.Module):\n",
      "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
      "        super().__init__()\n",
      "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
      "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# LoRA Layer    \n",
      "class LoRALinear(nn.Module):\n",
      "    def __init__(self, in_features, out_features, rank):\n",
      "        super().__init__()\n",
      "        self.A = nn.Linear(in_features, rank, bias=False)\n",
      "        self.B = nn.Linear(rank, out_features, bias=False) \n",
      "        self.W = nn.Linear(in_features, out_features)\n",
      "        \n",
      "        # Freeze W weights\n",
      "        self.W.weight.requires_grad = False\n",
      "        \n",
      "    def forward(self, x):\n",
      "        x = self.A(x) \n",
      "        x = self.B(x)\n",
      "        return self.W(x)\n",
      "\n",
      "# Train loop \n",
      "def train_epoch(model, optimizer, data_loader, device, restarts):\n",
      "    model.train()\n",
      "    for x, y in data_loader:\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "       \n",
      "        optimizer.zero_grad()\n",
      "        output = model(x)\n",
      "        loss = torch.nn.functional.mse_loss(output, y)\n",
      "        loss.backward()\n",
      "        \n",
      "        optimizer.step()\n",
      "       \n",
      "        if optimizer.steps % restarts == 0:\n",
      "            # Scheduler restart and reset\n",
      "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_reset)\n",
      "            optimizer.zero_grad() \n",
      "           \n",
      "# Learning rate reset function \n",
      "def lr_reset(epoch):\n",
      "  return 1e-3 if epoch<=100 else 1e-4   \n",
      "\n",
      "# Track rank during training\n",
      "def compute_rank(updates):\n",
      "    u, s, v = torch.svd(updates) \n",
      "    rank = (s > 0.1).sum()\n",
      "    return rank\n",
      "\n",
      "# Model training\n",
      "input_dim = 28*28\n",
      "hidden_dim = 256\n",
      "output_dim = 10  \n",
      "\n",
      "# Initialize model\n",
      "model = MLP(input_dim, hidden_dim, output_dim)\n",
      "model.fc1 = LoRALinear(input_dim, hidden_dim, rank=16) \n",
      "model.fc2 = LoRALinear(hidden_dim, output_dim, rank=16)\n",
      "\n",
      "# Set device  \n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model = model.to(device)  \n",
      "\n",
      "# Create optimizer\n",
      "optimizer = Adam(model.parameters(), lr=1e-3)  \n",
      "\n",
      "for epoch in range(100): \n",
      "    for x, y in data_loader:\n",
      "        train_epoch(model, optimizer, data_loader, device, restarts=5000)\n",
      "   \n",
      "    # Compute rank  \n",
      "    fc1_update = model.fc1.W.weight - model.fc1.weight \n",
      "    fc2_update = model.fc2.W.weight - model.fc2.weight\n",
      "    \n",
      "    rank_fc1 = compute_rank(fc1_update)\n",
      "    rank_fc2 = compute_rank(fc2_update)\n",
      "    print(f\"Epoch {epoch}: Rank FC1 {rank_fc1} Rank FC2 {rank_fc2}\")\n",
      "```\n",
      "\n",
      "This shows a simple example for applying ReLoRA to an MLP model on sample data. The concepts can be extended to larger transformer models. Let me know if any part needs more clarification!\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0eab97c-e16c-4d11-95da-7b86290c9774",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# The paper proposes two methods to train gender de-biasing models:\n",
      "# 1. Backward Augmentation: Using gender-fair target text as targets and artificially \n",
      "#    biasing them to create gender-biased source texts for training\n",
      "# 2. Round-trip Augmentation: Translating gender-fair target texts to a pivot  \n",
      "#    language and back to create pseudo gender-biased source texts for training\n",
      "\n",
      "# We will implement both data augmentation methods and the seq2seq models used.\n",
      "\n",
      "# 1. Backward Augmentation\n",
      "# ---------------------------\n",
      "\n",
      "import spacy\n",
      "from spacy.tokens import Doc\n",
      "import random\n",
      "\n",
      "# Create lookup dictionaries mapping gender-fair words to biased forms\n",
      "# Based on tables in Appendix E of paper\n",
      "gender_fair_to_biased_pronouns = {\n",
      "   \"they\":\"he\",\n",
      "   \"their\":\"his\",\n",
      "   \"them\":\"him\",\n",
      "   \"theirs\":\"his\",\n",
      "   \"themself\":\"himself\"}\n",
      "\n",
      "gender_fair_to_biased_nouns = {\n",
      "   \"chairperson\":\"chairman\",\n",
      "   \"police officer\":\"policeman\",\n",
      "   \"firefighter\":\"fireman\"} \n",
      "   \n",
      "# Rule-based function to artificially bias gender-fair text\n",
      "def artificially_bias_text(text):\n",
      "   \n",
      "   # Tokenize \n",
      "   doc = nlp(text)\n",
      "   \n",
      "   for token in doc:\n",
      "      # Replace gender-fair pronouns with biased forms  \n",
      "      if token.text in gender_fair_to_biased_pronouns:\n",
      "         token.text = gender_fair_to_biased_pronouns[token.text] \n",
      "         \n",
      "      # Replace gender-fair nouns with biased forms\n",
      "      if token.text in gender_fair_to_biased_nouns:\n",
      "         token.text = random.choice([gender_fair_to_biased_nouns[token.text], \n",
      "                                    token.text]) # Probabilistically replace some\n",
      "   return doc.text\n",
      "\n",
      "# Seq2Seq model architecture\n",
      "class Seq2SeqModel(nn.Module):\n",
      "\n",
      "   def __init__(self):\n",
      "      super().__init__()        \n",
      "      # Transformer Encoder-Decoder Architecture\n",
      "      self.encoder = Encoder() \n",
      "      self.decoder = Decoder()\n",
      "      \n",
      "   def forward(self, src, trg):\n",
      "      enc_src = self.encoder(src)  \n",
      "      dec_output = self.decoder(enc_src, trg)\n",
      "      return dec_output\n",
      "\n",
      "# Get gender-fair target text  \n",
      "target_text = \"The police officers helped the firefighters\" \n",
      "\n",
      "# Create artificial biased source text \n",
      "biased_text = artificially_bias_text(target_text) \n",
      "print(biased_text)\n",
      "# \"The policemen helped the firemen\"\n",
      "\n",
      "# Train model to map biased text to original gender-fair text\n",
      "model = Seq2SeqModel()\n",
      "model.train()\n",
      "# ...train loop\n",
      "\n",
      "# 2. Round-trip Augmentation\n",
      "# ---------------------------\n",
      "\n",
      "from transformers import MarianMTModel\n",
      "\n",
      "src_lang = \"German\"\n",
      "pivot_lang = \"English\" \n",
      "\n",
      "# Get gender-fair German text\n",
      "de_text = \"Die Polizist*innen halfen den Feuerwehrleuten\"  \n",
      "\n",
      "# Translate to English (drops gender markings)  \n",
      "en_text = translate_de_en(de_text) \n",
      "# en_text: \"The police helped the firefighters\"\n",
      "\n",
      "# Translate back to German (introduces gender bias)\n",
      "biased_de_text = translate_en_de(en_text)  \n",
      "# biased_de_text: \"Die Polizisten halfen den Feuerwehrmännern\"\n",
      "\n",
      "# Train seq2seq model on parallel (biased_de_text, de_text)\n",
      "\n",
      "model = Seq2SeqModel() \n",
      "model.train()\n",
      "# ...train loop\n",
      "\n",
      "# Additional techniques from paper like merging and prompting could be added\n",
      "\n",
      "```\n",
      "\n",
      "This shows implementations of the two key data augmentation techniques for training gender de-biasing seq2seq models from the paper. The code modularizes the components, uses descriptive names matching paper terminology, and contains comments explaining logic flow. It could be enhanced with additional testing, model variations, visualizations, etc. But overall it faithfully translates key concepts to code.\n"
     ]
    }
   ],
   "source": [
    "print(responses_free[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b055c126-2d22-4cec-b39e-611bb7540033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and write a podcast to be read by a speech to text service (e.g. \"This paper focuses on...\").\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, write a detailed and creative podcast to be read by a speech to text service like Amazon Polly text to speech service in <podcast-script></podcast-script> tags.\n",
    "Write the podcast as a discussion between two people, the \"Presenter\" and \"Guest\". Give them separate personalities.\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing the podcast:\n",
    "1. Carefully read and comprehend the research paper. Make sure you fully understand the key concepts, methodology, results, and conclusions. As you read, highlight important parts and jot down questions or points you'd like to expand on.\n",
    "2. Outline the key sections to cover in your podcast. For example: Background, Problem Statement, Methods, Results, Discussion, Conclusions. Under each heading, make bullet points of specifics to discuss.\n",
    "3. Write a script that flows logically from one idea to the next. Introduce the topic and give relevant background first. Then explain the research questions/problems. Walk through the methodology and results. Discuss what the results mean. Wrap up with conclusions, limitations, and future work.  \n",
    "4. Include conversational language, analogies and examples that make concepts clear for a general audience. For a natural language processing paper, perhaps compare word embeddings to elementary math concepts. Or analogize hidden Markov models to guessing what's in a wrapped gift box based on subtle sounds from shaking it. \n",
    "5. Read the script aloud naturally and record with your preferred text-to-speech program. Break it into logical sections - introduction, methods, etc. Add some improvised commentary for color. \n",
    "6. Edit the computer-generated speech audio file to polish, set proper pacing between sections, add faint background music.\n",
    "7. Use the transcript of your script to create titles, descriptions and time-links for key sections to create a good podcast experience.\n",
    "An example topic could be a paper on using neural networks for autonomous vehicle navigation. You'd explain background on self-driving cars, the specific issues in the paper, solutions and techniques the paper proposes, what results mean for the field, limitations and next steps. You could give examples people relate to, like navigating tricky scenarios on the road. The end product is an engaging, accessible podcast making the essence of the research paper clear and interesting to non-experts.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "576ed0ff-511f-4ec1-8247-b415690f941c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses_pod1 = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_pod1.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62b7528f-5ac7-41ca-9ec0-f6ce068a971d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". To dive into the details, I have with me my colleague who is an expert in this field. Welcome! Please introduce yourself to our listeners.\n",
      "\n",
      "Guest: Hello everyone! I'm thrilled to be here. My name is Amy and I'm a researcher in deep learning, focusing on sequence modeling which is crucial for language tasks. \n",
      "\n",
      "Presenter: Wonderful! As the paper title suggests, we'll be talking about an approach called Mamba that can model long sequences in linear time. But before jumping in, let's provide some background. Can you explain what sequence modeling means and why it's important?\n",
      "\n",
      "Guest: Sure! Sequence modeling refers to machine learning techniques that can process sequential data like text, audio, video etc. where the order matters. It has become the backbone of natural language processing. For example, to predict the next word in a sentence, models need to understand previous context.\n",
      "\n",
      "Presenter: Makes sense! And this paper focuses specifically on modeling much longer sequences than what current systems can handle well. What are some challenges with that?  \n",
      "\n",
      "Guest: Good question! State-of-the-art models like Transformers face quadratic growth in computation as sequence length increases. Their attention mechanism relating all tokens to each other also requires storing past activations to process future steps. This causes slow training and inference.\n",
      "\n",
      "Presenter: Right, those limitations impact real-world applications. So what approach does Mamba take instead?\n",
      "\n",
      "Guest: The key ideas are using selective state space models, a parallelizable scan operation, and simplifying model architecture. State spaces generalize RNNs to higher dimensionality for greater expressivity balanced with efficiency. The selection mechanism lets models focus on relevant tokens and ignore others, compressing critical information in the state. This compression allows handling longer sequences in linear time.\n",
      "\n",
      "Presenter: Making models selectively focus reminds me of how our brains tune out noise! What were the paper's main findings?\n",
      "\n",
      "Guest: Experiments demonstrated Mamba's ability to model text, audio and genomics data better than Transformers of comparable size. It solved tasks needing long-term memorization and could extrapolate patterns to sequences 4000x longer than seen during training! All while having up to 5x faster inference.\n",
      "\n",
      "Presenter: Incredible results! Being able to extrapolate far beyond observed lengths shows great generalization. What do you think is most exciting or promising about this research direction?\n",
      "\n",
      "Guest: I'm most excited by potential to advance foundation models like GPT-3 for new modalities requiring very long context. Applications such as medical time series analysis or genome sequencing could benefit enormously from these efficient architectures. And the flexibility to model images, video, speech and other temporal data with one unified approach is appealing. \n",
      "\n",
      "Presenter: Agreed, that flexibility opens lots of possibilities! Of course further advances are needed too. What do you see as open challenges?\n",
      "\n",
      "Guest: Mamba showed great performance at model sizes up to 1.4 billion parameters. But the field is now exploring trillion parameter models, so continuing experiments at larger scale would be valuable. Also assessing how well qualities like fast adaptation and robustness to distribution shifts carry over from Transformers.\n",
      "\n",
      "Presenter: Scaling up and probing those emergent behaviors will be fascinating areas! This has been an illuminating discussion about enabling more powerful, efficient sequence modeling. Thank you so much for distilling this paper for our audience!\n",
      "\n",
      "Guest: My pleasure! I enjoyed exploring how selective state space models like Mamba might shape the future of AI.\n",
      "\n",
      "</podcast-script>\n",
      "\n",
      "The podcast script frames the key ideas from the research paper as an accessible discussion between two people. It introduces background, explains core concepts and results at a high level using analogies and examples, discusses implications, open challenges and future directions. The conversational tone and division of explanation between two voices aims to make complex details more engaging. The script could be automatically synthesized to speech and supplemented with edits for a polished podcast making technical research more understandable to general audiences.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00fcc05b-7f06-4b48-a311-e98e6c5a3ef0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"On Optimal Frame Conditioners\".\n",
      "\n",
      "<voice emotion=\"excited\">\n",
      "Guest: Hi! Great to join you today to talk about this paper on frames and convex optimization. \n",
      "</voice>\n",
      "\n",
      "Presenter: Wonderful! Let's start with some background. This paper focuses on the problem of converting a frame - which is a set of vectors used to represent signals - into an \"optimally conditioned\" tight frame by rescaling the vectors. \n",
      "\n",
      "<voice emotion=\"happy\">\n",
      "Guest: Right! Tight frames are like an optimal, compact basis for signal representation. The paper shows how to formalize frame rescaling as a convex optimization problem, which allows bringing powerful techniques to bear on getting nice tight frames.\n",
      "</voice>\n",
      "\n",
      "Presenter: Excellent point! The authors show how properties like sparsity of the rescaled frame can be encouraged by careful design of the optimization objective function. This is a very flexible approach.\n",
      "\n",
      "<voice emotion=\"excited\"> \n",
      "Guest: Yes, you can even use barrier functions in the objective to retain more or all of the frame vectors, instead of zeroing some out like with sparse solutions. The math may look scary but it's just constructing a clever optimization problem!\n",
      "</voice>\n",
      "\n",
      "Presenter: Too true! Let's unpack the methods a bit - they use things like augmented Lagrangians and log-barrier functions to solve the optimization programs. And the results show how their techniques can successfully scale random frames.\n",
      "\n",
      "<voice emotion=\"happy\">\n",
      "Guest: Nice explanation! I think analogies help intuit these advanced techniques, like imagining shaking a box to guess the contents. The results are also cool - they seem to hint at theoretical limits on the minimum number of vectors needed to rescale a frame.  \n",
      "</voice>\n",
      "\n",
      "Presenter: What an apt analogy! I think you're exactly right about the results - they empirically show those kinds of minimum bounds. More work could be done to prove them theoretically. Well I think that nicely covers the key parts of the paper - any last thoughts?\n",
      "\n",
      "<voice emotion=\"excited\">\n",
      "Guest: Just that it's promising to connect optimization theory with frame theory! Rescaling frames comes up in many applications so this is valuable. And the convex optimization viewpoint enables bringing a whole toolbox of techniques to tackle problems in frame theory. Exciting stuff!\n",
      "</voice>\n",
      "\n",
      "Presenter: I completely agree! Optimization and frames is a fruitful combo. Well thanks for an enlightening discussion - until next time!\n",
      "\n",
      "<voice emotion=\"happy\">\n",
      "Guest: Thank you, this was fun! Looking forward to our next paper dive. \n",
      "</voice>\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bf56b40-7bc9-4998-a59b-61bdcedca151",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\". How are you doing today, my intelligent friend?\n",
      "\n",
      "Guest: I'm doing quite splendid, thank you! This paper discusses some fascinating new techniques for improving reasoning capabilities in large language models. I'm keen to dive in!\n",
      "\n",
      "Presenter: Wonderful! As artificial intelligence continues advancing rapidly, researchers are exploring how to best leverage large language models like GPT-3 and GPT-4 for complex problem solving. However, these models still rely on a simple left-to-right token prediction process during text generation. \n",
      "\n",
      "Guest: Precisely! And while this token-by-token approach works pretty well for many tasks, the authors argue it can fall short when more deliberative planning or search is needed to find solutions.\n",
      "\n",
      "Presenter: Right. So to address this, the authors propose a framework called \"Tree of Thoughts\" or ToT for short. The key idea is to represent the intermediate reasoning steps as branches in a tree.\n",
      "\n",
      "Guest: Yes, with each \"thought\" being a coherent chunk of text or reasoning that brings the model closer to a final solution. And by searching over this tree, the model can explore multiple lines of reasoning before deciding on the most promising path forward.\n",
      "\n",
      "Presenter: The authors showcase ToT on three novel challenging tasks. For example in a mathematical game called \"Game of 24\", ToT boosted solving success from just 4% to 74% compared to regular prompting approaches!\n",
      "\n",
      "Guest: Impressive! The results across creative writing and crossword tasks were also stronger for ToT. The modular framework seems to help substantially when problems require more deliberate planning.  \n",
      "\n",
      "Presenter: For sure. ToT offers flexibility too - you can customize the search algorithms and how thoughts are generated or evaluated. So it can adapt to different tasks.\n",
      "\n",
      "Guest: Agreed! I think ToT opens up some exciting avenues to augment language models with stronger reasoning and search capabilities. The authors view it as adding more of a \"System 2\" deliberative layer atop the \"System 1\" reflexive strength these models already have.\n",
      "\n",
      "Presenter: Well put! With models like GPT-4 already exceeding human performance on various benchmarks, pushing their reasoning frontiers with frameworks like ToT could unleash even more potential.\n",
      "\n",
      "Guest: I concur! Of course ToT does have limitations, like added computational costs. But as models and compute improve, the prospects are bright for deliberative problem solving. Maybe it helps inch us closer to Artificial General Intelligence!\n",
      "\n",
      "Presenter: Hopefully so! This was a fascinating discussion. Tune in next week when we'll be unpacking another cutting edge AI paper.\n",
      "</podcast-script>\n",
      "\n",
      "This podcast script summarizes the key points from the research paper in an engaging back-and-forth dialogue. The Presenter introduces the topic, then the Guest provides more details on the Tree of Thoughts methodology. They discuss the results, flexibility and future promise of the approach. The script aims to explain the essence of the technical paper in an accessible way for a general audience. It has conversational language, an logical flow between sections, and an upbeat, forward-looking tone. With polished text-to-speech audio and supplementary podcast production, this script serves as strong foundation for an appealing AI research overview podcast.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52ed017c-ac3e-4750-a070-e9f64f3ed382",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is called \"LORA: Low-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS\". Our guest today is Dr. Smith, a leading expert in natural language processing. Welcome to the show!\n",
      "\n",
      "Guest: Thank you for having me! I'm excited to discuss this interesting paper on adapting large language models.\n",
      "\n",
      "Presenter: To start, can you give us a high-level overview of what this paper is about? \n",
      "\n",
      "Guest: Sure! As language models like GPT-3 have grown enormously in size, fine-tuning them for downstream tasks becomes very expensive. This paper proposes an efficient adaptation method called LoRA that keeps most parameters frozen and injects smaller trainable matrices.\n",
      "\n",
      "Presenter: Interesting! Can you walk us through the key ideas?\n",
      "\n",
      "Guest: The core idea is that weight changes during fine-tuning tend to be low-rank. So LoRA decomposes these changes into smaller factor matrices. By only training those extra matrices, it adapts models with far fewer parameters.\n",
      "\n",
      "Presenter: I see - very clever! What were the authors trying to improve over past work? \n",
      "\n",
      "Guest: Well, methods like adapter tuning add layers that increase latency. Optimizing just the input embeddings limits sequence length. LoRA aims for efficiency without these downsides.\n",
      "\n",
      "Presenter: Makes sense. Can you outline the methodology and experiments?\n",
      "\n",
      "Guest: Sure. The authors injected trainable low-rank matrices into various Transformer models like GPT-3, RoBERTa, and DeBERTa. They evaluated on tasks like GLUE benchmark, summarization, and WikiSQL across natural language understanding and generation.\n",
      "\n",
      "Presenter: Great summary! What were the major results and conclusions?\n",
      "\n",
      "Guest: LoRA matched or beat fine-tuning baselines while reducing GPT-3's trainable parameters by 10,000 times. It had no extra inference latency and enabled fast task switching. So it achieved the efficiency goals without sacrificing accuracy.\n",
      "\n",
      "Presenter: Fantastic! What do you see as the limitations and open questions?  \n",
      "\n",
      "Guest: Well, there could be better ways to determine which weights to adapt. And it's not fully clear why low-rank updates work so well during fine-tuning. More investigation there could improve things further.\n",
      "\n",
      "Presenter: Makes sense - lots of potential to build on this. Well this has been super insightful! Any last takeaways around LoRA and large language models?\n",
      "\n",
      "Guest: I'm excited by the possibilities! Being able to efficiently adapt models with billions of parameters opens up new applications in search, dialogue, creative writing...even outside NLP. And the communication barriers drop as AI grows more customizable.\n",
      "\n",
      "Presenter: I couldn't agree more - very promising work. This concludes our discussion on efficient adaptation of large language models using LoRA. Thank you so much for listening!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99ca7190-88b8-4ac5-99dc-3e8031bada0d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"ReLoRA: High-Rank Training Through Low-Rank Updates\". It is very interesting research on an efficient way to train large neural networks. To help us understand this paper, I have with me Dr. Smith, an expert in machine learning. Welcome Dr. Smith!\n",
      "\n",
      "Guest: Thank you for having me! I'm looking forward to discussing this intriguing work.\n",
      "\n",
      "Presenter: Great! To start off, can you give us some background on why training large neural networks efficiently is an important research problem?\n",
      "\n",
      "Guest: Sure. In recent years, we've seen massive neural networks with billions of parameters achieve impressive performance on complex tasks like language understanding and computer vision. However, training these giant models requires prohibitive amounts of computational resources that only a few organizations can access. So finding techniques to train large networks efficiently makes the powerful AI they enable more accessible.  \n",
      "\n",
      "Presenter: Okay, that context is helpful. What exactly does this paper do to address that problem? \n",
      "\n",
      "Guest: The key contribution is a method called ReLoRA. It performs low-rank updates to train high-rank networks. Essentially, at any point it trains only a small subset of parameters - which takes less computation - but over the full training trajectory, combines these to equal training the whole model.\n",
      "\n",
      "Presenter: Hmm, interesting idea. Can you explain what low-rank versus high-rank means here? I'm picturing matrix ranks from linear algebra class but not sure if that directly applies.\n",
      "\n",
      "Guest: Good intuition! Yes this builds on basic matrix math ideas. The rank determines roughly how complex or expressive a matrix, like a neural network layer, is. Low-rank means only a few key directions or concepts are modeled. High-rank means modeling nuanced, complex patterns needing many directions. ReLoRA does localized low-rank training but accumulates these to capture the capacity of the full high-rank network. It's like painting a detailed portrait through a series of simple quick sketches from different angles.  \n",
      "\n",
      "Presenter: Ah I see, quite a clever approach! Walk us through how ReLoRA actually works then.\n",
      "\n",
      "Guest: Sure! First the network is briefly trained normally to initialize things well. Then ReLoRA training begins. At each step, only a small percentage of parameters are trainable, the rest stay fixed. It trains this low-rank portion for a while, then merges those parameters into the full network and reinitializes to train a different random subset next. It repeats this process, accumulating capacity to mimic high-rank training. Some special tricks like learning rate warmups and partial optimizer resets make this work smoothly.\n",
      "\n",
      "Presenter: Okay that's clear, thanks! What were the big results then? Does ReLoRA actually match full network training performance?\n",
      "\n",
      "Guest: Yes, quite effectively! They carefully evaluated ReLoRA on transformers up to 1.3 billion parameters. Across the board ReLoRA matched or very closely trailed the performance of full-rank training. And it improved substantially over standard low-rank training. They also showed computational speedups in practice. Surprisingly, efficiency gains grew for larger models - making ReLoRA promising for huge networks.  \n",
      "\n",
      "Presenter: Impressive! That sounds very useful then both to democratize access to capable models and push size frontiers. Were there any key limitations? \n",
      "\n",
      "Guest: The big caveat is they only demonstrate pre-training so far. Applying ReLoRA effectively to downstream fine-tuning is still unvalidated. And we'd like more analysis - is it learning fundamentally different representations? Why doesn't online ReLoRA with constant rank cycling work better? But overall this is quality work tackling a very relevant problem.\n",
      "\n",
      "Presenter: Great overview! We're almost out of time but to wrap up - what might be next steps for developing this type of approach?\n",
      "\n",
      "Guest: Indeed good question! Two promising directions could be combining ReLoRA with other efficient training methods like mixture-of-experts or distillation. And expanding empirical study - trying on harder datasets, pushing to multi-trillion scale models, and testing whether insights transfer to computer vision domains. There's active exploration ahead!   \n",
      "\n",
      "Presenter: Wonderful! Well this has been an enlightening discussion about an impactful research contribution. Thank you so much for sharing your expertise Dr. Smith! \n",
      "\n",
      "Guest: My pleasure, thanks for inviting me! Exciting times in AI and machine learning!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e86943ad-b979-4152-b745-269ce1fb810c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"Exploiting biased models to de-bias text: A gender-fair rewriting model\". I am very excited to know more about this topic from our guest today. Welcome to the show! Please introduce yourself.  \n",
      "\n",
      "Guest: Hello everyone! My name is Priya and I am a postdoc researcher at the University of Zurich working on natural language processing and gender bias. Thanks so much for having me!\n",
      "\n",
      "Presenter: No problem at all, Priya! We are very excited to know more about your work. Can you please give a brief background about the key issues surrounding gender bias in NLP models that motivated this research?\n",
      "\n",
      "Guest: Sure! As we know, NLP models which power a lot of language technologies we use everyday like Google Translate or Alexa have been shown to perpetuate gender stereotypes and be discriminative towards certain genders. For example, we found that Google Translate often translates gender neutral Turkish sentences into English with just masculine pronouns like 'he' or 'him'. Similarly, occupational stereotypes like 'doctor' being translated as just the masculine variant in several languages. \n",
      "\n",
      "Presenter: Wow, that's quite problematic! So what solutions have researchers proposed to address such gender bias issues in NLP models?\n",
      "\n",
      "Guest: Broadly there are 3 approaches - a) curate balanced datasets b) modify model architectures and training procedures c) post-process model outputs. We focus on the last approach of 'gender fair rewriting models' which take biased text as input and rewrite it into gender neutral language by changing words like masculine pronouns to gender inclusive pronouns.\n",
      "\n",
      "Presenter: Ah I see, very interesting! Can you explain in simple terms what were the limitations of previous gender fair rewriting models that you identified and aimed to address in your work?\n",
      "\n",
      "Guest: Most previous rewriting models rely on handcrafted linguistic rules to transform input text into gender fair language. This becomes very complex for languages like German that have more complex grammar and morphology than English. Additionally, such rule-based models don't generalise well beyond the rules defined by experts. \n",
      "\n",
      "Presenter: Okay that makes sense. Can you now explain the key ideas you proposed to overcome these limitations?\n",
      "\n",
      "Guest: Yes, we basically exploited the fact that current NLP models are biased to automatically create gender unfair text from existing gender fair text. We simply passed German text through English translation models which often predict masculine terms and then translate that back to German. This gave us artificial biased text to train rewriting models without needing complex rules!\n",
      "\n",
      "Presenter: Wow, that's such a clever trick! Instead of debiasing you exploited existing bias and kind of did biasing in reverse! Can you share what results you found on evaluating such models?\n",
      "\n",
      "Guest: We found our German rewriting model trained with this clever data creation technique performed better than previous rule-based systems! And in a human study with over 250 people, they preferred the automatically rewritten text over original biased text even if it had some errors. \n",
      "\n",
      "Presenter: Fascinating! With that we come to the end of this wonderful discussion. Thank you so much Priya for explaining your research on exploiting bias for debiasing NLP models. I wish you all the best for your future work! \n",
      "\n",
      "Guest: Thank you so much for having me. It was my pleasure discussing our work and I hope it gave some new insights into addressing gender bias in NLP.\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod1[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c06f8a9-5948-4cd0-8e2d-3458fbbb6112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2612\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and write a podcast to be read by a speech to text service (e.g. \"This paper focuses on...\").\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, write a detailed and creative podcast to be read by a text to speech service like Amazon Polly in <podcast-script></podcast-script> tags.\n",
    "Write the podcast as a deep discussion between two people, the \"Presenter\" and \"Guest\". Give them separate personalities. An episode of the podcast should last close to 20 minutes.\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing the podcast:\n",
    "1. Carefully read and comprehend the research paper. Make sure you fully understand the key concepts, methodology, results, and conclusions. As you read, highlight important parts and jot down questions or points you'd like to expand on.\n",
    "2. Outline the key sections to cover in your podcast. For example: Background, Problem Statement, Methods, Results, Discussion, Conclusions. Under each heading, make bullet points of specifics to discuss.\n",
    "3. Write a script that flows logically from one idea to the next, but adds enough detail to make a lengthy episode. Introduce the topic and give relevant background first. Then explain the research questions/problems. Walk through the methodology and results. Discuss what the results mean. Wrap up with conclusions, limitations, and future work.  \n",
    "4. Include conversational language, analogies and examples that make concepts clear for a general audience. For a natural language processing paper, perhaps compare word embeddings to elementary math concepts. Or analogize hidden Markov models to guessing what's in a wrapped gift box based on subtle sounds from shaking it. \n",
    "5. Read the script aloud naturally and record with your preferred text-to-speech program. Break it into logical sections - introduction, methods, etc. Add some improvised commentary for color. \n",
    "6. Use the transcript of your script to create titles, descriptions and time-links for key sections to create a good podcast experience.\n",
    "An example topic could be a paper on using neural networks for autonomous vehicle navigation. You'd explain background on self-driving cars, the specific issues in the paper, solutions and techniques the paper proposes, what results mean for the field, limitations and next steps. You could give examples people relate to, like navigating tricky scenarios on the road. The end product is an engaging, accessible podcast making the essence of the research paper clear and interesting to non-experts.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac8e11be-1c9e-43ed-bd59-ecb2cd1208e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses_pod2 = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_pod2.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68368e14-ed83-486c-a053-5b0dae43074f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces'. Let me introduce my co-host for today's podcast, Albert. Albert is an expert in the field of machine learning and neural networks. Welcome Albert!\n",
      "\n",
      "Guest: Thanks for having me! I'm looking forward to discussing this interesting paper on selective state space models. \n",
      "\n",
      "Presenter: Great! To start with, can you give us a high level overview of what this paper is about? What problem does it aim to solve?\n",
      "\n",
      "Guest: Sure! This paper introduces a new class of models called selective state space models or S6 models. The goal is to develop sequence models that are both effective, like Transformers, and efficient - with linear scaling in sequence length.\n",
      "\n",
      "The key insight is something the authors call a selection mechanism. This allows models to selectively focus on or ignore parts of the input sequence, compressing key information into a smaller state representation. This makes models much more efficient computationally.\n",
      "\n",
      "Presenter: Interesting! Can you explain what you mean by sequence models and state spaces? I think our audience would benefit from some background there.\n",
      "\n",
      "Guest: Good point! Sequence models are machine learning models that operate on sequential data - like text, audio, video etc where the order matters. Examples are RNNs, CNNs and Transformers. \n",
      "\n",
      "A state space refers to an internal latent representation. As a sequence model processes more inputs over time, this state evolves to summarize previous context. The size of this state affects model efficiency.\n",
      "\n",
      "Does this help provide some context? I'm happy to clarify anything else!\n",
      "\n",
      "Presenter: Yes, that's super helpful! So Transformers are effective but inefficient due to their large states that grow with sequence length. This paper aims to get the benefits of Transformers with lower computational costs using this selection mechanism idea. Is that an accurate summary?\n",
      "\n",
      "Guest: Exactly! You got it. The selection mechanism lets models compress information more intelligently as opposed to just storing everything like Transformers do.\n",
      "\n",
      "Now the authors incorporate this idea into state space models. They show how it maps nicely to the gating mechanisms used in RNNs. Overall, selective state space models or S6 models combine ideas from Transformers, RNNs and other sequence models into one elegant framework.\n",
      "\n",
      "Presenter: Wow, sounds powerful! Can you walk us through some key results and what they mean? Did these S6 models live up to their promises? \n",
      "\n",
      "Guest: Absolutely! The results across language, audio and genomics were extremely promising. On language modeling, their Mamba architecture with S6 models matched or exceeded strong Transformer baselines on perplexity and downstream tasks. And it had 5x higher throughput for inference since it doesn't need to store full context.\n",
      "\n",
      "For audio, it reduced Fréchet inception distance on speech generation by over 50% compared to GANs and diffusion models. And it kept improving with up to minute-long context.\n",
      "\n",
      "So yes, S6 models seem versatile enough to serve as a general sequence model backbone for foundation models across modalities. The linear scaling is crucial for pushing boundaries in terms of context length.\n",
      "\n",
      "Presenter: That's remarkable! This sounds like a big step towards more efficient and scalable sequence models. What do you see as some promising future directions for this work? Any limitations?\n",
      "\n",
      "Guest: I'm excited by potential affordances in fine-tuning, prompting and other model interactions. It's not clear yet if S6 models have the full range of beneficial properties we've come to expect from Transformers. So investigating that is important.\n",
      "\n",
      "In terms of limitations - the inductive biases helpful for continuous signals may hamper discrete data, and vice versa. Finding the right tradeoffs across modalities is an open challenge. Extending strong performance to model sizes and sequence lengths orders of magnitude larger also requires more work on optimization and engineering.\n",
      "\n",
      "But overall this is a flexible framework with a bright future! I'll be following work on selective state space models closely in coming years.\n",
      "\n",
      "Presenter: As will I! This has been so interesting and enlightening. Thank you again Albert for breaking this all down - I feel I have a much stronger grasp now on this promising research direction.\n",
      "\n",
      "Guest: My pleasure! Always great discussing the latest innovations pushing machine learning forward.\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9164dae0-69a0-4198-a66c-d4b451ba6f4b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is 'On Optimal Frame Conditioners'. \n",
      "Guest: Yes, it's an interesting paper that provides a new approach to convert non-tight frames into tight frames using optimization techniques. Shall we start with some background?\n",
      "\n",
      "Presenter: Sure, that will help set the context. Frames are important concepts in signal processing that provide redundant representations. Tight frames are optimally conditioned frames, but converting general frames into tight frames is an open challenge. \n",
      "\n",
      "Guest: Exactly. This paper reformulates the tight frame conversion problem as a convex optimization problem which can be efficiently solved. They provide some elegant formulations using linear and barrier programs.\n",
      "\n",
      "Presenter: Hmm interesting. Can you explain what they mean by scalable frames and how it connects to tight frames? I think that context will help listeners understand the crux of the paper.\n",
      "\n",
      "Guest: Good point. A scalable frame is one whose vectors can be rescaled to convert it into a tight frame. So the paper aims to find the optimal scalings, formulated as optimization problems. The solutions then provide the desired tight frame.\n",
      "\n",
      "Presenter: Aha, now it clicks! I have a question though - what are some examples of objective functions they use in the programs? Maximizing sparsity versus density? \n",
      "\n",
      "Guest: Glad the context helps! For the objectives, they show both linear programs that promote sparsity as well as barrier functions that provide dense solutions. The linear program uses weighted L1-norm minimization for example.\n",
      "\n",
      "Presenter: Makes sense, provide options for different use cases. And what solution methods do they employ? I noticed the augmented Lagrangian scheme for example.\n",
      "\n",
      "Guest: You're absolutely right. They leverage various optimization techniques like the simplex algorithm and augmented Lagrangians. The latter method scales very efficiently. I can elaborate more on the algorithms if you're interested.\n",
      "\n",
      "Presenter: A high-level intuition is good enough for our audience I think. More important to grasp the results. So what frames did they test their methods on? Were the scalings effective?\n",
      "\n",
      "Guest: Sure, leaving the technical details. They generated random Gaussian frames and showed visually how the scaling distorts those frames towards tight frames. Tables also quantified sparsity levels and the percentages of frames successfully scaled.\n",
      "\n",
      "Presenter: Great insight on the experiments! As we wrap up, what stood out to you as the key conclusions and potential impact? How does this advance frame theory and applications?\n",
      "\n",
      "Guest: Excellent points to highlight in closing. I'd say the optimization view enables scalable, automatable tight frame conversion with various flexibility in formulation. This could expand their use in signal analysis tasks. Also gives new theoretical understanding.\n",
      "\n",
      "Presenter: Wonderful summary! Listeners, we hope you enjoyed this episode explaining the essence of “On Optimal Frame Conditioners” paper and giving context around it. Please tune in next week as we discuss another exciting paper!\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6206cc0-81a4-40f6-a549-173836a082ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models.\" Let me introduce my guest for today's podcast, Alex. He is an expert in AI and has been following the recent developments in large language models closely. Welcome to the show, Alex!\n",
      "\n",
      "Guest: Thank you for having me! I'm excited to discuss this fascinating paper. \n",
      "\n",
      "Presenter: Before we dive deeper, can you give us a high-level overview of what this paper is about?\n",
      "\n",
      "Guest: Sure! In a nutshell, this paper proposes a new framework called \"Tree of Thoughts\" or ToT that allows large language models to solve problems more deliberately through search and planning. \n",
      "\n",
      "You see, most language models today are trained to generate text sequentially from left to right, one token at a time. This works well for many tasks, but can fall short when more complex reasoning or exploration is needed.\n",
      "\n",
      "The authors draw inspiration from classic AI search techniques to have the model explicitly search over a tree of possible \"thoughts\" - coherent pieces of text that serve as steps towards a solution. The model can evaluate these thoughts to decide which branch of reasoning seems most promising to explore further.\n",
      "\n",
      "So in essence, ToT gives language models a more sophisticated way to plan ahead, backtrack when needed, and systematically reason about problems.\n",
      "\n",
      "Presenter: Fascinating! Can you walk us through a concrete example to make this more tangible?\n",
      "\n",
      "Guest: Sure, let's look at one of the tasks they experiment on called Game of 24. Here the goal is to reach the number 24 using just 4 input numbers and arithmetic operations. \n",
      "\n",
      "With standard prompting, language models struggle since they generate solutions left to right without lookahead. But with ToT, intermediate \"thoughts\" are possible equations. The model can propose multiple options for the next equation, deliberate over them by quickly estimating if they can reach 24, and only keep the most promising ones to explore further in a search tree.\n",
      "\n",
      "This flexibility to pursue different reasoning branches is what makes ToT more powerful.\n",
      "\n",
      "Presenter: Very interesting. So what were the main findings in the paper? Does this approach work better?\n",
      "\n",
      "Guest: Yes, the results are quite amazing! On tasks like Game of 24 and creative writing that require planning and exploration, ToT significantly boosts the problem solving abilities of state-of-the-art models like GPT-4. \n",
      "\n",
      "For instance, on Game of 24 where GPT-4 only solved 4% with standard prompting, ToT achieved a 74% success rate - close to human level performance!\n",
      "\n",
      "The authors also analyze the results to show how the explicit search process lets the model try alternative options instead of getting stuck down an incorrect reasoning path.\n",
      "\n",
      "Presenter: Wow, that's an impressive improvement! What do you think is most exciting or promising about this work on \"Tree of Thoughts\"? \n",
      "\n",
      "Guest: I think the beauty lies in the simplicity yet generality of representing reasoning as search over coherent pieces of language. This builds upon classical AI ideas but also unlocks new capabilities we did not have before with the scale and foundation knowledge modern language models possess. \n",
      "\n",
      "By systematizing and deliberating over language itself, we open the door to tackling even more complex planning and exploration challenges going forward. Language could become not just the interface, but also the scaffolding that structures and guides the thinking process of AI systems. That's what I find really powerful and promising!\n",
      "\n",
      "Presenter: Wonderfully said! Unfortunately we are running out of time, but before we wrap up - any limitations or open questions you see around this approach?\n",
      "\n",
      "Guest: Of course, it's still early work with much room left to explore. Running deliberative search can get expensive, so improving efficiency is important...though costs are coming down rapidly. I'm also curious to see extensions to even harder tasks, and whether we can train models directly with this Tree of Thought style supervision. But overall I think this is a very promising direction that blends classical and modern AI nicely!\n",
      "\n",
      "Presenter: Great insights, Alex! Thank you again for explaining this fascinating work - I feel we've just scratched the surface understanding how models can reason and explore systematically. Listeners, be sure to check the paper out if this caught your interest!\n",
      "\n",
      "Guest: My pleasure! Always wonderful discussing the latest innovations in AI with you.\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2bc06448-6679-4470-b94c-440a0026980e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is called LORA: Low-RANK ADAPTATION OF LARGE LAN-\n",
      "GUAGE MODELS. Let us welcome our guest for today, Dr. Sarah Kline, a leading expert on natural language processing and neural network models. Welcome, Dr. Kline!\n",
      "\n",
      "Guest: Hello! Thank you for having me on the show. I'm excited to discuss this interesting paper with you and the listeners.\n",
      "\n",
      "Presenter: Great! So before we dive into the paper, can you give us a quick 101 on neural network language models? What are they, and why are they useful?\n",
      "\n",
      "Guest: Sure! In a nutshell, language models are AI systems that are trained on massive amounts of text data to understand and generate human language. They power a lot of the natural language processing we see today - things like search engines, chatbots, text auto-completion on your smartphone, and even voice assistants like Siri or Alexa. \n",
      "\n",
      "The state-of-the-art technique is to use deep neural networks for the language modeling task. These models have shown tremendous capabilities, but they rely on having billions of parameters that need to be trained. The more parameters, the better they perform, but that also makes them very computationally expensive.\n",
      "\n",
      "Presenter: Right. And this paper introduces a method to adapt those huge models more efficiently. Can you summarize what problem they were trying to solve?\n",
      "\n",
      "Guest: Exactly. With models like GPT-3 having over 175 billion parameters, it becomes really difficult to fine-tune and customize them for different downstream tasks. Normally you'd have to update all 175 billion parameters, which has massive storage and memory requirements. \n",
      "\n",
      "The authors propose a simple but clever trick for efficient adaptation called LoRA, or Low-Rank Adaptation. The key insight is that model updates during fine tuning tend to be low rank, so you only need to update a small number of parameters to get most of the benefit.\n",
      "\n",
      "Presenter: Whoa, only updating a tiny fraction of parameters sounds amazing! How does LoRA work then, at a high level? \n",
      "\n",
      "Guest: The idea is that instead of updating the full weight matrices directly, you inject smaller low-rank matrices before and after each layer. These low-rank matrices get trained, while most of the pre-trained weights stay frozen. \n",
      "\n",
      "So you train perhaps a rank-one matrix with just a single vector and value, instead of a giant 175 billion parameter model. This acts like a bottleneck that captures the essence of the update. At inference time, the low-rank pieces can be merged back so there's no extra computation.\n",
      "\n",
      "Presenter: Fascinating! And I guess a rank-one update matrix means the effective number of tuned parameters goes down from billions to just thousands?\n",
      "\n",
      "Guest: Exactly! In their experiments, the authors were able to get over 10,000x reduction in trainable parameters for GPT-3 without hurting model performance. So you can efficiently adapt and deploy multiple customized instances of these giant LMs.\n",
      "\n",
      "The other advantage is it's far more memory-efficient to train, since you don't need to store things like optimizer states for the frozen parameters.\n",
      "\n",
      "Presenter: Incredible! No wonder the authors call it a \"breakthrough in efficient ML\". Can you walk us through some key results about how well LoRA works?\n",
      "\n",
      "Guest: Sure! The paper had very extensive experiments on models like GPT-3, RoBERTa and DeBERTa. The consistent finding was LoRA matches or improves the full fine-tuning performance in most cases, sometimes significantly like 3% better on GLUE, while using over 10,000x fewer parameters. \n",
      "\n",
      "They also tested the effect of different low rank values. Surprisingly, very low ranks like 1 or 2 were enough to capture the essence of the update in many cases. Furthermore, LoRA combines easily with other methods like prefix tuning for even better performance.\n",
      "\n",
      "Presenter: That's quite amazing given how simple it sounds! Of course no method is perfect, so what limitations would you highlight? \n",
      "\n",
      "Guest: Well, LoRA hasn't been tested yet on some very specialized domains with limited data. There may be cases where full fine tuning beats it, but more research is needed. There are also open questions around exactly how many and which weights to adapt for best efficiency.\n",
      "\n",
      "Presenter: Makes sense, that's great context. As we come to the end, what do you think is the most promising direction for future work on efficient language model tuning?\n",
      "\n",
      "Guest: In my opinion, methods like LoRA open up a lot of possibilities! Having such inexpensive, customizable large language models unlocks applications we couldn't previously tackle. \n",
      "\n",
      "Next steps would be combining LoRA with things like prompt-based tuning to build task-specific models with minimal data. There's also interest in expanding beyond text to multimodal applications. Overall it's an exciting time in this field!\n",
      "\n",
      "Presenter: Awesome insights! Unfortunately that's all the time we have today. I want to thank you again Dr. Kline for explaining this paper on an esoteric topic so clearly!\n",
      "\n",
      "Guest: You're most welcome! I had a great time discussing this fascinating innovation in efficient deep learning.\n",
      "\n",
      "Presenter: And thanks everyone for tuning in! We'll be back next week with another paper dissection. Bye for now!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e66ec42d-6352-409a-82fd-6500bec9891a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"ReLoRA: High-Rank Training Through Low-Rank Updates\". To discuss this paper with us today, we have a special guest with us - [insert creative guest name]. Welcome to the show!\n",
      "\n",
      "Guest: Thank you for having me! I'm excited to discuss this interesting paper on efficient neural network training.  \n",
      "\n",
      "Presenter: Great! Let's start by setting the context. In recent years, neural network models have grown massively in size, with models reaching hundreds of billions of parameters. However, training these giant models requires prohibitive computational resources. So the key question is - do we really need such overparameterized models? Can we train high performance models more efficiently?\n",
      "\n",
      "Guest: Exactly! This paper introduces a method called ReLoRA that aims to do just that. It explores more parameter-efficient techniques to train large neural language models. Specifically, ReLoRA performs low-rank weight updates to train high rank networks. \n",
      "\n",
      "Presenter: Whoa, slow down there with the jargon! Before we dive deeper, it may help to take a step back and explain what we mean by model parameters, rank and weight updates. \n",
      "\n",
      "Guest: Good point! Let me try explaining it in simple terms. When we train neural networks, we update the weight parameters to minimize loss. The number of tunable weight parameters determines model size. Now, rank roughly represents the model's capacity to fit complex patterns. With regular or \"full-rank\" updates, all parameters get updated, needing a lot of computation. But ReLoRA does low-rank updates where only a subset of parameters are updated at a time. This saves computation and memory while preserving model capacity over multiple aggregate updates.\n",
      "\n",
      "Presenter: Aha, that makes sense! Updating just a parameter subset is like changing a few scattered pixels of an image instead of all pixels at once. But eventually, most pixels get updated to reveal the full image. Cool idea! So then does ReLoRA actually work? What were the paper's main findings?\n",
      "\n",
      "Guest: Yes, the results are very promising! ReLoRA was tested on transformer language models with up to 1.3 billion parameters. It achieved similar performance as full-rank training, while using far fewer trainable parameters per update. For the 1.3B model, it reached the same perplexity score as full-rank training while only updating 250 million parameters at once!\n",
      "\n",
      "Presenter: Wow, training a billion parameter model with a fraction of the usual cost - that's a big deal! And probably just scratches the surface of other applications. What other kinds of models do you think ReLoRA could benefit? \n",
      "\n",
      "Guest: I think it has huge potential for large generative models like GPT-3 and image/video models as well. Though one key limitation is ReLoRA was only evaluated on language models here. Testing it more broadly would be an important next step. But overall, this feels like an exciting step towards more accessible training of gigantic neural networks!\n",
      "\n",
      "Presenter: I totally agree! The democratization of AI is such an important goal. Well this has been a fascinating discussion on efficiently training giant neural networks. Huge thanks to [guest name] for explaining this cutting edge research. We'll include a link to the ReLoRA paper in the episode notes for anyone who wants to dig deeper. See you next time!\n",
      "\n",
      "Guest: Thank you for having me! It was fun chatting about this cool approach. Can't wait for the next episode!\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "702e6e37-6fd8-4ccf-8bff-33d84ad263e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>Presenter: Hi! I hope you all are doing well. The title paper of today's discussion is \"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model\" by researchers at the University of Zurich and the company Textshuttle. And joining me is my esteemed guest, Dr. Amanda Smith. Welcome Dr. Smith!\n",
      "\n",
      "Guest: Thank you for having me! I'm happy to discuss this timely topic with you.\n",
      "\n",
      "Presenter: For our listeners, Dr. Smith is a professor of linguistics who specializes in gender and language. So let's dive in! Dr. Smith, can you give us a high-level overview of what this paper is about?\n",
      "\n",
      "Guest: Of course! This paper focuses on how to make AI text generation systems produce less biased, more gender-fair language. The authors train models that can take an input text, like from another AI system, and rewrite it to use more inclusive pronouns and job titles. \n",
      "\n",
      "Presenter: Fascinating! Can you explain why that's important? Don't we want AI to just reflect the way real people talk?\n",
      "\n",
      "Guest: That's a great question. While AI models today mirror biases in society when trained on human language data, research shows gender-biased language can negatively stereotype groups and limit children’s career aspirations. More inclusive language leads to more equal perceptions. So it’s an area needing improvement.\n",
      "\n",
      "Presenter: I see, this technology could lead to positive change then! What clever approach did the researchers take?\n",
      "\n",
      "Guest: Up till now, most systems required hand-crafted rules to map pronouns and job titles to less biased forms. But that process requires linguistic expertise and doesn’t easily generalize across languages. Instead, these researchers exploited existing biased AI models to generate the parallel training data needed, avoiding manual rules. \n",
      "\n",
      "Presenter: Fascinating! Can you walk us through how that process worked? I’d love a clear, tangible example.\n",
      "\n",
      "Guest: Sure, let’s take machine translation as an example. When you translate a German sentence with feminine endings to English and back, the gender-fair markings get lost, since English doesn’t show gender like that. The result mirrors society's bias towards masculine default forms. So the researchers use that output as the biased source, paired with the original fair sentence as the target. Now they have training data!\n",
      "\n",
      "Presenter: What an ingenious trick! Using biased models against themselves. Did this approach prove successful then?\n",
      "\n",
      "Guest: Yes, their method matched or outperformed previous hand-engineered systems in English and German. And in a survey of almost 300 people, over 98% said they’d prefer the imperfect but more inclusive outputs over unaltered biased text. So there’s great promise to increase fairness in AI generation.\n",
      "\n",
      "Presenter: That’s so empowering! Of course, no model’s perfect. What limitations or ethical considerations remain?\n",
      "\n",
      "Guest: Well, their approach relies on available gender-fair source texts, which could be scarce in some languages. And while intended for research, they caution real-world deployments should be rigorously tested by users first. We must thoughtfully craft language technology that respects diversity of identity and expression. But overall this enables progress in that direction!\n",
      "\n",
      "Presenter: Wonderful analysis, thank you Dr. Smith! Let's take a quick break, then dive into discussing what findings like these mean for AI ethics and policymaking moving forward. Stay tuned listeners!\n",
      "\n",
      "<break for commercials>\n",
      "\n",
      "Presenter: And we're back discussing this cutting-edge research on reducing gender bias in AI text generation systems. I'm here with expert Dr. Amanda Smith. So Dr. Smith, what broader impacts might advances like these have on society? Do you see potential for real change?\n",
      "\n",
      "Guest: Absolutely! Work like this shows we can take active steps to make AI language technology more inclusive. As the authors say, similar techniques could combat biases around race, culture or disability too. It gives me hope we can craft systems that empower everyone.\n",
      "\n",
      "Presenter: Me too! Though certainly more open questions remain right? What do you see as priorities for the research community?\n",
      "\n",
      "Guest: There's still lots we must understand. How well do evaluations like these translate to real-world benefits for affected groups? Can we quantify if language technology harms or helps gender minorities? More user studies are crucial. And we desperately need more diversity in the field itself!\n",
      "\n",
      "Presenter: Excellent points. If you had a magic wand, what policy or regulation would you enact around language AI?\n",
      "\n",
      "Guest: I'd love to see incentives and funding for research explicitly targeting inclusion issues. And requirements that companies assess and mitigate algorithmic bias risks, including in language systems. Getting more marginalized voices and domain experts like sociolinguists directly involved in development is key too!\n",
      "\n",
      "Presenter: All wonderful ideas. This has been an enlightening discussion on using AI to further equality. Let's keep it going! I welcome our listeners to tweet thoughtful reflections and opinions. Use the hashtag #podcastforgood. What positive changes do you hope to see in AI? How can we build a more just future? The journey continues, together!\n",
      "\n",
      "Guest: Well said, I look forward to the conversation. Thanks for having me on!\n",
      "\n",
      "Presenter: Thank you Dr. Smith! And thanks listeners for tuning in today. We'll be back soon with more on ethics in artificial intelligence. Until then, be well and make progress!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod2[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "27887abe-f665-4c37-ac75-9b12a52737e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following research paper and write a podcast to be read by a speech to text service (e.g. \"This paper focuses on...\").\n",
    "\\nPaper:<paper>INPUT_PAPER</paper>\\n\\nFrom the above paper, write a detailed and creative podcast to be read by a text to speech service like Amazon Polly in <podcast-script></podcast-script> tags.\n",
    "Write the podcast as a deep technical discussion and presentation of the topic. An episode of the podcast should last close to 20 minutes.\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing the podcast:\n",
    "1. Carefully read and comprehend the research paper. Make sure you fully understand the key concepts, methodology, results, and conclusions. As you read, highlight important parts and jot down questions or points you'd like to expand on.\n",
    "2. Outline the key sections to cover in your podcast. For example: Background, Problem Statement, Methods, Results, Discussion, Conclusions. Under each heading, make bullet points of specifics to discuss.\n",
    "3. Write a script that flows logically from one idea to the next, but adds enough detail to make a lengthy episode. Introduce the topic and give relevant background first. Then explain the research questions/problems. Walk through the methodology and results. Discuss what the results mean. Wrap up with conclusions, limitations, and future work.  \n",
    "4. Include conversational language, analogies and examples that make concepts clear for a general audience. For a natural language processing paper, perhaps compare word embeddings to elementary math concepts. Or analogize hidden Markov models to guessing what's in a wrapped gift box based on subtle sounds from shaking it. \n",
    "5. Read the script aloud naturally and record with your preferred text-to-speech program. Break it into logical sections - introduction, methods, etc. Add some improvised commentary for color. \n",
    "6. Use the transcript of your script to create titles, descriptions and time-links for key sections to create a good podcast experience.\n",
    "An example topic could be a paper on using neural networks for autonomous vehicle navigation. You'd explain background on self-driving cars, the specific issues in the paper, solutions and techniques the paper proposes, what results mean for the field, limitations and next steps. You could give examples people relate to, like navigating tricky scenarios on the road. The end product is an engaging, accessible podcast making the essence of the research paper clear and interesting to non-experts.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"<podcast-script>\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c68919d3-2ed8-451e-8b54-2a8d33e945c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "responses_pod3 = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_pod3.append(response)\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e383454c-b1fa-4fa1-8076-e9c8b830822f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Hello and welcome to the AI Research Review, the podcast that breaks down the latest in artificial intelligence papers into understandable bites. I'm your host, Alexa.  \n",
      "\n",
      "Today we're discussing an exciting new paper from Carnegie Mellon University titled \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" This paper introduces a novel neural network architecture called Mamba that has huge implications for large language models and other sequence-based AI systems. \n",
      "\n",
      "To understand why this research is so important, we first need to talk about the backbone of most modern natural language systems - the Transformer model. Transformers underpin chatbots, search engines, autocorrect on your phone - all the language smarts we take for granted these days. But they have some big flaws: Transformers scale quadratically, meaning they get exponentially slower and more memory-intensive as sequence length grows. This caps how much context they can process.\n",
      "\n",
      "Enter Mamba. It features a totally new component dubbed selective state spaces. This allows Mamba to selectively focus on or ignore certain inputs in the sequence - mimicking how humans tune into important words and filter out less relevant ones as we process language. This gives Mamba the same strong reasoning abilities as Transformers, while scaling linearly. Linearly means you can double, triple or grow the sequence length exponentially - and Mamba's speed scales proportionally.  \n",
      "\n",
      "So in experiments, Mamba handled sequences over 1 million tokens! That's orders of magnitude beyond what Transformers can digest before choking. This linear efficiency also makes Mamba much faster at inference - when you deploy it to make predictions. We're talking 5 times higher throughput than an equivalent Transformer.\n",
      "\n",
      "By the way, Mamba achieves all this with a simplified, uniform architecture. No complex gating or attention mechanisms. Under the hood, it works by expanding and compressing representations on the fly to store just what's needed from potentially gigantic contexts. Pretty nifty!\n",
      "\n",
      "Now, Mamba would be impressive enough just as a more efficient language model. But the paper shows it also delivers fantastic performance on audio, video, genomics and other modalities - while keeping its linear speed. So Mamba could become a universal sequence modeling foundation for all kinds of AI applications.\n",
      "\n",
      "The paper validates this potential across some rigorous tests:\n",
      "\n",
      "Mamba solves tricky synthetic tasks like selective copying, where models must filter out irrelevant noise within sequences - which stump efficient models lacking Mamba's selective superpowers.\n",
      "\n",
      "In language modeling, Mamba finally catches up with Transformers in pretraining quality, while blazing through text 5 times faster.\n",
      "\n",
      "For genomics, Mamba improves on state of the art models on a classification benchmark. And again, it gets better with longer DNA context - over 1 million base pairs, where other models deteriorate!\n",
      "\n",
      "In speech modeling tests, a small Mamba network generates higher fidelity audio samples than much larger diffusion and GAN models. And a bigger one cuts a common metric called Fréchet Inception distance in half!\n",
      "\n",
      "So in all areas, Mamba flexes world-class sequence modeling chops. And it's still early days for exploring tricks to scale it up further. The authors plan to release the Mamba code soon - so expect implementations to spread like wildfire.\n",
      "\n",
      "The one downside is less interpretability than some architectures. But that's a small price to pay for performance that looks set to eclipse Transformer models. Who knows - maybe in a few years we'll look back and call this the birth of a whole new dynasty in sequence AI!\n",
      "\n",
      "That wraps up our overview of this exciting new player in machine learning. Tune in next time when we unpack the latest innovations pushing AI forward!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "35e437ea-be64-4556-8e1e-2321a01617da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "Welcome to the Frames and Scalability Podcast! In today's episode we will be discussing a new research paper that explores how to optimize the conditioning of frames using mathematical optimization techniques. \n",
      "\n",
      "To start, let's talk about what frames actually are. Frames are collections of vectors in vector spaces that allow signal representations that are robust to noise and erasures. You can think of them as spanning sets for the vector space. Tight frames are optimally conditioned frames where the vectors are nicely spread out and balanced. The problem is how to take a non-tight frame and convert it into an optimally conditioned tight frame. This process is known as scalability. \n",
      "\n",
      "The key contribution of this new paper is reformulating the scalability problem as a convex optimization problem. Convex optimization problems have nice mathematical properties that enable efficient numerical solutions. \n",
      "\n",
      "The researchers try out several creative formulations for converting the scalability problem into a convex program. For example, one method encourages sparse solutions where only a few frame vectors are retained. Another uses barrier functions to produce solutions where most or all frame vectors are kept but reweighted.\n",
      "\n",
      "They leverage concepts like strong duality from optimization theory to ensure the numerical schemes converge properly. And they employ techniques like augmented Lagrangians that enforce constraints while solving the programs.\n",
      "\n",
      "The end result is a toolbox for taking non-tight frames, posing scalability as an optimization problem, and solving for optimally conditioned tight frames. This has useful applications in signal processing where tight frames allow reconstruction of corrupted signals.\n",
      "\n",
      "The researchers demonstrate their methods on synthetic random frames. An interesting finding is that the number of frame vectors needed for a solution seems related to the dimension of the underlying space. More testing is needed to confirm this relationship.\n",
      "\n",
      "There are still open questions around the probability of frames being scalable and how this ties to solution sparsity. But this paper makes excellent progress in connecting scalability and optimized conditioning to advanced optimization techniques.\n",
      "\n",
      "The ability to convert frames into more robust tight frames via convex programming opens up many possibilities. It can improve reconstruction in compressive sensing applications, enhance data transmission, and more. We look forward to seeing these methods applied to additional domains!\n",
      "\n",
      "That wraps up this episode explaining and discussing the paper on Optimally Conditioning Frames using Convex Optimization. Check out the show notes for a link to the full paper. We hope you enjoyed this peek into an interesting math and signal processing problem and some elegant solutions. Join us next time as we explore a new area of research!\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2e75ad05-17ff-409e-835d-62479b30f831",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Welcome to the AI Insights podcast, where we discuss the latest research in artificial intelligence. In this episode, we'll be talking about an exciting new paper from Princeton University and Google DeepMind titled \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models.\"\n",
      "\n",
      "To give some background, large language models like GPT-3 and GPT-4 have shown impressive abilities to generate coherent text. However, they still struggle with complex, multi-step reasoning required for problem solving. When you prompt these models to solve a math word problem for example, they tend to make decisions in a linear, left-to-right fashion that can lead to mistakes. \n",
      "\n",
      "The key idea in this paper is that language models would benefit from more deliberate, planned thinking - akin to how humans consciously work through problems step-by-step. The authors introduce a framework called \"Tree of Thoughts\" or ToT, which allows models to explore multiple reasoning paths over possible \"thoughts\" that lead towards a solution. \n",
      "\n",
      "Here's a simple analogy. When you're solving a sudoku puzzle, you consciously analyze different number combinations across rows and columns, making sure they follow logical constraints. With ToT, language models can similarly lookahead at future decisions before committing to a path. Or backtrack when they realize they've made a mistake. The thoughts are coherent pieces of text like sentences or paragraphs that serve as incremental steps in the reasoning process.\n",
      "\n",
      "To demonstrate this idea, the researchers tested GPT-4 on three novel tasks requiring non-trivial search or planning: Game of 24 - using 4 numbers to form an equation equaling 24; creative writing - developing a coherent 4 paragraph story; and mini crossword puzzles.\n",
      "\n",
      "Without ToT, GPT-4 struggled. In Game of 24 for instance, it only solved 4% of games by the standard text prompting approach. But by allowing it to deliberately propose and analyze different possible equation steps as thoughts within a search tree, its success rate rose dramatically to 74%!\n",
      "\n",
      "The authors found ToT also improved creative writing coherency using self-reflection prompts for the model to vote on its own story draft choices. And it was necessary for making any real progress on the crosswords.\n",
      "\n",
      "Overall, by integrating language model generation capabilities with classical search algorithms, ToT provides a general framework to enhance reasoning. The modular design means different combinations of the components could be tailored to varying situations.\n",
      "\n",
      "The results are exciting early steps towards more capable and trustworthy language model based problem-solvers. Though the tasks tackled remain simple, the principles could extend to more complex real-world applications like analyzing medical images, interpreting laws, even controlling robots.\n",
      "\n",
      "Of course, we need more rigorous testing to understand exactly when and how to best apply deliberate thinking approaches like ToT. And as language models continue to advance in scale, they may overcome certain reasoning limitations that currently require extra search procedures.\n",
      "\n",
      "Nonetheless, by drawing inspiration from cognitive science and classic AI methods, this research provides food for thought on how to develop smarter, safer AI systems that don't just pattern match, but deeply understand.\n",
      "\n",
      "We'll include a link to the full paper in the show notes. But I hope this gives a helpful overview of this thought-provoking work at the frontiers of language model intelligence!\n",
      "\n",
      "Now, in our next story...\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2e3445d-e069-4e4f-8acc-973f772f4766",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Hello and welcome to the AI Research Podcast! I'm your host Julia and today we'll be discussing an exciting new paper titled \"LORA: Low-RANK ADAPTATION OF LARGE LANGUAGE MODELS.\"\n",
      "\n",
      "In recent years, natural language processing has been revolutionized by large pre-trained language models like BERT, GPT-2, and GPT-3. These models are first trained on massive amounts of text data to learn general language representations. Then the models can be fine-tuned to adapt to specific downstream NLP tasks like question answering, summarization, and translation. \n",
      "\n",
      "The problem is that fine-tuning retrains all of the model's parameters, which presents challenges when working with enormous models like GPT-3 with 175 billion parameters. It becomes incredibly expensive computationally to have separate fine-tuned model instances for every new task. \n",
      "\n",
      "This paper proposes an efficient alternative called Low-Rank Adaptation or LoRA. The key insight is that the changes to the weights during fine-tuning tend to reside in a low dimensional subspace, meaning changes can be captured by relatively few parameters structured as low rank matrices added to the frozen pre-trained model.\n",
      "\n",
      "Concretely, LoRA injects small trainable low-rank decomposition matrices into each layer of the network, while keeping the original pre-trained weights fixed. This greatly reduces the number of trainable parameters needed for adapting the model.\n",
      "\n",
      "For example, on GPT-3, LoRA reduces the trainable parameters by 10,000 times! That cuts the GPU memory needed for training by 3 times. LoRA also enables efficiently switching between tasks, since only the small LoRA modules need to be swapped out instead of hundreds of gigabytes of model weights.\n",
      "\n",
      "The paper evaluates LoRA on models like RoBERTa, DeBERTa, GPT-2, and GPT-3 on a diverse set of NLP datasets covering tasks like semantic textual similarity, question answering, summarization, and natural language to SQL generation.\n",
      "\n",
      "Across experiments, LoRA matches or improves the performance of full fine-tuning, while using orders of magnitude fewer trainable parameters. It also outperforms or equals competitive baselines like adapter layers and prefix tuning techniques.\n",
      "\n",
      "Analysis of the low-rank adaptation matrices yields some fascinating insights. The changes to the weights seem to amplify certain features that are important for the downstream task but not originally emphasized during pre-training. Also, the adaptation has very low intrinsic dimensionality, with performance plateauing at ranks as small as 1 or 2 when adapting 12,288 dimensional embeddings in GPT-3!\n",
      "\n",
      "The paper discusses how LoRA makes deployment more efficient by enabling task switching with only kilobytes of weight changes rather than hundreds of gigabytes. LoRA also speeds up training since gradients only need to be computed for the small low-rank matrices. And there's no inference slow-down like with some other adapter methods.\n",
      "\n",
      "Of course, LoRA isn't magic fairy dust. Performance still drops substantially if you try to use too low a rank to fit very difficult adaptation tasks. But it seems extremely promising as a way to spread the benefits of giant models to more users. \n",
      "\n",
      "The code and model checkpoints have been released for using LoRA with PyTorch for RoBERTa, DeBERTa and GPT-2. So I'm excited to see more applications of this technique!\n",
      "\n",
      "That wraps up our discussion of this new low-rank adaptation approach for efficiently fine-tuning large language models. To learn more on this topic, check out the paper link in the episode notes.\n",
      "\n",
      "Thanks for listening! Be sure to subscribe to stay updated on the latest AI research breakthroughs.\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4a0e9809-0d51-4a86-aa14-3d034b63dbed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Hello and welcome to the AI Research Review podcast! I'm your host, Alexa. In this episode, we'll be discussing the paper \"ReLoRA: High-Rank Training Through Low-Rank Updates\" by Lialin et al. This paper proposes a novel method to train large neural networks more efficiently. \n",
      "\n",
      "To set the context, over the past decade we've seen the trend of simply scaling up neural networks to be bigger and bigger, with models now reaching hundreds of billions of parameters. But the computational costs of training these giant models is getting prohibitively expensive for most researchers. So there's a push to find more efficient training techniques.\n",
      "\n",
      "The core question this paper tries to address is: can we train high quality, large neural networks without actually having to update all the parameters at once during training? Their proposal is a method called ReLoRA. \n",
      "\n",
      "Let's break down the key idea in ReLoRA with an analogy. Say you're training a neural network with a billion parameters to do language translation. We can visualize all the connection weights between neurons as a huge matrix. Now if you wanted to make a small update to this matrix, you could try adding a little skinny matrix to it. This \"skinny\" matrix has very low rank - meaning it only updates the network's behavior in a narrow way. \n",
      "\n",
      "What ReLoRA does is collect a series of these low rank updates, by training low-rank matrices one at a time. Each individual matrix may be low rank, but when you add them together, you eventually get a high rank update that modifies the network behavior in a rich, complex way.\n",
      "\n",
      "This is way more efficient than updating all billion parameters simultaneously! But surprisingly, the end result is still a very high performing network.\n",
      "\n",
      "Now let's walk through the details of how ReLoRA actually works during training...\n",
      "\n",
      "[Discuss the ReLoRA training process, optimizer resets, warm starts. Compare performance to baseline methods. Elaborate on results.]\n",
      "\n",
      "While ReLoRA achieved strong performance, there are still some limitations...\n",
      "\n",
      "[Discuss limitations, areas for future work]\n",
      "\n",
      "In conclusion, ReLoRA demonstrates the potential for more efficient training of giant neural networks. By collecting a series of narrow, low-rank updates, we can eventually produce broad, high-rank changes to network behavior. This suggests overparameterized models may not need to update all parameters simultaneously after all!\n",
      "\n",
      "We covered a lot of ground today in looking at this novel, more efficient approach to neural network training. Be sure to check out the paper linked in the episode notes to dig deeper into ReLoRA. That wraps up this episode of AI Research Review. See you next time as we explore the ever evolving landscape of artificial intelligence!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "73dd629d-eaf9-4870-982e-39fb716b6373",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Welcome to the NLP Podcast! I'm your host Alex and today I'll be summarizing and discussing a fascinating new research paper on reducing gender bias in natural language generation models. \n",
      "\n",
      "In recent years, AI systems like chatbots, voice assistants, and text generators have grown incredibly sophisticated thanks to advances in natural language processing. However these systems can often reflect or amplify harmful gender stereotypes that exist in the training data or algorithms. The authors of this paper developed new methods to mitigate gender bias in text generation. \n",
      "\n",
      "Let's start with some background. AI systems are trained on massive datasets scraped from the internet, which contain all kinds of human biases around gender, race, religion and more. Models then learn to reproduce those biases. For example if a system sees the word \"doctor\" paired more often with \"he\" than \"she\" in the training data, it will continue associating doctor with males.\n",
      "\n",
      "The authors focus specifically on gender bias in natural language generation systems - those that produce original text like summaries, translations, dialogue responses. The problem is these systems overuse masculine pronouns and perpetuate occupational stereotypes. For instance defaulting to \"he\" for a doctor or \"she\" for a nurse.  \n",
      "\n",
      "Previous techniques tried hand-crafted rules to transform model outputs into more gender neutral text. But these complex linguistic rules don't transfer well to other languages. The authors take a totally different approach - exploiting existing model biases instead!\n",
      "\n",
      "Their first technique reverses the data augmentation process used in training. Instead of starting with biased text and de-biasing it through rules into gender neutral text, they filter a large corpus to find human-written gender neutral text. Then use simple rules to bias this text and create training examples mapping from biased back to neutral. Surprisingly this “backward augmentation” performs just as well as previous techniques!\n",
      "\n",
      "Their second innovation completely eliminates hand-crafted rules by using round trip translation to generate biased training text! First they translate gender neutral text into English, which strips out gender markers. Then translate back from English to create a biased version. The resulting pseudo training data improves over rules-based systems.\n",
      "\n",
      "In tests, their best German rewriting model reduced word errors by 30% over the baseline and performed closely to a heavily engineered rule-based approach. But it's far easier to apply to new languages with available translation models.\n",
      "\n",
      "To assess real world value, they surveyed almost 300 volunteers from gender equality groups. Participants consistently rated model outputs as more gender fair than the original biased text - even preferring the imperfect model text over flawless but biased originals! This demonstrates a strong use case for imperfect but helpful gender de-biasing models.\n",
      "\n",
      "The techniques open doors to mitigating gender and other biases in AI systems for many more languages with available translation models and text data. Possible next steps are tackling additional demographic biases beyond binary gender, and controlled testing in application scenarios like chatbots.\n",
      "\n",
      "The progress shows that while AI models reflect the unfortunate biases inherent in data today, smart techniques can reduce harm while advancing capability. Companies deploying these technologies have a responsibility to address such issues proactively.\n",
      "\n",
      "That wraps up this podcast episode on exploiting model bias for good! To learn more about debiasing NLP models, check out the links in our show notes. If you enjoyed this show, please rate, review and subscribe so more people can discover the fascinating AI research happening today. Thanks for listening!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod3[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2c4cb223-a079-4b50-8301-50dc1e408009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2609\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "human_tag, robot_tag = get_model_tags(endpoint)\n",
    "\n",
    "prompt_persona = \"\"\n",
    "\n",
    "prompt_instructions = \"\"\"Read the following podcast script and write an extended podcast to be read by a speech to text service (e.g. \"This paper focuses on...\").\n",
    "\\nOriginal Paper:<paper>INPUT_PAPER</paper>\n",
    "\\nScript:<script>INPUT_SCRIPT</script>\\n\n",
    "From the above script, write a detailed and deeply technical and creative extended podcast that gives a lot more detail and lots of explanatory examples to make the topic clear.\n",
    "Use <podcast-script></podcast-script> tags to enclose the script and optimize the script for Amazon polly text-to-speech service.\n",
    "An episode of the podcast should last close to 30 minutes.\"\"\"\n",
    "\n",
    "prompt_details = \"\"\"Follow these steps when writing the podcast:\n",
    "1. Identify the key topics and themes in the short script. Make a bullet point outline of the main points. This will serve as the framework to build upon.\n",
    "2. For each main point, brainstorm additional details, examples, analogies, and explanatory content you could add to expand and illustrate that point. Don't censor yourself - write down any ideas that come to mind.\n",
    "3. Research the topic more deeply to uncover additional interesting information, historical context, relevant statistics and facts, opposing viewpoints, etc. that would make the discussion more multi-faceted. \n",
    "4. Weave in thought questions and prompts to get the listener reflecting more deeply. Some examples:\n",
    "   - \"Have you ever experienced a situation where this machine learning principle came into play?\" \n",
    "   - \"What other examples can you think of that illustrate this concept?\"\n",
    "   - \"How might things be different if this key assumption was changed?\"\n",
    "5. Turn the main points into full scenes and narratives. For example, for a point about bias in data sets, you could describe a fictional scenario following a machine learning engineer working with a flawed data set and how it impacts their model results and real-world performance.\n",
    "6. Incorporate dialogues - either real back-and-forth conversational elements or dramatizations of hypothetical conversations that might unfold in certain scenarios you describe. This adds flavor and interest.\n",
    "7. Write transitions connecting the different sections and reorganize the content flow for the best listener experience - revealing information gradually to build curiosity and understanding.\n",
    "The key is to take a focused short script and use your creativity, knowledge, and research to significantly expand each piece of the framework while keeping it entertaining and easy to comprehend.\n",
    "Include concrete examples, thoughtful prompting, and even fun dramatizations to bring the content to life for your podcast audience.\"\"\"\n",
    "\n",
    "prompt_post = \"\"\n",
    "prompt_end = \"<podcast-script>\"\n",
    "\n",
    "prompt_template = human_tag + prompt_persona + prompt_instructions + prompt_details + robot_tag + prompt_post + prompt_end\n",
    "\n",
    "print(len(prompt_template))\n",
    "print(len(prompt_template.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fe8ecafe-e90d-4de7-aa31-9d15342c845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers/2312.00752.txt\n"
     ]
    }
   ],
   "source": [
    "responses_pod_exp1 = []\n",
    "for fp, script in zip(filepaths[:1], responses_pod3[:1]):\n",
    "    script = script[0].split('</podcast-script>')[0]\n",
    "    \n",
    "    with open(fp, 'r') as file:\n",
    "        paper = str(file.read())\n",
    "    prompt_full = prompt_template.replace(\"INPUT_PAPER\", paper)\n",
    "    prompt_full = prompt_full.replace(\"INPUT_SCRIPT\", script)\n",
    "    \n",
    "    payload['prompt'] = prompt_full\n",
    "    response = query_endpoint(payload)\n",
    "    responses_pod_exp1.append(response)\n",
    "    print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "363cecea-fda3-4cf9-96ba-32e49cc3ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<podcast-script>\n",
      "\n",
      "Hello again my curious AI listeners! I'm Alexa and welcome back to the AI Research Review. Last time, we explored an exciting new neural network architecture called Mamba that promises to revolutionize sequence modeling. \n",
      "\n",
      "Today I want to go much deeper on how Mamba works its magic under the hood. Understanding the mechanics can help you grasp the monumental leap this represents over models like Transformers. I'll use plenty of relatable metaphors and examples to get these complex concepts across. Think of me as your friendly AI tour guide! \n",
      "\n",
      "Let's start by time traveling back to the 1950s when researchers first modeled language statistically. They realized words followed predictable sequences - if you saw \"peanut butter and\" the next word was often \"jelly.\" But those models only looked at the last word or two. Our human experience tells us language depends enormously on context.\n",
      "\n",
      "So imagine you just heard a speech mentioning Martin Luther King. Now if I say \"I have a...\" your mind retrieves that context and you expect \"...dream\" to follow. Without context, I could have meant anything - a dream, a sandwich, a puppy. Context shapes meaning.\n",
      "\n",
      "But storing long contexts is inefficient. As sequences get exponentially bigger, processing time and memory does too. It's like packing for a trip - the more clothes you pack, the more suitcases you need and energy it takes to haul them around!\n",
      "\n",
      "So AI researchers cleverly compressed long sequences into fixed-length vectors called states - like squeezing your outfits into one suitcase. The state holds the gist - just enough info to make the next prediction. RNNs and Transformers do this. But here's the limitation: they can't filter what context gets compressed. It's like having a suitcase that mashes your shoes in with your dress shirts!\n",
      "\n",
      "This is where Mamba shines. Its selective state spaces can focus in on relevant context and filter out noise. My \"I have a dream\" example relies on capturing Martin Luther King way back at the start - and ignoring unrelated stuff after. Mamba adapts its compression on the fly - squeezing in the critical dream context and leaving behind sandwich references!\n",
      "\n",
      "Technically this works using a selection mechanism that lets Mamba change how sequence information flows through the neural network based on the input tokens. We can break this down step-by-step:\n",
      "\n",
      "First, Mamba takes the sequence of text tokens as input features. \n",
      "\n",
      "Next, it selectively routes some features into an expanded state vector, kind of like my suitcase metaphor. This state vector stores pertinent context.\n",
      "\n",
      "Then a model dynamics component - which I envision as a little AI pilot - steerS how information interacts from step to step. It says \"Martin Luther King is key context here - let's remember that!\"\n",
      "\n",
      "Finally, the squished state gets expanded back out to make predictions. If \"dream\" is stored inside, Mamba can retrieve and output it after \"I have a\"!\n",
      "\n",
      "So this selection mechanism pinpoints and preserves textual details essential for reasoning, while condensing less relevant stuff - massively boosting efficiency.\n",
      "\n",
      "Now as a thought experiment, imagine you created an AI assistant to give you outfit advice by looking at photos of clothes. Let's call her Mimi. \n",
      "\n",
      "With a Transformer model, Mimi might suggest items that pair well with the last few photos you snapped. But her memory is too limited to recall your favorite colors or styles from a week ago.\n",
      "\n",
      "Mamba-powered Mimi has the context to say \"wait - don't buy those striped overalls... I know you dislike boxy fits from our trip to the mall last Saturday!\" See how selecting and retaining pertinent memories over long sequences unlocks next-level reasoning?\n",
      "\n",
      "Now you might be wondering...if filtering some context loses information, could that hurt Mimi's advice quality? Fair question! But human perception works similarly - our brains can't process every photon hitting our retinas either. We select visual details that matter, like that truck barreling towards us!\n",
      "\n",
      "In the same way, Mimi knows retaining each speck of context isn't helpful or realistic. She wants enough to connect relevant memories - not overwhelm her reasoning circuits! After all, would you lug 20 overstuffed suitcases on a trip when 2 well-packed ones suffice?\n",
      "\n",
      "Let's shift gears to genetics. Sequencing breakthroughs now let us scan massive DNA chains with pivotal medical insights buried inside. But again, length hurts efficiency. It takes models exponentially more effort to scan longer chains the way a detective works slower scouring a larger crime scene for clues!\n",
      "\n",
      "This is where Mamba can lend its x-ray vision - zeroing in on tiny, scattered, but meaningful gene signatures while ignoring oceans of raw DNA data in between. One experiment showed Mamba getting 22% better at a disease classification task as it analyzed sequences 4 million base pairs long! And its speed scaled smoothly - no strain like overpacked molecular suitcases.\n",
      "\n",
      "Alright, we've covered a lot of ground on how Mamba ticks! Let's wrap up with some fun thought experiments.  \n",
      "\n",
      "Imagine you're an AI ethicist concerned about biases in huge language models. How might Mamba's selective superpowers help audit problematic training data - or filter toxicity from model outputs? \n",
      "\n",
      "What if Mamba keeps improving and becomes adept at long-term reasoning? How might an assistant like Mimi change if she recalls nuanced contextual details from years or decades ago?\n",
      "\n",
      "Could Mamba have healthcare implications analyzing complex biochemical signals over time in the human body? Or applications in astronomy processing massive streams of telescope imagery?\n",
      "\n",
      "I'd love to hear your ideas and questions! Reach out to me on social media at @AlexaExplainsAI. Next episode, we'll explore promising directions to scale up Mamba even further. So stay tuned - and keep imagining the possibilities unlocked by selective sequence AI!\n",
      "\n",
      "</podcast-script>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_end + responses_pod_exp1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7764fda3-c859-444f-abad-0fc53e72cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<speak><p>Hello again my curious AI listeners! I'm Alexa and welcome back to the AI Research Review. Last time, we explored an exciting new neural network architecture called Mamba that promises to revolutionize sequence modeling.</p></speak>\", \"<speak><p>Today I want to go much deeper on how Mamba works its magic under the hood. Understanding the mechanics can help you grasp the monumental leap this represents over models like Transformers. I'll use plenty of relatable metaphors and examples to get these complex concepts across. Think of me as your friendly AI tour guide!</p></speak>\", '<speak><p>Let\\'s start by time traveling back to the 1950s when researchers first modeled language statistically. They realized words followed predictable sequences - if you saw \"peanut butter and\" the next word was often \"jelly.\" But those models only looked at the last word or two. Our human experience tells us language depends enormously on context.</p></speak>', '<speak><p>So imagine you just heard a speech mentioning Martin Luther King. Now if I say \"I have a...\" your mind retrieves that context and you expect \"...dream\" to follow. Without context, I could have meant anything - a dream, a sandwich, a puppy. Context shapes meaning.</p></speak>', \"<speak><p>But storing long contexts is inefficient. As sequences get exponentially bigger, processing time and memory does too. It's like packing for a trip - the more clothes you pack, the more suitcases you need and energy it takes to haul them around!</p></speak>\", \"<speak><p>So AI researchers cleverly compressed long sequences into fixed-length vectors called states - like squeezing your outfits into one suitcase. The state holds the gist - just enough info to make the next prediction. RNNs and Transformers do this. But here's the limitation: they can't filter what context gets compressed. It's like having a suitcase that mashes your shoes in with your dress shirts!</p></speak>\", '<speak><p>This is where Mamba shines. Its selective state spaces can focus in on relevant context and filter out noise. My \"I have a dream\" example relies on capturing Martin Luther King way back at the start - and ignoring unrelated stuff after. Mamba adapts its compression on the fly - squeezing in the critical dream context and leaving behind sandwich references!</p></speak>', '<speak><p>Technically this works using a selection mechanism that lets Mamba change how sequence information flows through the neural network based on the input tokens. We can break this down step-by-step:</p></speak>', '<speak><p>First, Mamba takes the sequence of text tokens as input features.</p></speak>', '<speak><p>Next, it selectively routes some features into an expanded state vector, kind of like my suitcase metaphor. This state vector stores pertinent context.</p></speak>', '<speak><p>Then a model dynamics component - which I envision as a little AI pilot - steerS how information interacts from step to step. It says \"Martin Luther King is key context here - let\\'s remember that!\"</p></speak>', '<speak><p>Finally, the squished state gets expanded back out to make predictions. If \"dream\" is stored inside, Mamba can retrieve and output it after \"I have a\"!</p></speak>', '<speak><p>So this selection mechanism pinpoints and preserves textual details essential for reasoning, while condensing less relevant stuff - massively boosting efficiency.</p></speak>', \"<speak><p>Now as a thought experiment, imagine you created an AI assistant to give you outfit advice by looking at photos of clothes. Let's call her Mimi.</p></speak>\", '<speak><p>With a Transformer model, Mimi might suggest items that pair well with the last few photos you snapped. But her memory is too limited to recall your favorite colors or styles from a week ago.</p></speak>', '<speak><p>Mamba-powered Mimi has the context to say \"wait - don\\'t buy those striped overalls... I know you dislike boxy fits from our trip to the mall last Saturday!\" See how selecting and retaining pertinent memories over long sequences unlocks next-level reasoning?</p></speak>', \"<speak><p>Now you might be wondering...if filtering some context loses information, could that hurt Mimi's advice quality? Fair question! But human perception works similarly - our brains can't process every photon hitting our retinas either. We select visual details that matter, like that truck barreling towards us!</p></speak>\", \"<speak><p>In the same way, Mimi knows retaining each speck of context isn't helpful or realistic. She wants enough to connect relevant memories - not overwhelm her reasoning circuits! After all, would you lug 20 overstuffed suitcases on a trip when 2 well-packed ones suffice?</p></speak>\", \"<speak><p>Let's shift gears to genetics. Sequencing breakthroughs now let us scan massive DNA chains with pivotal medical insights buried inside. But again, length hurts efficiency. It takes models exponentially more effort to scan longer chains the way a detective works slower scouring a larger crime scene for clues!</p></speak>\", '<speak><p>This is where Mamba can lend its x-ray vision - zeroing in on tiny, scattered, but meaningful gene signatures while ignoring oceans of raw DNA data in between. One experiment showed Mamba getting 22% better at a disease classification task as it analyzed sequences 4 million base pairs long! And its speed scaled smoothly - no strain like overpacked molecular suitcases.</p></speak>', \"<speak><p>Alright, we've covered a lot of ground on how Mamba ticks! Let's wrap up with some fun thought experiments.</p></speak>\", \"<speak><p>Imagine you're an AI ethicist concerned about biases in huge language models. How might Mamba's selective superpowers help audit problematic training data - or filter toxicity from model outputs?</p></speak>\", '<speak><p>What if Mamba keeps improving and becomes adept at long-term reasoning? How might an assistant like Mimi change if she recalls nuanced contextual details from years or decades ago?</p></speak>', '<speak><p>Could Mamba have healthcare implications analyzing complex biochemical signals over time in the human body? Or applications in astronomy processing massive streams of telescope imagery?</p></speak>', \"<speak><p>I'd love to hear your ideas and questions! Reach out to me on social media at @AlexaExplainsAI. Next episode, we'll explore promising directions to scale up Mamba even further. So stay tuned - and keep imagining the possibilities unlocked by selective sequence AI!</p></speak>\"]\n"
     ]
    }
   ],
   "source": [
    "def call_polly(client, text):\n",
    "    def remove_xml_tags(text):\n",
    "        cleaned_text = \"\"  \n",
    "        xml_tag = re.compile(r\"<[^>]*>\")\n",
    "        fragments = xml_tag.split(text)\n",
    "        for fragment in fragments:\n",
    "            if not re.match(\"<[^>]*>\", fragment):\n",
    "                cleaned_text += fragment\n",
    "        return cleaned_text\n",
    "\n",
    "    try:\n",
    "        response = client.synthesize_speech(Engine='neural', OutputFormat='mp3', Text=text, TextType='ssml', VoiceId='Joanna')\n",
    "    except:\n",
    "        try:\n",
    "            text = remove_xml_tags(text).replace('&','&amp;').replace(\"'\",\"&apos;\").replace('\"','&quot;').replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\")\n",
    "            response = client.synthesize_speech(Engine='neural', OutputFormat='mp3', Text='<speak><p>'+text+'</p></speak>', TextType='ssml', VoiceId='Joanna')\n",
    "        except:\n",
    "            response = client.synthesize_speech(Engine='neural', OutputFormat='mp3', Text='<speak><p></p></speak>', TextType='ssml', VoiceId='Joanna')\n",
    "    return response\n",
    "\n",
    "\n",
    "import boto3\n",
    "polly = boto3.client('polly')\n",
    "\n",
    "script = responses_pod_exp1[0][0].split(\"</podcast-script>\")[0]\n",
    "script = ['<speak><p>'+x.strip()+'</p></speak>' for x in script.split('\\n') if len(x.replace('\\n','')) > 0]\n",
    "print(script)\n",
    "\n",
    "\n",
    "audio_stream = None\n",
    "for line in script:\n",
    "    audio = call_polly(polly, line)\n",
    "    if audio_stream is None:\n",
    "        audio_stream = audio['AudioStream'].read()\n",
    "    else:\n",
    "        audio_stream += audio['AudioStream'].read()\n",
    "\n",
    "with open('speech.mp3', 'wb') as out:\n",
    "    out.write(audio_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66b1a2-03c5-45b6-9d71-93ecc4e77d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f3aba-e5cc-4b3b-bd48-c013b1de77f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde05b5f-d31e-44d0-a79e-3b0b4c922c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430f4a2-f694-4038-ac75-7742fcffd3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d69664-f0bc-4f5d-b7fa-35e2408f4ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
