{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609fb9c2-545c-4610-8ce2-b8bce325b4dc",
   "metadata": {},
   "source": [
    "# CloudWatch Metrics for Amazon Bedrock\n",
    "**Last Updated:** Oct 14th 2024\n",
    "\n",
    "**Modified by:** Chae Clark\n",
    "\n",
    "This notebook shows example useage of Amazon CloudWatch as a store of metrics for Amazon Bedrock usage. You can use this to log custom metrics related to your Bedrock usage along the lines of latency, token usage, retry requests, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451f9048-5a15-4816-b30b-fcaa2dd622cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from typing import List, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c6271-35cc-424c-8db0-de06cd14dbac",
   "metadata": {},
   "source": [
    "## Methods\n",
    "The three methods:\n",
    "1. query_endpoint - calls the bedrock service. This is just to create real metadata\n",
    "2. set_bedrock_metadata_in_cloudwatch - puts the metadata metrics into CloudWatch\n",
    "3. get_bedrock_metadata_from_cloudwatch - gets the selected metrics and dimensions from CloudWatch\n",
    "\n",
    "**NOTE:** It may take a few minutes for the CloudWatch metrics to populate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32e3ac4-6faf-45a3-9121-e0e4d073c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint(\n",
    "    prompt: List[str],\n",
    "    history: List[Dict[str, str]],\n",
    "    model: Dict[str, str],\n",
    "    system: Optional[str] = None,\n",
    "    client: Optional[boto3.client] = None,\n",
    "    max_tokens: int = 300,\n",
    "    temperature: float = 0.01,\n",
    "    top_p: float = 0.99,\n",
    "    region: str = 'us-east-1'\n",
    ") -> Tuple[str, int, int, float]:\n",
    "    \"\"\"\n",
    "    Query the Bedrock endpoint with the given prompt and history.\n",
    "\n",
    "    Args:\n",
    "        prompt (List[str]): List of alternating user and assistant messages.\n",
    "        history (List[Dict[str, str]]): Previous conversation history.\n",
    "        model (Dict[str, str]): Model configuration including endpoint.\n",
    "        system (Optional[str]): System message for the conversation.\n",
    "        client (Optional[boto3.client]): Boto3 client for Bedrock runtime.\n",
    "        max_tokens (int): Maximum number of tokens in the response.\n",
    "        temperature (float): Sampling temperature for response generation.\n",
    "        top_p (float): Top-p sampling parameter.\n",
    "        region (str): AWS region for the Bedrock client.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, int, int, float]: Generated text, input tokens, output tokens, and latency.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        client = boto3.client('bedrock-runtime', region_name=region)\n",
    "\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        messages.extend([\n",
    "            {'role': 'user', 'content': [{'text': msg['prompt']}]},\n",
    "            {'role': 'assistant', 'content': [{'text': msg['response']}]}\n",
    "        ])\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'user' if i % 2 == 0 else 'assistant', 'content': [{'text': msg}]}\n",
    "        for i, msg in enumerate(prompt)\n",
    "    ]\n",
    "\n",
    "    inference_config = {\n",
    "        'maxTokens': max_tokens,\n",
    "        'temperature': temperature,\n",
    "        'topP': top_p,\n",
    "    }\n",
    "\n",
    "    kwargs = {\n",
    "        'modelId': model['endpoint'],\n",
    "        'messages': messages,\n",
    "        'inferenceConfig': inference_config\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        kwargs['system'] = [{'text': system}]\n",
    "\n",
    "    response = client.converse(**kwargs)\n",
    "\n",
    "    generated_text = response['output']['message']['content'][0]['text']\n",
    "    tokens_in = response['usage']['inputTokens']\n",
    "    tokens_out = response['usage']['outputTokens']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "\n",
    "    return generated_text, tokens_in, tokens_out, latency, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ea8775-ecb7-4c40-b715-dc5b0a25d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bedrock_metadata_in_cloudwatch(\n",
    "    js: Dict,\n",
    "    metric_namespace: str,\n",
    "    username: str = 'NA',\n",
    "    department: str = 'NA',\n",
    "    client: Optional[boto3.client] = None,\n",
    "    region: str = 'us-east-1',\n",
    "    debug: bool = False\n",
    "):\n",
    "    if client is None:\n",
    "        client = boto3.client('cloudwatch', region_name=region)\n",
    "\n",
    "    dimensions = [\n",
    "        {'Name': 'BedrockCall', 'Value': 'ConverseAPI'},\n",
    "        {'Name': 'User', 'Value': username},\n",
    "        {'Name': 'Department', 'Value': department}\n",
    "    ]\n",
    "\n",
    "    # Put the metric data\n",
    "    response = client.put_metric_data(\n",
    "        Namespace=metric_namespace,\n",
    "        MetricData=[\n",
    "            {\n",
    "                'MetricName': 'Latency',\n",
    "                'Value': int(js.get('metrics').get('latencyMs')),\n",
    "                'Unit': 'Milliseconds',\n",
    "                'Timestamp': datetime.utcnow(),\n",
    "                'Dimensions': dimensions\n",
    "            },\n",
    "            {\n",
    "                'MetricName': 'InputTokens',\n",
    "                'Value': int(js.get('usage').get('inputTokens')),\n",
    "                'Unit': 'Count',\n",
    "                'Timestamp': datetime.utcnow(),\n",
    "                'Dimensions': dimensions\n",
    "            },\n",
    "            {\n",
    "                'MetricName': 'OutputTokens',\n",
    "                'Value': int(js.get('usage').get('outputTokens')),\n",
    "                'Unit': 'Count',\n",
    "                'Timestamp': datetime.utcnow(),\n",
    "                'Dimensions': dimensions\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Metric data stored in CloudWatch: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02875c56-c027-46cd-88db-d5f7a8ea0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bedrock_metadata_from_cloudwatch(\n",
    "    metric_namespace: str,\n",
    "    metrics: List[str],\n",
    "    start_time,\n",
    "    end_time,\n",
    "    username: Optional[str] = None,\n",
    "    department: Optional[str] = None,\n",
    "    client: Optional[boto3.client] = None,\n",
    "    region: str = 'us-east-1',\n",
    "    debug: bool = False\n",
    "):\n",
    "\n",
    "    if client is None:\n",
    "        client = boto3.client('cloudwatch', region_name=region)\n",
    "\n",
    "    dimensions = [{'Name': 'BedrockCall', 'Value': 'ConverseAPI'}]\n",
    "    if not (username is None):\n",
    "        dimensions.append({'Name': 'User', 'Value': username})\n",
    "    if not (department is None):\n",
    "        dimensions.append({'Name': 'Department', 'Value': department})\n",
    "\n",
    "    response = []\n",
    "    for metric_name in metrics:\n",
    "        response.append(\n",
    "            client.get_metric_statistics(\n",
    "                Namespace=metric_namespace,\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=dimensions,\n",
    "                StartTime=start_time,\n",
    "                EndTime=end_time,\n",
    "                Period=600,  # 10-minute periods\n",
    "                Statistics=['Average', 'Maximum', 'Minimum']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if debug:\n",
    "        print(response[0]['Datapoints'])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962aa02-764a-466d-af0f-aba51741ed7e",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "This generates sample data calling the Bedrock service and returning the result along withe metadta that we can place into CloudWatch. The model used here doesn't change any of the following code, as we are leveraging the ConverseAPI that standardizes the call and response from the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5899b9e-7fb9-4d47-b4f3-a15eb85222a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1e561d50-88db-4fa0-a74c-dee25c358c6b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 14 Oct 2024 15:51:08 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '281',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '1e561d50-88db-4fa0-a74c-dee25c358c6b'},\n",
       "  'RetryAttempts': 0},\n",
       " 'output': {'message': {'role': 'assistant',\n",
       "   'content': [{'text': \"\\n\\nHello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\"}]}},\n",
       " 'stopReason': 'end_turn',\n",
       " 'usage': {'inputTokens': 15, 'outputTokens': 27, 'totalTokens': 42},\n",
       " 'metrics': {'latencyMs': 426}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = 'us-east-1'\n",
    "\n",
    "models_list = [\n",
    "    {'name':'Claude 3 Haiku', 'endpoint':'anthropic.claude-3-haiku-20240307-v1:0', 'source':'bedrock'},\n",
    "    {'name':'Claude 3 Sonnet', 'endpoint':'anthropic.claude-3-sonnet-20240229-v1:0', 'source':'bedrock'},\n",
    "    {'name':'Claude 3.5 Sonnet', 'endpoint':'anthropic.claude-3-5-sonnet-20240620-v1:0', 'source':'bedrock'},\n",
    "    {'name':'LLAMA 3 70B', 'endpoint':'meta.llama3-70b-instruct-v1:0', 'source':'bedrock'},\n",
    "    {'name':'LLAMA 3 8B', 'endpoint':'meta.llama3-8b-instruct-v1:0', 'source':'bedrock'},\n",
    "    {'name':'Mistral Large', 'endpoint':'mistral.mistral-large-2402-v1:0', 'source':'bedrock'},\n",
    "]\n",
    "\n",
    "generated_text, tokens_in, tokens_out, latency, response = query_endpoint(\n",
    "    prompt=['Hello'],\n",
    "    history=[],\n",
    "    model=models_list[4],\n",
    "    max_tokens=300,\n",
    "    temperature=0.99,\n",
    "    top_p=0.99,\n",
    "    region=region\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f4d6f-9b08-48b4-8943-5d4b5ac32888",
   "metadata": {},
   "source": [
    "## Store Bedrock Metadata in CloudWatch\n",
    "This loops through a number of calls ans stores the data for fictional users and departments into CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbacce88-0f11-4004-9262-a05cf4658cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = ['arnold.kelsey', 'admin.it', 'jessica.wolsley']\n",
    "departments = ['research', 'hr', 'platforms-div-2']\n",
    "\n",
    "for _ in range(500):\n",
    "    set_bedrock_metadata_in_cloudwatch(response, metric_namespace='BedrockUsage', username=np.random.choice(usernames, 1)[0], department=np.random.choice(departments, 1)[0])\n",
    "    time.sleep(.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fb406-6bf6-4c05-ab31-567080cf709d",
   "metadata": {},
   "source": [
    "## Retrieve Metrics from CloudWatch\n",
    "This retrieves statistics for the metrics stored in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff2e26d-9c31-4b3b-8250-a0067707aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "Timestamp: 2024-10-14 15:21:00+00:00, Average: 12.0\n",
      "Timestamp: 2024-10-14 15:31:00+00:00, Average: 12.0\n",
      "Timestamp: 2024-10-14 15:31:00+00:00, Average: 250.0\n",
      "Timestamp: 2024-10-14 15:21:00+00:00, Average: 250.0\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(hours=10)  # Get data for the last 10 hours\n",
    "\n",
    "metrics = get_bedrock_metadata_from_cloudwatch('BedrockUsage', ['OutputTokens', 'Latency'], start_time, end_time, 'arnold.kelsey', 'research')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "for metric in metrics:\n",
    "    datapoints = metric['Datapoints']\n",
    "    for datapoint in datapoints:\n",
    "        print(f\"Timestamp: {datapoint['Timestamp']}, Average: {datapoint['Average']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc4bb7-c249-4d2c-bcab-51ac5457fb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
