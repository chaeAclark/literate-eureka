{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cde8a1a1-2cea-42d1-9804-8f20bd09311b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker as sm\n",
    "\n",
    "sagemaker_session  = sm.session.Session()\n",
    "region             = sagemaker_session._region_name\n",
    "\n",
    "# This is the application metadata that contains any model endpoints we have access to.\n",
    "application_metadata = {\n",
    "    'models':[\n",
    "        {'name':'Anthropic Claude (Bedrock)', 'endpoint':'anthropic.claude-v2'},\n",
    "        {'name':'AI21 Labs J2 Ultra (Bedrock)', 'endpoint':'ai21.j2-jumbo-instruct'},\n",
    "        {'name':'Amazon Titan Large (Bedrock)', 'endpoint':'amazon.titan-tg1-large'},\n",
    "        {'name':'LLAMA-2 7B (SageMaker)', 'endpoint':'llama-2-7b'},\n",
    "        {'name':'Falcon 7B Instruct (SageMaker)', 'endpoint':'falcon-7b-instruct'}\n",
    "    ],\n",
    "    'embedding':[\n",
    "        {'name':'Amazon Titan Embed (Bedrock)', 'endpoint':'amazon.titan-e1t-medium'}\n",
    "    ],\n",
    "    'region':region,\n",
    "}\n",
    "json.dump(application_metadata, open('demo_fewshot_config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c93703d-2074-4284-b71d-d72480049bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aws_demo_fewshot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_fewshot.py\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "\n",
    "def get_description():\n",
    "    \"\"\" Returns a description of the demo. \"\"\"\n",
    "    text = '''This Streamlit app demonstrates how to build effective few-shot prompts for large language models hosted on Amazon SageMaker and Bedrock. It allows the user to interactively construct a prompt, generate an initial response from the model, provide instructions to improve the response, and finalize the few-shot example. The code loads model metadata, provides helper functions to call SageMaker and Bedrock endpoints, builds the Streamlit interface, and logs the user's few-shot examples. Key steps include initializing the app state, getting user input on the prompt and instructions, querying the model endpoint with the prompt to generate responses, and checking consistency by testing similar prompts. Each few-shot example that meets consistency thresholds is added to the history. The user can iterate to build effective prompts that provide informative responses tailored to their use case.'''\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_about():\n",
    "    \"\"\" Returns an about section for the demo. \"\"\"\n",
    "    text = '''This demo shows the power of few-shot prompts when applied to compat models. You can leverage more powerful/flexible models when designing the example, and switch to a light-weight model when moving to production. This can be time and cost effective.'''\n",
    "    return text\n",
    "\n",
    "\n",
    "def download_logs():\n",
    "    \"\"\" Downloads logs of user interactions as JSON. \"\"\"\n",
    "    data = json.dumps(st.session_state['history'], indent=4)\n",
    "    return data\n",
    "\n",
    "\n",
    "def initialize_session():\n",
    "    \"\"\" Initializes session state variables. \"\"\"\n",
    "    st.session_state['prompt'] = ''\n",
    "    st.session_state['similar_prompt'] = ''\n",
    "    st.session_state['initial_response'] = ''\n",
    "    st.session_state['initial_detail'] = ''\n",
    "    st.session_state['full_response'] = ''\n",
    "    st.session_state['full_detail'] = ''\n",
    "    st.session_state['finished_response'] = ''\n",
    "    st.session_state['feasible'] = ''\n",
    "    st.session_state['consistent'] = ''\n",
    "    st.session_state['feasible_model'] = ''\n",
    "    st.session_state['history'] = []\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_metadata(filepath):\n",
    "    \"\"\"Loads JSON configuration for AWS demo connecting SageMaker and Bedrock models. Initializes data structures to track interactions. \"\"\" \n",
    "    md = json.load(open(filepath, 'r'))\n",
    "    models = {d['name']: d['endpoint'] for d in md['models']}\n",
    "    embedding = {d['name']: d['endpoint'] for d in md['embedding']}\n",
    "    region = md['region']\n",
    "    \n",
    "    try:\n",
    "        sagemaker = boto3.client('sagemaker-runtime', region_name=region)\n",
    "    except:\n",
    "        sagemaker = None\n",
    "    \n",
    "    try:\n",
    "        bedrock = boto3.client(service_name='bedrock', region_name=region, endpoint_url='https://bedrock.us-east-1.amazonaws.com')\n",
    "    except:\n",
    "        bedrock = None\n",
    "    md = {\n",
    "        'models': models,\n",
    "        'embedding': embedding,\n",
    "        'region': region,\n",
    "        'sagemaker': sagemaker,\n",
    "        'bedrock': bedrock\n",
    "    }\n",
    "    return md\n",
    "\n",
    "\n",
    "def get_model_tags(endpoint):\n",
    "    \"\"\" Returns chat tokens/tags for models. \"\"\"\n",
    "    if 'claude' in endpoint:\n",
    "        human_tag = '\\n\\nHuman:'\n",
    "        robot_tag = '\\n\\nAssistant:'\n",
    "    elif 'ai21.j2' in endpoint:\n",
    "        human_tag = '\\n##\\n'\n",
    "        robot_tag = ''\n",
    "    elif 'titan' in endpoint:\n",
    "        human_tag = '\\nUser:'\n",
    "        robot_tag = '\\nBot:'\n",
    "    else:\n",
    "        human_tag = '\\n\\n'\n",
    "        robot_tag = ''\n",
    "    return human_tag, robot_tag\n",
    "\n",
    "\n",
    "def call_bedrock(body, endpoint, attempts=5):\n",
    "    \"\"\" Makes request to Bedrock endpoint. \"\"\" \n",
    "    for _ in range(attempts):\n",
    "        try:\n",
    "            response = st.session_state['md']['bedrock'].invoke_model(\n",
    "                body=body,\n",
    "                modelId=endpoint,\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_sagemaker(body, endpoint, eula=False):\n",
    "    \"\"\" Makes request to SageMaker endpoint. \"\"\"\n",
    "    if eula is True:\n",
    "        response = st.session_state['md']['sagemaker'].invoke_endpoint(\n",
    "            EndpointName=endpoint,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=body,\n",
    "            CustomAttributes=\"accept_eula=true\"\n",
    "        )\n",
    "    else:\n",
    "        response = st.session_state['md']['sagemaker'].invoke_endpoint(\n",
    "            EndpointName=endpoint,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=body,\n",
    "        )\n",
    "    return response\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def query_embedding(endpoint, payload):\n",
    "    \"\"\" Gets embedding from model. \"\"\"\n",
    "    if 'titan' in endpoint:\n",
    "        body = json.dumps({\"inputText\": payload['prompt']})\n",
    "        response = call_bedrock(body, endpoint)\n",
    "        response_body = json.loads(response.get('body').read()).get('embedding')\n",
    "    else:\n",
    "        response_body = None\n",
    "    return response_body\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def query_endpoint(endpoint, payload):\n",
    "    \"\"\" Queries language model endpoint. \"\"\"\n",
    "    if 'llama' in endpoint:\n",
    "        body = json.dumps({\n",
    "            \"inputs\": [[{\"role\": \"user\", \"content\": payload['prompt']}]], \n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\":payload['max_len'],\n",
    "                \"top_p\":payload['top_p'],\n",
    "                \"temperature\":payload['temp']\n",
    "            }})\n",
    "        response = call_sagemaker(body, endpoint, eula=True)\n",
    "        response_body = json.loads(response['Body'].read().decode('utf-8'))[0]['generation']['content']\n",
    "    elif 'falcon' in endpoint:\n",
    "        body = json.dumps({\n",
    "            \"inputs\": payload['prompt'], \n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\":payload['max_len'],\n",
    "                \"top_p\":min([0.99, payload['top_p']]),\n",
    "                \"temperature\":payload['temp']\n",
    "            }})\n",
    "        response = call_sagemaker(body, endpoint)\n",
    "        response_body = json.loads(response['Body'].read().decode('utf-8'))[0]['generated_text']\n",
    "    elif 'titan' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'inputText':payload['prompt'],\n",
    "            'textGenerationConfig':{\n",
    "                'maxTokenCount':payload['max_len'],\n",
    "                'temperature':payload['temp'],\n",
    "                'topP':payload['top_p'],\n",
    "        }})\n",
    "        response = call_bedrock(body, endpoint)\n",
    "        response_body = json.loads(response.get('body').read()).get('results')[0].get('outputText')\n",
    "    elif 'claude' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'prompt':payload['prompt'],\n",
    "            'max_tokens_to_sample':payload['max_len'],\n",
    "            'temperature':payload['temp'],\n",
    "            'top_p':payload['top_p'],\n",
    "        })\n",
    "        response = call_bedrock(body, endpoint)\n",
    "        response_body = json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "    elif 'ai21.j2' in endpoint:\n",
    "        body = json.dumps({\n",
    "            'prompt':payload['prompt'],\n",
    "            'maxTokens':payload['max_len'],\n",
    "            'temperature':payload['temp'],\n",
    "            'topP':payload['top_p'],\n",
    "            'stopSequences':['##']\n",
    "        })\n",
    "        response = call_bedrock(body, endpoint)\n",
    "        response_body = json.loads(response.get(\"body\").read()).get(\"completions\")[0].get(\"data\").get(\"text\")\n",
    "    else:\n",
    "        response_body = None\n",
    "    return response_body\n",
    "\n",
    "\n",
    "def query_model(endpoint, payload):\n",
    "    \"\"\" Formats prompt with conversation tags and queries model. \"\"\"\n",
    "    human_tag = payload['human_tag']\n",
    "    robot_tag = payload['robot_tag']\n",
    "    payload['prompt'] = human_tag + payload['prompt'] + robot_tag\n",
    "    response = query_endpoint(endpoint, payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "def query_model_w_instructions(endpoint, payload):\n",
    "    \"\"\" Formats prompt with instructions and queries model. \"\"\"\n",
    "    human_tag = payload['human_tag']\n",
    "    robot_tag = payload['robot_tag']\n",
    "    payload['prompt'] = f'''{human_tag} Read the following DOCUMENT and expand using the instructions provided.\n",
    "\\n\\nDOCUMENT:<document>{payload['prompt']}</document>\\n\\n\n",
    "Read the following INSTRUCTIONS below and re-write the DOCUMENT above.\n",
    "\\n\\nINSTRUCTIONS:<instructions>{payload['instructions']}</instructions>\\n\\n\n",
    "Re-write the DOCUMENT.{robot_tag}'''\n",
    "    response = query_endpoint(endpoint, payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "def check_feasibility(endpoint, payload):\n",
    "    \"\"\" Calculates feasibility metric (BLEU) between responses. \"\"\"\n",
    "    human_tag = payload['human_tag']\n",
    "    robot_tag = payload['robot_tag']\n",
    "    prompt = st.session_state['prompt']\n",
    "    response_finished = st.session_state['finished_response']\n",
    "    payload['prompt'] = f'''{human_tag} {prompt}{robot_tag} {response_finished}\\n\\n{human_tag} {prompt}{robot_tag}'''\n",
    "    response = query_endpoint(endpoint, payload)\n",
    "    references = [st.session_state['finished_response'].replace('\\n',' ').replace('  ', ' ').split()]\n",
    "    candidate_new = response.replace('\\n',' ').replace('  ', ' ').split()\n",
    "    bleu_new = sentence_bleu(references, candidate_new)\n",
    "    bleu_new = np.round(bleu_new*100, 2)\n",
    "    return bleu_new\n",
    "\n",
    "\n",
    "def check_consistency(endpoint, payload):\n",
    "    \"\"\" Checks consistency between similar prompts. \"\"\"\n",
    "    human_tag = payload['human_tag']\n",
    "    robot_tag = payload['robot_tag']\n",
    "    prompt = st.session_state['prompt']\n",
    "    response = st.session_state['finished_response']\n",
    "    similar_prompt = st.session_state['similar_prompt']\n",
    "    similar_prompt = ' '.join([st.session_state['similar_prompt'], st.session_state['initial_detail'], st.session_state['full_detail']])\n",
    "    payload['prompt'] = f'''{human_tag} {prompt}{robot_tag} {response}\\n\\n{human_tag} {similar_prompt}{robot_tag}'''\n",
    "    response = query_endpoint(endpoint, payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "def sidebar():\n",
    "    \"\"\" Initializes sidebar. \"\"\"\n",
    "    st.sidebar.header('About this demo')\n",
    "    st.sidebar.write(get_about())\n",
    "    st.sidebar.header('User Preferences')\n",
    "    \n",
    "    col1, col2 = st.sidebar.columns(2)\n",
    "    with col1:\n",
    "        if st.button('Clear History'):\n",
    "            st.session_state['history'] = []\n",
    "    with col2:\n",
    "        st.download_button('Download Logs', download_logs(), file_name=\"logs_fewshot.json\", mime=\"application/json\")\n",
    "        pass\n",
    "    embed_name = st.sidebar.selectbox('Select Embedding Model', st.session_state['md']['embedding'].keys())\n",
    "    model_name = st.sidebar.selectbox('Select Generation Model', st.session_state['md']['models'].keys())\n",
    "    if 'Claude' in model_name:\n",
    "        temp_start = 1.\n",
    "    elif 'AI21' in model_name:\n",
    "        temp_start = .75\n",
    "    elif 'Titan' in model_name:\n",
    "        temp_start = .05\n",
    "    else:\n",
    "        temp_start = .25\n",
    "    max_len = st.sidebar.slider('Max Generation Length', 500, 9000, 2000, 500) \n",
    "    top_p = st.sidebar.slider('Top p', 0., 1., 1., .01)\n",
    "    temp = st.sidebar.slider('Temperature', 0.01, 1., temp_start, .01)\n",
    "    st.sidebar.write('---')\n",
    "    st.sidebar.subheader('Example:')\n",
    "    st.sidebar.write('**Prompt:** Write a short blog about using Amazon SageMaker to create and train an image classification model.')\n",
    "    st.sidebar.write('**Initial Instructions:** Add a section showing example code for creating and training a SageMaker estimator.')\n",
    "    st.sidebar.write('**Finishing Instructions:** Copy-edit the blog post re-write it in the style of an AWS blog.')\n",
    "    st.sidebar.write('**Similar Prompt:** Write a short blog about using Amazon SageMaker to create and train an XGBoost model.')\n",
    "    \n",
    "    params = {\n",
    "        'endpoint': st.session_state['md']['models'][model_name],\n",
    "        'embedding': st.session_state['md']['embedding'][embed_name],\n",
    "        'max_len': max_len,\n",
    "        'top_p': top_p,\n",
    "        'temp': temp,\n",
    "        'model_name': model_name,\n",
    "        'embedding_name': embed_name\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \"\"\" Main function that executes app logic. \"\"\"\n",
    "    st.title('Create Few-Shot Prompts with Amazon Bedrock and SageMaker')\n",
    "    with st.expander('Tell me more about this works'):\n",
    "        st.write(get_description())\n",
    "    payload = {\n",
    "        'prompt': '',\n",
    "        'instructons': '',\n",
    "        'response': '',\n",
    "        'max_len': params['max_len'],\n",
    "        'temp': params['temp'],\n",
    "        'top_p': params['top_p']\n",
    "    }\n",
    "    endpoint = params['endpoint']\n",
    "    embedding = params['embedding']\n",
    "    model_name = params['model_name']\n",
    "    human_tag, robot_tag = get_model_tags(endpoint=endpoint)\n",
    "    payload['human_tag'] = human_tag\n",
    "    payload['robot_tag'] = robot_tag\n",
    "    \n",
    "    prompt = st.chat_input(\"Let's build a few-shot example! Write a prompt.\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if prompt:\n",
    "            st.session_state['prompt'] = prompt\n",
    "            payload['prompt'] = prompt\n",
    "            payload['instructions'] = ''\n",
    "            st.session_state['initial_response'] = query_model(endpoint, payload)\n",
    "        \n",
    "        if st.session_state['prompt']:\n",
    "            with st.chat_message('user'):\n",
    "                st.write(f\"**Prompt:** {st.session_state['prompt']}\")\n",
    "            \n",
    "        if st.session_state['initial_response']:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.session_state['initial_response'] = st.text_area(\"**Initial Response:**\", st.session_state['initial_response'], height=400)\n",
    "            st.session_state['initial_detail'] = st.text_input(\"**Initial Instructions:**\", \"\")\n",
    "        \n",
    "            if st.button('Update Response'):\n",
    "                payload['prompt'] = st.session_state['initial_response']\n",
    "                payload['instructions'] = st.session_state['initial_detail']\n",
    "                st.session_state['full_response'] = query_model_w_instructions(endpoint, payload)\n",
    "        \n",
    "        if st.session_state['full_response']:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.session_state['full_response'] = st.text_area(\"**Full Response:**\", st.session_state['full_response'], height=400, key='ta_1')\n",
    "            st.session_state['full_detail'] = st.text_input(\"**Finishing Instructions:**\", \"\")\n",
    "        \n",
    "            if st.button('Finalize Response'):\n",
    "                payload['prompt'] = st.session_state['full_response']\n",
    "                payload['instructions'] = st.session_state['full_detail']\n",
    "                st.session_state['finished_response'] = query_model_w_instructions(endpoint, payload)\n",
    "        \n",
    "        if st.session_state['finished_response']:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.session_state['finished_response'] = st.text_area(\"**Full Response:**\", st.session_state['finished_response'], height=400, key='ta_2')\n",
    "            \n",
    "            st.session_state['similar_prompt'] = st.text_input('To check the consistency of this few-shot example, type a new (similar) prompt.')\n",
    "            btn1, btn2, btn3 = st.columns(3)\n",
    "            with btn1:\n",
    "                if st.button('Check Feasibility'):\n",
    "                    st.session_state['feasible'] = check_feasibility(endpoint, payload)\n",
    "                    st.session_state['feasible_model'] = model_name\n",
    "            with btn2:\n",
    "                if st.button('Check Consistency'):\n",
    "                    st.session_state['consistent'] = check_consistency(endpoint, payload)\n",
    "            with btn3:\n",
    "                if st.button('Add Few-Shot Example') and st.session_state['feasible'] and st.session_state['consistent']:\n",
    "                    st.session_state['history'].append({\n",
    "                        'prompt': st.session_state['prompt'],\n",
    "                        'response': st.session_state['finished_response'],\n",
    "                        'feasible': st.session_state['feasible'],\n",
    "                        'feasible_model': st.session_state['feasible_model'],\n",
    "                        'similar_prompt': st.session_state['similar_prompt'],\n",
    "                        'similar_response': st.session_state['consistent'],\n",
    "                        'instructions': [st.session_state['initial_detail'], st.session_state['full_detail']]\n",
    "                    })\n",
    "        \n",
    "        if st.session_state['feasible']:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.write(f\"The feasibility score (BLEU score) is: {st.session_state['feasible']}%. You should interpret this as the model's ability to parrot the finished response given only the initial prompt.\\n\\nNOTE: Lowering the temperature may result in a higher Feasibility score. Additionally, it should be noted that this metric is somewhat independent of the model's ability to perform well with similar prompts.\")\n",
    "        \n",
    "        if st.session_state['consistent']:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.text_area(\"**Does this response follow the instructions you detailed above?**\", st.session_state['consistent'], height=400, key='ta_3')\n",
    "        \n",
    "    with col2:\n",
    "        for msg in st.session_state['history']:\n",
    "            with st.chat_message('user'):\n",
    "                st.write(f\"**Prompt ({st.session_state['feasible']}% feasible with {st.session_state['feasible_model']}):** {msg['prompt']}\")\n",
    "                st.write(f\"**Response:**\\n{msg['response']}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FILEPATH = 'demo_fewshot_config.json'\n",
    "    \n",
    "    if 'prompt' not in st.session_state:\n",
    "        initialize_session()\n",
    "    \n",
    "    st.session_state['md'] = get_metadata(filepath=FILEPATH)\n",
    "    params = sidebar()\n",
    "    main(params)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
