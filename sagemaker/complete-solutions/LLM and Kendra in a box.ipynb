{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c1f3f6-a06d-40e8-990f-2fb6515f764e",
   "metadata": {},
   "source": [
    "# LLMs in a Box for Search\n",
    "1. Create a SageMaker Studio Domain if you don't have one\n",
    "2. Open SageMaker Studio under the user you plan to launch this applicatio\n",
    "3. Either upload this notebook, or clone the repository: [repo](https://github.com/chaeAclark/literate-eureka.git)\n",
    "4. Open the notebook `LLM and Kendra in a box.ipynb`\n",
    "5. You can run the entire notebook by clicking Run > Run All Cells\n",
    "6. Alternatively, you can run the cells individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7499-d5bb-4fc8-b05e-b0ebf9feaa80",
   "metadata": {},
   "source": [
    "### Terminal Installation\n",
    "You need to ensure you have installed all needed packages in the terminal you are using.\n",
    "1. boto3\n",
    "2. streamlit\n",
    "3. pdf2image\n",
    "4. ai21[SM]\n",
    "5. Pillow\n",
    "6. pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577093f2-cd73-453c-9497-671a0e29acc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "boto3\n",
    "streamlit\n",
    "pdf2image\n",
    "ai21[SM]\n",
    "Pillow\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ccfabe-ed9e-40b3-9b85-d5e260a103ef",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964a254-8b04-49c0-8224-d1e8e24fd33f",
   "metadata": {},
   "source": [
    "#### Update SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e1d828-cf2e-4eac-af5c-81fc3a552ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a85aa9-80f3-444e-9698-de9733c6d18d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8b6ad-9840-483b-8e30-0d4d0d0912df",
   "metadata": {},
   "source": [
    "### General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3550339b-a1a5-41b5-a64c-2d7eccb9acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bc81c-1aef-418f-8e12-c5480c159af9",
   "metadata": {},
   "source": [
    "### SageMaker Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2917d6eb-ee5a-4e1f-95c8-8873372fe090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import model_uris\n",
    "from sagemaker import script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0fdd6-3030-4d0f-97b6-d685c14a8fa5",
   "metadata": {},
   "source": [
    "### Deploy and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee8680a-3684-4cc7-8dd6-fcadb28542d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sm.Session:\n",
    "    \"\"\"\n",
    "    # Create a SageMaker Session\n",
    "    # This function is used to create a SageMaker Session object.\n",
    "    # The SageMaker Session object is used to create a SageMaker Endpoint,\n",
    "    # SageMaker Model, and SageMaker Endpoint Config.\n",
    "    \"\"\"\n",
    "    sagemaker_client = boto3.client(service_name=\"sagemaker\", region_name=boto3.Session().region_name)\n",
    "    session_settings = sm.session_settings.SessionSettings(local_download_dir=local_download_dir)\n",
    "    session = sm.session.Session(sagemaker_client=sagemaker_client, settings=session_settings)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02eb635b-97a2-44b5-b60d-0649e494b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './download_dir'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c60275-6f96-4baf-8058-35eb2bf2d590",
   "metadata": {},
   "source": [
    "### SageMaker Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1519db94-193e-47e0-bc9f-2bd0a8700954",
   "metadata": {},
   "outputs": [],
   "source": [
    "role               = sm.get_execution_role()\n",
    "sagemaker_session  = get_sagemaker_session(model_path) # sm.session.Session()\n",
    "region             = sagemaker_session._region_name\n",
    "\n",
    "# These are needed to show where the streamlit app is hosted\n",
    "sagemaker_metadata = json.load(open('/opt/ml/metadata/resource-metadata.json', 'r'))\n",
    "domain_id          = sagemaker_metadata['DomainId']\n",
    "resource_name      = sagemaker_metadata['ResourceName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473a009-0de5-4336-b600-043391652d61",
   "metadata": {},
   "source": [
    "### Boto Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ce7521-cb17-4e82-886d-f2522d1f1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'DomainSpecificSearch'\n",
    "index_id   = '2f16f2e2-f745-4bd6-bc0c-5fec8947e9c3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190d2f3-9e29-49f8-9366-6734d6d98909",
   "metadata": {},
   "source": [
    "# Model\n",
    "The following section will deploy the JumpStart model `flan-###`. There are additional steps required if launching 3rd-party proprietary models. These steps are detailed in another section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507d925-e504-4947-9745-2849324bed6d",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b8511f-19a0-4a04-9469-21b9eb9906f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available text2text Models:\n",
      "--------------------------------\n",
      "huggingface-text2text-bart4csc-base-chinese\n",
      "huggingface-text2text-bigscience-t0pp\n",
      "huggingface-text2text-bigscience-t0pp-bnb-int8\n",
      "huggingface-text2text-bigscience-t0pp-fp16\n",
      "huggingface-text2text-flan-t5-base\n",
      "huggingface-text2text-flan-t5-base-samsum\n",
      "huggingface-text2text-flan-t5-large\n",
      "huggingface-text2text-flan-t5-small\n",
      "huggingface-text2text-flan-t5-xl\n",
      "huggingface-text2text-flan-t5-xxl\n",
      "huggingface-text2text-flan-t5-xxl-bnb-int8\n",
      "huggingface-text2text-flan-t5-xxl-fp16\n",
      "huggingface-text2text-flan-ul2-bf16\n",
      "huggingface-text2text-pegasus-paraphrase\n",
      "huggingface-text2text-qcpg-sentences\n",
      "huggingface-text2text-t5-one-line-summary\n"
     ]
    }
   ],
   "source": [
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "print('Available text2text Models:\\n--------------------------------')\n",
    "_ = [print(m) for m in text_generation_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b902cc-7aa2-416e-a39d-4efa949f236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model that will be deployed is: huggingface-text2text-flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "model_id = text_generation_models[7]\n",
    "model_version = '*'\n",
    "print(f'The model that will be deployed is: {model_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31359a-2373-4c7b-ba5f-9f7e4091d9b9",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd51fb1-b23e-4064-a045-54db4817b7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: LLM-in-a-box-huggingface-text2text-flan-2023-05-18-22-09-56-127\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = name_from_base(f\"LLM-in-a-box-{model_id}\")\n",
    "print(f'Endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483be2d7-bb22-4d26-a592-5fdae7f208a5",
   "metadata": {},
   "source": [
    "#### Collect Model Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7654cf-9de8-4d69-a890-811d0c5687da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image URI is:  763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "The model data is: s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.0.1/infer-prepack-huggingface-text2text-flan-t5-small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "model_data = model_uris.retrieve(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f'The image URI is:  {image_uri}')\n",
    "print(f'The model data is: {model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60571448-807e-4e3f-b652-8526a5779541",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57995fb1-9488-4655-a152-b3ad01a24d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649c67c-5db4-47c5-8067-5d741c268311",
   "metadata": {},
   "source": [
    "#### Deploy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa25faaa-1c69-4e28-9e06-b0829a6f967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        predictor_cls=Predictor,\n",
    "        endpoint_name=endpoint_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eee911-c201-4d99-8c04-96fcc6fc2af3",
   "metadata": {},
   "source": [
    "#### Test that the model is deployed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d90ea6d-86b0-4c22-915b-d5fe17e41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    sagemaker_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "    input_question = 'Tell me the steps to make a pizza:'\n",
    "    payload = {\n",
    "        \"text_inputs\": input_question,\n",
    "        \"max_length\": 50,\n",
    "        \"max_time\": 50,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "\n",
    "    response = sagemaker.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload).encode('utf-8')\n",
    "    )\n",
    "    output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "    print(output_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64695926-b8f5-4e6a-a6fb-1bb331577759",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to deploy 3rd-party Foundation Models\n",
    "1. Gain access to the foundation models\n",
    "    1. Go to the SageMaker Console\n",
    "    2. There will be a tab for JumpStart > Foundation Models\n",
    "    3. You must request access if you do not already have it\n",
    "2. Select the Foundation you would like to deploy\n",
    "3. Click `Subscribe` in the top-right corner\n",
    "4. After completing, this will allow you to open a notebook that lets you deploy the model\n",
    "5. Open the notebook\n",
    "6. You run this notebook to deploy the model, the caveat is that you must have access to any instance you choose to run.\n",
    "    1. For AI21 Summarization model, you can use something like: ml.g4dn.12xlarge\n",
    "    2. For AI21 Grande Instruct, you can use: ml.g5.24xlarge\n",
    "    3. For AI21 Jumbo Instruct, you can use: ml.g5.48xlarge\n",
    "    4. These were tested to work as of 2023-05-16\n",
    "    5. Collect these endpoint names and use them in the application_metadata JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b66f1-2750-4569-8750-618b002a4735",
   "metadata": {},
   "source": [
    "# Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4fbf7-82e9-4770-839f-ebd98c23a2b5",
   "metadata": {},
   "source": [
    "### Record any parameters that need to be passed to the Streamlit app\n",
    "App Metadata Structure:\n",
    "#### application_metadata\n",
    " - models: a dictionary that contains the model display name, SageMaker endpoint name, and the model type (Currently 'sm' or 'ai21')\n",
    "   - name\n",
    "   - endpoint\n",
    "   - type\n",
    " - summary_model: the summary model endpoint name\n",
    " - region: the region (us-east-1 etc)\n",
    " - role: the permissions for the application. it should include (SageMaker, Textract, and Kendra access)\n",
    " - datastore: a dictionary that contains the bucket and folder prefix used to store document data\n",
    "   - bucket\n",
    "   - prefix\n",
    " - kendra: a dictionary that contains information on the Kendra index to be used when searching\n",
    "   - index_id\n",
    "   - index_name\n",
    "   - index_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e1c2bf-58b6-4339-b2d7-ef7e33655f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_metadata = {\n",
    "    'models':[\n",
    "        {'name':'FLAN-XXL', 'endpoint':'LLM-in-a-box-huggingface-text2text-flan-2023-05-18-19-23-11-479', 'type':'sm'},\n",
    "        {'name':'Super Special Model', 'endpoint':'endpoint_name', 'type':'ai21'}],\n",
    "    'summary_model':'summarize',\n",
    "    'region':region,\n",
    "    'role':role,\n",
    "    'kendra':\n",
    "        {'index_id':index_id, 'index_name':index_name, 'index_description':''},\n",
    "}\n",
    "json.dump(application_metadata, open('application_metadata_search.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec73e1-bc9a-4647-9d57-bd4170f3b949",
   "metadata": {},
   "source": [
    "### Write the Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da3bc03-01df-4a28-bb63-77a47aebe367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_search.py\n",
    "import os\n",
    "import time\n",
    "import ai21\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "from io import BytesIO\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "APP_MD    = json.load(open('application_metadata_search.json', 'r'))\n",
    "MODELS    = {d['name']: d['endpoint'] for d in APP_MD['models']}\n",
    "MODEL_SUM = APP_MD['summary_model']\n",
    "REGION    = APP_MD['region']\n",
    "KENDRA_ID = APP_MD['kendra']['index_id']\n",
    "\n",
    "KENDRA        = boto3.client('kendra', region_name=REGION)\n",
    "SAGEMAKER     = boto3.client('sagemaker-runtime', region_name=REGION)\n",
    "\n",
    "\n",
    "def query_endpoint(endpoint_name, payload):\n",
    "    if 'huggingface' in endpoint_name:\n",
    "        response = SAGEMAKER.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload).encode('utf-8')\n",
    "        )\n",
    "        output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "    else:\n",
    "        response = ai21.Completion.execute(\n",
    "            sm_endpoint=endpoint_name,\n",
    "            prompt=payload['text_inputs'],\n",
    "            maxTokens=payload['max_length'],\n",
    "            temperature=payload['temperature'],\n",
    "            stopSequences=['##'],\n",
    "            numResults=1\n",
    "        )\n",
    "        output_answer = response['completions'][0]['data']['text']\n",
    "    return str(output_answer)\n",
    "\n",
    "\n",
    "def query_index(query):\n",
    "    response = KENDRA.query(\n",
    "        QueryText = query,\n",
    "        IndexId = KENDRA_ID\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def summarize_context(context):\n",
    "    try:\n",
    "        response = ai21.Summarize.execute(\n",
    "            source=context,\n",
    "            sourceType=\"TEXT\",\n",
    "            sm_endpoint=MODEL_SUM\n",
    "        )\n",
    "        return response.summary\n",
    "    except:\n",
    "        return 'No summarization endpoint connected'\n",
    "\n",
    "\n",
    "def action_search(params):\n",
    "    st.title('Search you Internal Documents')\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        query = st.text_input('**Enter a Search Query:**', '')\n",
    "        button_search = st.button('Search')\n",
    "        if button_search:\n",
    "            response = query_index(query)\n",
    "            for sr in response['ResultItems']:\n",
    "                st.write(f\"**[{sr['ScoreAttributes']['ScoreConfidence']}]** | {sr['DocumentTitle']['Text']} [Link to Source Document]({sr['DocumentURI']})\")\n",
    "                st.write(sr['DocumentExcerpt']['Text'])\n",
    "                st.write('---')\n",
    "            context = '\\n'.join([sr['DocumentExcerpt']['Text'] for sr in response['ResultItems']])\n",
    "            with open('context.txt', 'w') as fp:\n",
    "                fp.write(context)\n",
    "    with col2:\n",
    "        if button_search:\n",
    "            time.sleep(2)\n",
    "            summary = summarize_context(context)\n",
    "            st.text_area('**Search Result Summary:**', summary)\n",
    "        input_question = st.text_input('**Please ask a question of the search results:**', '')\n",
    "        if st.button('Send Question') and len(input_question) > 3:\n",
    "            with open('context.txt', 'r') as fp:\n",
    "                context = fp.read()\n",
    "            payload = {\n",
    "                \"text_inputs\": context + '##\\n' + input_question,\n",
    "                \"max_length\": params['max_len'],\n",
    "                \"max_time\": 50,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"top_k\": 50,\n",
    "                \"temperature\": params['temp'],\n",
    "                \"top_p\": params['top_p'],\n",
    "                \"do_sample\": True,\n",
    "            }\n",
    "            output_answer = query_endpoint(params['endpoint'], payload)\n",
    "            st.text_area('Response:', output_answer)\n",
    "\n",
    "\n",
    "def app_sidebar():\n",
    "    with st.sidebar:\n",
    "        st.write('## How to use:')\n",
    "        description = \"\"\"Welcome to our LLM tool extraction and query answering application. With this app, you can aske general question, \n",
    "        ask questions of a specific document, or intelligently search an internal document corpus. By selection the action you would like to perform,\n",
    "         you can ask general questions, or questions of your document. Additionally, you can select the model you use, to perform real-world tests to determine model strengths and weakneses.\"\"\"\n",
    "        st.write(description)\n",
    "        st.write('---')\n",
    "        st.write('### User Preference')\n",
    "        action_name = st.selectbox('Choose Activity', options=['Corpus Search'])\n",
    "        model_name = st.selectbox('Select Model', options=MODELS.keys())\n",
    "        max_len = st.slider('Max Length', min_value=50, max_value=500, value=150, step=10)\n",
    "        top_p = st.slider('Top p', min_value=0., max_value=1., value=1., step=.01)\n",
    "        temp = st.slider('Temperature', min_value=0.01, max_value=1., value=1., step=.01)\n",
    "        st.write('---')\n",
    "        st.write('## FAQ')\n",
    "        st.write(f'**1. Where is the model stored?** \\n\\nThe current model is: `{model_name}` and is running within your account.')\n",
    "        st.write(f'**2. Where is my data stored?**\\n\\nCurrently the queries you make to the endpoint are not stored, but you can enaable this by capturing data from your endpoint.')\n",
    "        st.write('---')\n",
    "        params = {'action_name':action_name,'endpoint':MODELS[model_name], 'max_len':max_len, 'top_p':top_p, 'temp':temp, 'model_name':model_name}\n",
    "        return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    params = app_sidebar()\n",
    "    endpoint=params['endpoint']\n",
    "    if params['action_name'] == 'Corpus Search':\n",
    "        action_search(params)\n",
    "    else:\n",
    "        raise ValueError('Invalid Action')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c91914-9a00-4961-bed8-37a7e543adff",
   "metadata": {},
   "source": [
    "## Start App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10ab32-44a5-47ef-8e6c-c6825dd8e6ee",
   "metadata": {},
   "source": [
    "### Run Streamlit\n",
    "To run the application:\n",
    "1. Select File > New > Terminal\n",
    "2. In the terminal, use the command: `streamlit run app_search.py --server.runOnSave true`\n",
    "   1. Note: ensure you have installed all required packages\n",
    "3. If this is successful, you will be able to interact with the app by using the web address below\n",
    "4. An important thing to note is that when you run the above command, you should see an output similar to below.\n",
    "5. The port thats  displayed is the same port that MUST be used after the `proxy` folder below.\n",
    "`\n",
    "You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://###.###.###.###:8501\\\n",
    "  External URL: http://###.###.###.###:8501\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31453036-6eff-42da-9b6c-f53cc8b54381",
   "metadata": {},
   "source": [
    "#### Display Link to Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06fa9910-1dc9-4d76-a854-dc5f7fdd73dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://d-qxdwe39zkab0.studio.us-east-1.sagemaker.aws/jupyter/default/proxy/8501/\n"
     ]
    }
   ],
   "source": [
    "print(f'http://{domain_id}.studio.{region}.sagemaker.aws/jupyter/default/proxy/8501/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b072d7-3df3-4878-996e-5acec9a0be55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
