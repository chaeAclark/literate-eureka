{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab7900a-d929-4f98-b786-5974a4e93b9e",
   "metadata": {},
   "source": [
    "# LLMs in a Box\n",
    "1. Create a SageMaker Studio Domain if you don't have one\n",
    "2. Open SageMaker Studio under the user you plan to launch this applicatio\n",
    "3. Either upload this notebook, or clone the repository: [repo](https://github.com/chaeAclark/literate-eureka.git)\n",
    "4. Open the notebook `LLM in a box.ipynb`\n",
    "5. You can run the entire notebook by clicking Run > Run All Cells\n",
    "6. Alternatively, you can run the cells individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e3621-2eb2-4440-99a8-fdeea14ea4d9",
   "metadata": {},
   "source": [
    "### Terminal Installation\n",
    "You need to ensure you have installed all needed packages in the terminal you are using.\n",
    "1. boto3\n",
    "2. streamlit\n",
    "3. pdf2image\n",
    "4. ai21[SM]\n",
    "5. Pillow\n",
    "6. pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcd832-45a6-436a-b89d-ac05acbcad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "boto3\n",
    "streamlit\n",
    "pdf2image\n",
    "ai21[SM]\n",
    "Pillow\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf69b5-b237-4561-8687-b775306cbbec",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a34d4-2b82-4551-b19a-fe35f06ae656",
   "metadata": {},
   "source": [
    "#### Update SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd712b-6149-4a0b-a610-f60f11fa1baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.27.111 requires botocore==1.29.111, but you have botocore 1.29.135 which is incompatible.\n",
      "awscli 1.27.111 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.27.111 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.135 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf58094-779d-4c6d-8744-b8761008c280",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f39667-487c-45e9-821d-11081c785aa5",
   "metadata": {},
   "source": [
    "### General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fe1c41-5215-47ac-b8cb-a26fd330c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399b8fa-9126-4da3-870e-57d1c7f7d0a3",
   "metadata": {},
   "source": [
    "### SageMaker Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3892c9-1dcf-40f9-a434-82b75d48b529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import model_uris\n",
    "from sagemaker import script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc0a1b-9a9a-4a87-af2c-1c6694b31d00",
   "metadata": {},
   "source": [
    "### Deploy and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9317ed0-72d5-4b55-8733-eeea358897dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sm.Session:\n",
    "    \"\"\"\n",
    "    # Create a SageMaker Session\n",
    "    # This function is used to create a SageMaker Session object.\n",
    "    # The SageMaker Session object is used to create a SageMaker Endpoint,\n",
    "    # SageMaker Model, and SageMaker Endpoint Config.\n",
    "    \"\"\"\n",
    "    sagemaker_client = boto3.client(service_name=\"sagemaker\", region_name=boto3.Session().region_name)\n",
    "    session_settings = sm.session_settings.SessionSettings(local_download_dir=local_download_dir)\n",
    "    session = sm.session.Session(sagemaker_client=sagemaker_client, settings=session_settings)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3c8cdf-e341-49f8-bf52-737d96359f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './download_dir'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2f8c7-b992-4f54-ba7c-586a19582056",
   "metadata": {},
   "source": [
    "### SageMaker Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac40b85-edf2-4c98-8a64-cba9372a40d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role               = sm.get_execution_role()\n",
    "sagemaker_session  = get_sagemaker_session(model_path) # sm.session.Session()\n",
    "region             = sagemaker_session._region_name\n",
    "\n",
    "# These are needed to show where the streamlit app is hosted\n",
    "sagemaker_metadata = json.load(open('/opt/ml/metadata/resource-metadata.json', 'r'))\n",
    "domain_id          = sagemaker_metadata['DomainId']\n",
    "resource_name      = sagemaker_metadata['ResourceName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8807f9a-5541-4ed1-b63b-c46b2698c0f1",
   "metadata": {},
   "source": [
    "### Boto Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d6dfaa-d90d-4094-9450-6b778479f58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6e8a58-4fdb-4a11-bfcf-88b90f380d42",
   "metadata": {},
   "source": [
    "# Model\n",
    "The following section will deploy the JumpStart model `flan-###`. There are additional steps required if launching 3rd-party proprietary models. These steps are detailed in another section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37eda5-8c47-4942-bd71-c6667f8e56f3",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0724ec9a-9389-4d2d-9b6a-72758447bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available text2text Models:\n",
      "--------------------------------\n",
      "huggingface-text2text-bart4csc-base-chinese\n",
      "huggingface-text2text-bigscience-t0pp\n",
      "huggingface-text2text-bigscience-t0pp-bnb-int8\n",
      "huggingface-text2text-bigscience-t0pp-fp16\n",
      "huggingface-text2text-flan-t5-base\n",
      "huggingface-text2text-flan-t5-base-samsum\n",
      "huggingface-text2text-flan-t5-large\n",
      "huggingface-text2text-flan-t5-small\n",
      "huggingface-text2text-flan-t5-xl\n",
      "huggingface-text2text-flan-t5-xxl\n",
      "huggingface-text2text-flan-t5-xxl-bnb-int8\n",
      "huggingface-text2text-flan-t5-xxl-fp16\n",
      "huggingface-text2text-flan-ul2-bf16\n",
      "huggingface-text2text-pegasus-paraphrase\n",
      "huggingface-text2text-qcpg-sentences\n",
      "huggingface-text2text-t5-one-line-summary\n"
     ]
    }
   ],
   "source": [
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "print('Available text2text Models:\\n--------------------------------')\n",
    "_ = [print(m) for m in text_generation_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df38c2e6-5d99-406a-b3a9-3c4162d1a44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model that will be deployed is: huggingface-text2text-flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "model_id = text_generation_models[7]\n",
    "model_version = '*'\n",
    "print(f'The model that will be deployed is: {model_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306720c-2ee4-48fb-802e-2064b0d44f01",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a7586a-4c4e-486d-bfd0-998a7a8f69f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: LLM-in-a-box-huggingface-text2text-flan-2023-05-18-19-23-11-479\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = name_from_base(f\"LLM-in-a-box-{model_id}\")\n",
    "print(f'Endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe20c5-81dc-464c-99f3-08f871ad4fec",
   "metadata": {},
   "source": [
    "#### Collect Model Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a4ca61-a4c0-4732-bd15-65eaa48cfc77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image URI is:  763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "The model data is: s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.0.1/infer-prepack-huggingface-text2text-flan-t5-small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "model_data = model_uris.retrieve(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f'The image URI is:  {image_uri}')\n",
    "print(f'The model data is: {model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625234b-0401-4e03-8f5a-fbce67884f7a",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6257ecb7-910a-4407-b357-2d38cf354290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29b980-a806-42f8-a365-e45930a3e691",
   "metadata": {},
   "source": [
    "#### Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f08bafe-c739-47ac-a2fd-66d8829ff87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f1410-f994-499f-a886-2adef921cef7",
   "metadata": {},
   "source": [
    "#### Test that the model is correctly deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "796ef379-3675-4c7b-84dd-8fff204108f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make a pizza, first you should use 3 pizzas per day for 2 pizzas. To make a pizza, you will need 2 slices each. You don't need to use a heavy spout to scoop\n"
     ]
    }
   ],
   "source": [
    "sagemaker = boto3.client('sagemaker-runtime', region_name=region)\n",
    "input_question = 'Tell me the steps to make a pizza:'\n",
    "payload = {\n",
    "    \"text_inputs\": input_question,\n",
    "    \"max_length\": 50,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "response = sagemaker.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload).encode('utf-8')\n",
    ")\n",
    "output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697b6b8-e864-41ef-8e5e-4f2039b437cf",
   "metadata": {},
   "source": [
    "## How to deploy 3rd-party Foundation Models\n",
    "1. Gain access to the foundation models\n",
    "    1. Go to the SageMaker Console\n",
    "    2. There will be a tab for JumpStart > Foundation Models\n",
    "    3. You must request access if you do not already have it\n",
    "2. Select the Foundation you would like to deploy\n",
    "3. Click `Subscribe` in the top-right corner\n",
    "4. After completing, this will allow you to open a notebook that lets you deploy the model\n",
    "5. Open the notebook\n",
    "6. You run this notebook to deploy the model, the caveat is that you must have access to any instance you choose to run.\n",
    "    1. For AI21 Summarization model, you can use something like: ml.g4dn.12xlarge\n",
    "    2. For AI21 Grande Instruct, you can use: ml.g5.24xlarge\n",
    "    3. For AI21 Jumbo Instruct, you can use: ml.g5.48xlarge\n",
    "    4. These were tested to work as of 2023-05-16\n",
    "    5. Collect these endpoint names and use them in the application_metadata JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5593b8-ef3d-40ee-82b0-8a3c2ba78540",
   "metadata": {},
   "source": [
    "# Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bfe17a-4334-40df-844b-9e195b3014b1",
   "metadata": {},
   "source": [
    "### Record any parameters that need to be passed to the Streamlit app\n",
    "App Metadata Structure:\n",
    "#### application_metadata\n",
    " - models: a dictionary that contains the model display name, SageMaker endpoint name, and the model type (Currently 'sm' or 'ai21')\n",
    "   - name\n",
    "   - endpoint\n",
    "   - type\n",
    " - summary_model: the summary model endpoint name\n",
    " - region: the region (us-east-1 etc)\n",
    " - role: the permissions for the application. it should include (SageMaker, Textract, and Kendra access)\n",
    " - datastore: a dictionary that contains the bucket and folder prefix used to store document data\n",
    "   - bucket\n",
    "   - prefix\n",
    " - kendra: a dictionary that contains information on the Kendra index to be used when searching\n",
    "   - index_id\n",
    "   - index_name\n",
    "   - index_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b14d430-1f59-492c-bfb7-3aafa1a9924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_metadata = {\n",
    "    'models':[\n",
    "        {'name':'FLAN-Small', 'endpoint':'LLM-in-a-box-huggingface-text2text-flan-2023-05-18-19-23-11-479', 'type':'sm'},\n",
    "        {'name':'Super Fancy Model', 'endpoint':'', 'type':'ai21'}],\n",
    "    'region':region,\n",
    "    'role':role,\n",
    "}\n",
    "json.dump(application_metadata, open('application_metadata_qna.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82af863-7f0e-4eac-8b01-127d55a84fa5",
   "metadata": {},
   "source": [
    "### Write the Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d219f79-5816-4f6b-8705-535df9cc614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_qna.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_qna.py\n",
    "import os\n",
    "import ai21\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "from datetime import datetime\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "APP_MD        = json.load(open('application_metadata_qna.json', 'r'))\n",
    "MODELS        = {d['name']: d['endpoint'] for d in APP_MD['models']}\n",
    "REGION        = APP_MD['region']\n",
    "SAGEMAKER     = boto3.client('sagemaker-runtime', region_name=REGION)\n",
    "CHAT_FILENAME = 'chat.csv'\n",
    "\n",
    "def query_endpoint(endpoint_name, payload):\n",
    "    \"\"\"\n",
    "    Query the endpoint and return the answer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint_name : str\n",
    "        The name of the endpoint.\n",
    "    payload : dict\n",
    "        The payload to send to the endpoint.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The answer from the endpoint.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the endpoint is not found.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function is a wrapper around the SageMaker Runtime API.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMaker.Client.invoke_endpoint\n",
    "    https://docs.aws.amazon.com/sagemaker/latest/dg/API_InvokeEndpoint.html\n",
    "    https://docs.aws.amazon.com/sagemaker/latest/dg/API_InvokeEndpoint.html#SageMaker-Type-InvokeEndpoint-Body\n",
    "    https://docs.aws.amazon.com/sagemaker/latest/dg/API_InvokeEndpoint.html#SageMaker-Type-InvokeEndpoint-ContentType\n",
    "    \"\"\"\n",
    "    if 'huggingface' in endpoint_name:\n",
    "        response = SAGEMAKER.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload).encode('utf-8')\n",
    "        )\n",
    "        output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "    else:\n",
    "        response = ai21.Completion.execute(\n",
    "            sm_endpoint=endpoint_name,\n",
    "            prompt=payload['text_inputs'],\n",
    "            maxTokens=payload['max_length'],\n",
    "            temperature=payload['temperature'],\n",
    "            stopSequences=['##'],\n",
    "            numResults=1\n",
    "        )\n",
    "        output_answer = response['completions'][0]['data']['text']\n",
    "    return str(output_answer)\n",
    "\n",
    "\n",
    "def action_qna(params):\n",
    "    \"\"\"\n",
    "    Generates the interface around asking questions of your LLM endpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        The parameters of the application.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function is a wrapper around the Streamlit app.\n",
    "    \"\"\"\n",
    "    st.title('Ask Questions of your Model')\n",
    "    try:\n",
    "        chat_df = pd.read_csv(CHAT_FILENAME)\n",
    "    except:\n",
    "        chat_df = pd.DataFrame([], columns=['timestamp', 'question', 'response'])\n",
    "    \n",
    "    input_question = st.text_input('**Please ask a question:**', '')\n",
    "    if st.button('Send Question') and len(input_question) > 3:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        context = '\\n'.join(['Context: ' + str(r.question) + '##\\n\\n: ' + str(r.response) + '\\n' for idx,r in chat_df.iloc[-10:].iterrows()])\n",
    "        payload = {\n",
    "            \"text_inputs\": input_question, #context + '\\n' + input_question,\n",
    "            \"max_length\": params['max_len'],\n",
    "            \"max_time\": 50,\n",
    "            \"num_return_sequences\": 1,\n",
    "            \"top_k\": 50,\n",
    "            \"temperature\": params['temp'],\n",
    "            \"top_p\": params['top_p'],\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "        output_answer = query_endpoint(params['endpoint'], payload)\n",
    "        st.text_area('Response:', output_answer)\n",
    "        chat_df.loc[len(chat_df.index)] = [timestamp, input_question, output_answer]\n",
    "        chat_df.tail(10).to_csv(CHAT_FILENAME, index=False)\n",
    "            \n",
    "    st.subheader('Recent Questions:')\n",
    "    for idx,row in chat_df.iloc[::-1].head(10).iterrows():\n",
    "        st.write(f'**{row.timestamp}**')\n",
    "        st.write(row.question)\n",
    "        st.write(row.response)\n",
    "        st.write('---')\n",
    "\n",
    "\n",
    "def app_sidebar():\n",
    "    \"\"\"\n",
    "    The sidebar of the Streamlit app.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The parameters of the application.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function is a wrapper around the Streamlit sidebar.\n",
    "    \"\"\"\n",
    "    with st.sidebar:\n",
    "        st.write('## How to use:')\n",
    "        description = \"\"\"Welcome to our LLM tool extraction and query answering application. With this app, you can aske general question, \n",
    "        ask questions of a specific document, or intelligently search an internal document corpus. By selection the action you would like to perform,\n",
    "         you can ask general questions, or questions of your document. Additionally, you can select the model you use, to perform real-world tests to determine model strengths and weakneses.\"\"\"\n",
    "        st.write(description)\n",
    "        st.write('---')\n",
    "        st.write('### User Preference')\n",
    "        if st.button('Clear Context'):\n",
    "            pd.DataFrame([], columns=['timestamp', 'question', 'response']).to_csv(CHAT_FILENAME, index=False)\n",
    "        action_name = st.selectbox('Choose Activity', options=['Question/Answer'])\n",
    "        model_name = st.selectbox('Select Model', options=MODELS.keys())\n",
    "        max_len = st.slider('Max Length', min_value=50, max_value=500, value=150, step=10)\n",
    "        top_p = st.slider('Top p', min_value=0., max_value=1., value=1., step=.01)\n",
    "        temp = st.slider('Temperature', min_value=0.01, max_value=1., value=1., step=.01)\n",
    "        st.write('---')\n",
    "        st.write('## FAQ')\n",
    "        st.write(f'**1. Where is the model stored?** \\n\\nThe current model is: `{model_name}` and is running within your account.')\n",
    "        st.write(f'**2. Where is my data stored?**\\n\\nA limited context window is stored locally. This is retained as a csv file, but for production it would be prudent to use a more robust datastore (e.g.DynamoDB).')\n",
    "        st.write('---')\n",
    "        params = {'action_name':action_name,'endpoint':MODELS[model_name], 'max_len':max_len, 'top_p':top_p, 'temp':temp, 'model_name':model_name}\n",
    "    return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    params = app_sidebar()\n",
    "    endpoint=params['endpoint']\n",
    "    if params['action_name'] == 'Question/Answer':\n",
    "        action_qna(params)\n",
    "    else:\n",
    "        raise ValueError('Invalid action name.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f8983-425c-4600-bda2-ee9db3aa35c6",
   "metadata": {},
   "source": [
    "# Start App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f01d9-2999-4605-94dd-f3a01793ed7a",
   "metadata": {},
   "source": [
    "### Run Streamlit\n",
    "To run the application:\n",
    "1. Select File > New > Terminal\n",
    "2. In the terminal, use the command: `streamlit run app_qna.py --server.runOnSave true`\n",
    "   1. Note: ensure you have installed all required packages\n",
    "3. If this is successful, you will be able to interact with the app by using the web address below\n",
    "4. An important thing to note is that when you run the above command, you should see an output similar to below.\n",
    "5. The port thats  displayed is the same port that MUST be used after the `proxy` folder below.\n",
    "`\n",
    "You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://###.###.###.###:8501\\\n",
    "  External URL: http://###.###.###.###:8501\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fce21e-bd5a-423c-800b-ebef4f6b0977",
   "metadata": {},
   "source": [
    "#### Display Link to Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "379d0c40-e193-41bc-a349-c41272e684ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://d-qxdwe39zkab0.studio.us-east-1.sagemaker.aws/jupyter/default/proxy/8501/\n"
     ]
    }
   ],
   "source": [
    "print(f'http://{domain_id}.studio.{region}.sagemaker.aws/jupyter/default/proxy/8501/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22618b7b-3ddb-4004-bed2-44366f9b4552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
