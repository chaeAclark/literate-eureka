{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba85b71a-bb9e-4e82-aa70-1c37ecf5a3eb",
   "metadata": {},
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2608884-d637-4c3c-a6d6-78bc3e58060e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sagemaker as sm\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.processing import ProcessingInput\n",
    "from sagemaker.processing import ProcessingOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2517a-1a5a-45ee-a6d0-15857bb799c2",
   "metadata": {},
   "source": [
    "#### SageMaker Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f2a33d-1419-4fad-9e1d-d3a5d505ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-047840628716\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "today_str = today.strftime('%Y-%m-%d')\n",
    "role = sm.get_execution_role()\n",
    "sagemaker_session = sm.session.Session()\n",
    "region = sagemaker_session._region_name\n",
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecd814-7986-49e8-acda-ed8c4600cab8",
   "metadata": {},
   "source": [
    "#### Requirements File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94304e59-e4f3-4d85-947f-a4e80392c209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source_dir/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./source_dir/requirements.txt\n",
    "transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9e93e-09e6-41b0-9845-f8109ccc7f07",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f8f2c-2562-4f1b-b510-d5d96e82dd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad103208-5bfe-4ed7-b9bf-4a042bf2f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source_dir/medical_language_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./source_dir/medical_language_processing.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "RANDOM_STATE = 2023\n",
    "random.seed(RANDOM_STATE)\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768, bos_token='<|startoftext|>', eos_token='<|endoftext|>'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(bos_token + txt + eos_token, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "\n",
    "def save_df(df, filepath, labels=None, labels_path=None):\n",
    "    df.to_csv(filepath)\n",
    "    if labels is not None:\n",
    "        with open(labels_path, \"wb\") as fp:   \n",
    "            pickle.dump(labels, fp)\n",
    "\n",
    "\n",
    "def save_dataset(dataset, filepath):\n",
    "    torch.save(dataset, filepath)\n",
    "\n",
    "\n",
    "def load_data(filename, left_col, right_col, index_col=0):\n",
    "    filepath = os.path.join(\"/opt/ml/processing/input/\", filename)\n",
    "    df = pd.read_csv(filepath, index_col=index_col).dropna().reset_index(drop=True)\n",
    "    labels = list(df[left_col].drop_duplicates().dropna().values.ravel())\n",
    "    df = df[left_col] + ' | ' + df[right_col]\n",
    "    return df, labels\n",
    "\n",
    "\n",
    "def process_data(df, test_size, valid_size):\n",
    "    ids = list(df.index.drop_duplicates().values.ravel())\n",
    "    id_train, id_test = train_test_split(ids, test_size=test_size, shuffle=True, random_state=RANDOM_STATE)\n",
    "    id_test, id_valid = train_test_split(id_test, test_size=valid_size, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    df_train = df[df.index.isin(id_train)]\n",
    "    df_valid = df[df.index.isin(id_valid)]\n",
    "    df_test = df[df.index.isin(id_test)]\n",
    "\n",
    "    print(f\"Training Data Length:   {len(df_train)}\")\n",
    "    print(f\"Testing Data Length:    {len(df_test)}\")\n",
    "    print(f\"Validation Data Length: {len(df_valid)}\")\n",
    "    return df_train, df_test, df_valid\n",
    "\n",
    "\n",
    "def create_dataset(df, pretrained_path, max_length, batch_size=2, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_path, bos_token=bos_token, eos_token=eos_token, pad_token=pad_token)\n",
    "    dataset = GPT2Dataset(df, tokenizer, max_length=max_length, bos_token=bos_token, eos_token=eos_token)\n",
    "    dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test_size', type=str, default='0.2')\n",
    "    parser.add_argument('--valid_size', type=str, default='0.5')\n",
    "    parser.add_argument('--left_col', type=str, default='')\n",
    "    parser.add_argument('--right_col', type=str, default='')\n",
    "    parser.add_argument('--pretrained_path', type=str, default='distilgpt2')\n",
    "    parser.add_argument('--filename', type=str, default='')\n",
    "    parser.add_argument('--max_length', type=str, default='128')\n",
    "    parser.add_argument('--bos_token', type=str, default='<|startoftext|>')\n",
    "    parser.add_argument('--eos_token', type=str, default='<|endoftext|>')\n",
    "    parser.add_argument('--pad_token', type=str, default='<|pad|>')\n",
    "    parser.add_argument('--batch_size', type=str, default='3')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    df, labels = load_data(args.filename, args.left_col, args.right_col)\n",
    "    df_train, df_test, df_valid = process_data(df, float(args.test_size), float(args.valid_size))\n",
    "    dataloader_train = create_dataset(df_train, args.pretrained_path, int(args.max_length), int(args.batch_size), args.bos_token, args.eos_token, args.pad_token)\n",
    "    dataloader_test = create_dataset(df_test, args.pretrained_path, int(args.max_length), int(args.batch_size), args.bos_token, args.eos_token, args.pad_token)\n",
    "    dataloader_valid = create_dataset(df_valid, args.pretrained_path, int(args.max_length), int(args.batch_size), args.bos_token, args.eos_token, args.pad_token)\n",
    "    \n",
    "    save_df(df_train, os.path.join('/opt/ml/processing/output/','df_train.csv'), labels=labels, labels_path=os.path.join('/opt/ml/processing/output/','labels.pkl'))\n",
    "    save_df(df_test, os.path.join('/opt/ml/processing/output/','df_test.csv'))\n",
    "    save_df(df_valid, os.path.join('/opt/ml/processing/output/','df_valid.csv'))\n",
    "    save_dataset(dataloader_train, os.path.join('/opt/ml/processing/output/','dataset_train.bin'))\n",
    "    save_dataset(dataloader_test, os.path.join('/opt/ml/processing/output/','dataset_test.bin'))\n",
    "    save_dataset(dataloader_valid, os.path.join('/opt/ml/processing/output/','dataset_valid.bin'))\n",
    "    print(\"Completed Processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280909c-c25d-4544-afe4-58cc5f68f5b0",
   "metadata": {},
   "source": [
    "#### Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c97f4c-aadc-4189-92b5-06c7d40d0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceProcessor only supports GPU for now\n",
    "pytorch_processor = PyTorchProcessor(\n",
    "    framework_version='1.8',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name='distilGPT-Processing-Job'\n",
    ")\n",
    "\n",
    "#s3://sagemaker-us-east-1-047840628716/data/raw/text/\n",
    "source = f\"s3://{bucket}/data/raw/text\"\n",
    "destination = \"/opt/ml/processing/input\"\n",
    "inputs=[ProcessingInput(source=source, destination=destination)]\n",
    "\n",
    "\n",
    "source = \"/opt/ml/processing/output\"\n",
    "destination = f\"s3://{bucket}/data/processed/text\"\n",
    "outputs = [ProcessingOutput(source=source, destination=destination)]\n",
    "\n",
    "\n",
    "arguments = [\n",
    "    \"--test_size\", \"0.2\",\n",
    "    \"--valid_size\", \"0.5\",\n",
    "    \"--left_col\", \"medical_specialty\",\n",
    "    \"--right_col\", \"description\",\n",
    "    \"--pretrained_path\", \"distilgpt2\",\n",
    "    \"--filename\", \"mtsamples.csv\",\n",
    "    \"--max_len\", \"128\",\n",
    "    \"--bos_token\", \"<|startoftext|>\",\n",
    "    \"--eos_token\", \"<|endoftext|>\",\n",
    "    \"--pad_token\", \"<|pad|>\",\n",
    "    \"--batch_size\", \"3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f6e10-c50b-417a-b25a-382f69e9a9cf",
   "metadata": {},
   "source": [
    "#### Run Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d78ced-b60d-47d5-b894-ba0ce27cf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    pytorch_processor.run(\n",
    "        code='medical_language_processing.py',\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        arguments=arguments,\n",
    "        source_dir='./source_dir'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ecbcf-7cd4-4c73-9a32-0360317a558d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0976f7c-c3d5-43ee-86bf-9867272a8f93",
   "metadata": {},
   "source": [
    "#### Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5ab91b-3c38-49c6-89f3-22015340415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source_dir/medical_language_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./source_dir/medical_language_training.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Config\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EXAMPLE_PROMPTS = ['']\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768, bos_token='<|startoftext|>', eos_token='<|endoftext|>'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(bos_token + txt + eos_token, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "\n",
    "def load_data(filename, path, label_filename=None, label_path=None):\n",
    "    filepath = os.path.join(path, filename)\n",
    "    dataloader = torch.load(filepath)\n",
    "    \n",
    "    if label_filename is not None:\n",
    "        labels = pickle.load(open(os.path.join(label_path, label_filename), 'rb'))\n",
    "        EXAMPLE_PROMPTS.remove('')\n",
    "        EXAMPLE_PROMPTS.extend(labels)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def create_model(pretrained_path, device, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>'):\n",
    "    config = GPT2Config.from_pretrained(pretrained_path, output_hidden_states=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_path, bos_token=bos_token, eos_token=eos_token, pad_token=pad_token)\n",
    "    model = GPT2LMHeadModel.from_pretrained(pretrained_path, config=config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def set_training_parameters(dataloader, learning_rate, epsilon, warmup_steps, epochs):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "    total_steps = len(dataloader) * epochs \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    print(f\"This training run will process {total_steps} steps in total.\")\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, tokenizer, epochs, optimizer, scheduler, device, sample_every):\n",
    "    for epoch_i in range(epochs):\n",
    "        model.train()\n",
    "        loss_total = 0.\n",
    "\n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "            model.zero_grad() \n",
    "\n",
    "            ids_batch = batch[0].to(device)\n",
    "            labels_batch = batch[0].to(device)\n",
    "            mask_batch = batch[1].to(device)\n",
    "\n",
    "            output = model(ids_batch, labels=labels_batch, attention_mask=mask_batch)\n",
    "            loss = output[0]\n",
    "            loss_batch = loss.item()\n",
    "            loss_total += loss_batch\n",
    "\n",
    "            if (step % sample_every) == 0:\n",
    "                model.eval()\n",
    "                prompt_embedding = torch.tensor(tokenizer.encode(random.choice(EXAMPLE_PROMPTS) + ' | ')).unsqueeze(0).to(device)\n",
    "                generated_sample = model.generate(\n",
    "                    prompt_embedding,\n",
    "                    pad_token_id=50256,\n",
    "                    do_sample=True,   \n",
    "                    top_k=50, \n",
    "                    max_length=128,\n",
    "                    top_p=0.99, \n",
    "                    num_return_sequences=5\n",
    "                )\n",
    "                for example in generated_sample:\n",
    "                    generated_sample = tokenizer.decode(example, skip_special_tokens=True)\n",
    "                    print(generated_sample)\n",
    "                print('\\n')\n",
    "                model.train()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_loss = loss_total / len(dataloader_train)\n",
    "    print(f'Epochs={epoch_i+1}; TotLoss={loss_total};')\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--valid', type=str, default=os.environ['SM_CHANNEL_VALID'])\n",
    "    parser.add_argument('--labels', type=str, default=os.environ['SM_CHANNEL_LABELS'])\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "    parser.add_argument('--filename_train', type=str, default='')\n",
    "    parser.add_argument('--filename_valid', type=str, default='')\n",
    "    parser.add_argument('--filename_test', type=str, default='')\n",
    "    parser.add_argument('--filename_labels', type=str, default='labels.pkl')\n",
    "    parser.add_argument('--max_length', type=int, default=128)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
    "    parser.add_argument('--warmup_steps', type=int, default=100)\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8)\n",
    "    parser.add_argument('--sample_every', type=int, default=100)\n",
    "    parser.add_argument('--bos_token', type=str, default='<|startoftext|>')\n",
    "    parser.add_argument('--eos_token', type=str, default='<|endoftext|>')\n",
    "    parser.add_argument('--pad_token', type=str, default='<|pad|>')\n",
    "    parser.add_argument('--pretrained_path', type=str, default='distilgpt2')\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "    dataloader_train =  load_data(args.filename_train, path='/opt/ml/input/data/train/', label_filename=args.filename_labels, label_path='/opt/ml/input/data/labels/')\n",
    "    if len(args.filename_valid) > 0:\n",
    "        dataloader_valid =  load_data(args.filename_valid, path='/opt/ml/input/data/valid/')\n",
    "    if len(args.filename_test) > 0:\n",
    "        dataloader_test =  load_data(args.filename_test, path='/opt/ml/input/data/test/')\n",
    "    model, tokenizer = create_model(args.pretrained_path, device=device, bos_token=args.bos_token, eos_token=args.eos_token, pad_token=args.pad_token)\n",
    "    optimizer, scheduler = set_training_parameters(dataloader_train, args.learning_rate, args.epsilon, args.warmup_steps, args.epochs)\n",
    "    model = train_model(model, dataloader_train, tokenizer, args.epochs, optimizer, scheduler, device, args.sample_every)\n",
    "    \n",
    "    model.save_pretrained(os.path.join(args.model_dir, \"20230112_distilgpt2_medical_generator/\"))\n",
    "    tokenizer.save_pretrained(os.path.join(args.model_dir, \"20230112_distilgpt2_medical_generator/\"))\n",
    "    print(\"Completed Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640bbd1-7922-432b-b85e-78867d9fc93c",
   "metadata": {},
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bffb9c-ff33-4592-820e-be79bdc3b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/data/processed/text/dataset_train.bin\"\n",
    "valid_path = f\"s3://{bucket}/data/processed/text/dataset_valid.bin\"\n",
    "labels_path = f\"s3://{bucket}/data/processed/text/labels.pkl\"\n",
    "inputs = {\"train\": train_path, \"valid\": valid_path, \"labels\":labels_path}\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"device\":\"cpu\",\n",
    "    \"filename_train\":\"dataset_train.bin\",\n",
    "    \"filename_valid\":\"dataset_valid.bin\",\n",
    "    \"filename_test\":\"\",\n",
    "    \"max_length\":128,\n",
    "    \"batch_size\":4,\n",
    "    \"epochs\":10,\n",
    "    \"learning_rate\":5e-4,\n",
    "    \"warmup_steps\":100,\n",
    "    \"epsilon\":1e-8,\n",
    "    \"sample_every\":100,\n",
    "    \"pretrained_path\":\"distilgpt2\",\n",
    "}\n",
    "\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"medical_language_training.py\",\n",
    "    source_dir='./source_dir',\n",
    "    role=role,\n",
    "    py_version=\"py36\",\n",
    "    framework_version='1.8',\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_job_name='distilGPT-Training-Job-test',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:epoch', 'Regex': 'Epochs=(.*?);'},\n",
    "        {'Name': 'train:loss', 'Regex': 'TotLoss=(.*?);'}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45780a52-adb2-4bd1-9db0-fd6d8aef6fbc",
   "metadata": {},
   "source": [
    "#### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa16fec-0620-492e-8285-878f9ea5ff4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    pytorch_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b15e1-ed96-4447-9224-7f4640271588",
   "metadata": {},
   "source": [
    "#### Create Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a2dd2f-75d6-4e44-95a9-27295a8efbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source_dir/medical_language_endpoint_cpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./source_dir/medical_language_endpoint_cpu.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "MODELNAME = '20230112_distilgpt2_medical_generator'\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'text/csv':\n",
    "        return request_body\n",
    "    else:\n",
    "        return 'Letters | '\n",
    "\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model_path = os.path.join(model_dir, MODELNAME)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "    model_dict = {'model': model, 'tokenizer':tokenizer}\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    prompt = input_object\n",
    "    prompt = torch.tensor(model['tokenizer'].encode(prompt)).unsqueeze(0)\n",
    "    prompt = prompt.to(DEVICE)\n",
    "    response = model['model'].generate(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=75,\n",
    "        max_length=300,\n",
    "        top_p=0.99,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    response = [model['tokenizer'].decode(x, skip_special_tokens=True).replace(input_object, '') for x in response][0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fb45c-bb48-40b7-a7ee-43a38d055504",
   "metadata": {},
   "source": [
    "#### Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bcdefeb-00d2-4f94-be0a-22f2a7d54615",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 50\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_data = f's3://{bucket}/distilGPT-Training-Job-test-2023-01-18-23-12-15-239/output/model.tar.gz'\n",
    "endpoint_name = 'distilGPT-Medical-Endpoint'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    source_dir='./source_dir',\n",
    "    entry_point='medical_language_endpoint_cpu.py',\n",
    "    framework_version='1.8',\n",
    "    py_version='py3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0390ee-abb9-41e4-a7cf-322dfe364efe",
   "metadata": {},
   "source": [
    "#### Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f92b9b-6da0-4978-a9be-815598211f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    endpoint = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2a5bf-7b5d-4afa-9b47-1057ebed236e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755b49-8676-4a9b-94ce-3e2166973168",
   "metadata": {},
   "source": [
    "#### Generate Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "164a35fd-412b-4c8d-a663-635d8965a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Allergy / Immunology\n",
      " Bariatrics\n",
      " Cardiovascular / Pulmonary\n",
      " Dentistry\n",
      " Urology\n",
      " General Medicine\n",
      " Surgery\n",
      " Speech - Language\n",
      " SOAP / Chart / Progress Notes\n",
      " Sleep Medicine\n",
      " Rheumatology\n",
      " Radiology\n",
      " Psychiatry / Psychology\n",
      " Podiatry\n",
      " Physical Medicine - Rehab\n",
      " Pediatrics - Neonatal\n",
      " Pain Management\n",
      " Orthopedic\n",
      " Ophthalmology\n",
      " Office Notes\n",
      " Obstetrics / Gynecology\n",
      " Neurosurgery\n",
      " Neurology\n",
      " Nephrology\n",
      " Letters\n",
      " Lab Medicine - Pathology\n",
      " IME-QME-Work Comp etc.\n",
      " Hospice - Palliative Care\n",
      " Hematology - Oncology\n",
      " Gastroenterology\n",
      " ENT - Otolaryngology\n",
      " Endocrinology\n",
      " Emergency Room Reports\n",
      " Discharge Summary\n",
      " Diets and Nutritions\n",
      " Dermatology\n",
      " Cosmetic / Plastic Surgery\n",
      " Consult - History and Phy.\n",
      " Chiropractic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>The patient is a 17-year-old female, who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Possible inflammatory bowel disease.  Polyp o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>Chiropractic</td>\n",
       "      <td>Diagnostic fiberoptic bronchoscopy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>Chiropractic</td>\n",
       "      <td>The patient with epigastric and right upper q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>Chiropractic</td>\n",
       "      <td>Acute acalculous cholecystitis.  Open cholecy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>Chiropractic</td>\n",
       "      <td>Excision of left breast mass.  The mass was i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>Chiropractic</td>\n",
       "      <td>CT-guided needle placement biopsy of right re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          medical_specialty                                        description\n",
       "0      Allergy / Immunology   A 23-year-old white female presents with comp...\n",
       "1      Allergy / Immunology   The patient is a 17-year-old female, who pres...\n",
       "2      Allergy / Immunology   Possible inflammatory bowel disease.  Polyp o...\n",
       "3      Allergy / Immunology   A 23-year-old white female presents with comp...\n",
       "4      Allergy / Immunology   A 23-year-old white female presents with comp...\n",
       "...                     ...                                                ...\n",
       "1945           Chiropractic                Diagnostic fiberoptic bronchoscopy.\n",
       "1946           Chiropractic   The patient with epigastric and right upper q...\n",
       "1947           Chiropractic   Acute acalculous cholecystitis.  Open cholecy...\n",
       "1948           Chiropractic   Excision of left breast mass.  The mass was i...\n",
       "1949           Chiropractic   CT-guided needle placement biopsy of right re...\n",
       "\n",
       "[1950 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pickle.load(open('labels.pkl', 'rb'))\n",
    "generated_samples = []\n",
    "generated_labels = []\n",
    "\n",
    "\n",
    "for lbl in labels:\n",
    "    print(lbl)\n",
    "    prompt = lbl + ' | '\n",
    "    for _ in range(trials):\n",
    "        response = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(EndpointName=endpoint_name, Body=prompt.encode(encoding=\"ascii\"), ContentType=\"text/csv\")\n",
    "        try:\n",
    "            generated_samples.append(response['Body'].read().decode('ascii'))\n",
    "        except:\n",
    "            generated_samples.append('')\n",
    "        generated_labels.append(lbl)\n",
    "df_generated = pd.DataFrame({'medical_specialty':generated_labels, 'description':generated_samples})\n",
    "df_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390af58-62c8-4bf9-bb4e-26e8888a2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    endpoint.sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d066853-73eb-4917-9855-f9ac691474a4",
   "metadata": {},
   "source": [
    "#### Save Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a716fedf-022c-41e3-82fb-0db6333a0c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_generated.drop_duplicates().to_csv('./mtsamples_generated_modern.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
